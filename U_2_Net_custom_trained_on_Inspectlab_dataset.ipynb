{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U-2-Net.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1**"
      ],
      "metadata": {
        "id": "GqfhA6Nk8_98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nMh4Ey5eKmFF",
        "outputId": "dfbd2cd3-9b3a-45c4-a391-8c5aa8521a71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUudnWISOdSe",
        "outputId": "4bb1ec75-d1a1-4c95-87a2-39b43c3af123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/xuebinqin/U-2-Net.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UpGCxiqOki5",
        "outputId": "233844f0-0918-4148-f441-423a442a910a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'U-2-Net'...\n",
            "remote: Enumerating objects: 1017, done.\u001b[K\n",
            "remote: Counting objects: 100% (348/348), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 1017 (delta 305), reused 270 (delta 270), pack-reused 669\u001b[K\n",
            "Receiving objects: 100% (1017/1017), 48.06 MiB | 12.03 MiB/s, done.\n",
            "Resolving deltas: 100% (499/499), done.\n",
            "Checking out files: 100% (193/193), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/U-2-Net/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwdRnmtr5FHA",
        "outputId": "cf50e7c6-4a2a-4e9c-dbc7-aad49f6efb84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/U-2-Net\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing the dependencies.\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "0HnABMb_OvCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5898da1f-0a70-4322-c135-816462165eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.15.2\n",
            "  Downloading numpy-1.15.2-cp37-cp37m-manylinux1_x86_64.whl (13.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting scikit-image==0.14.0\n",
            "  Downloading scikit_image-0.14.0-cp37-cp37m-manylinux1_x86_64.whl (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.12.0+cu113)\n",
            "Collecting pillow==8.1.1\n",
            "  Downloading Pillow-8.1.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.1.2.30)\n",
            "Collecting paddlepaddle\n",
            "  Downloading paddlepaddle-2.2.2-cp37-cp37m-manylinux1_x86_64.whl (108.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 108.4 MB 26 kB/s \n",
            "\u001b[?25hCollecting paddlehub\n",
            "  Downloading paddlehub-2.2.0-py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 39.6 MB/s \n",
            "\u001b[?25hCollecting gradio\n",
            "  Downloading gradio-2.9.4-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.14.0->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.14.0->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.14.0->-r requirements.txt (line 2)) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.14.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.14.0->-r requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: dask[array]>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.14.0->-r requirements.txt (line 2)) (2.12.0)\n",
            "Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.14.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[array]>=0.9.0->scikit-image==0.14.0->-r requirements.txt (line 2)) (0.11.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->scikit-image==0.14.0->-r requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->scikit-image==0.14.0->-r requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->scikit-image==0.14.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->scikit-image==0.14.0->-r requirements.txt (line 2)) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->scikit-image==0.14.0->-r requirements.txt (line 2)) (4.2.0)\n",
            "Collecting PyWavelets>=0.4.0\n",
            "  Downloading PyWavelets-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (6.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.1 MB 19.7 MB/s \n",
            "\u001b[?25h  Downloading PyWavelets-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 31.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle->-r requirements.txt (line 7)) (3.17.3)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from paddlepaddle->-r requirements.txt (line 7)) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from paddlepaddle->-r requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r requirements.txt (line 4)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from paddlehub->-r requirements.txt (line 8)) (4.64.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from paddlehub->-r requirements.txt (line 8)) (3.13)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from paddlehub->-r requirements.txt (line 8)) (22.3.0)\n",
            "Collecting paddlenlp>=2.0.0\n",
            "  Downloading paddlenlp-2.2.6-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 40.9 MB/s \n",
            "\u001b[?25hCollecting visualdl>=2.0.0\n",
            "  Downloading visualdl-2.2.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 40.1 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: flask>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from paddlehub->-r requirements.txt (line 8)) (1.1.4)\n",
            "Collecting paddle2onnx>=0.5.1\n",
            "  Downloading paddle2onnx-0.9.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 39.3 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting rarfile\n",
            "  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from paddlehub->-r requirements.txt (line 8)) (21.3)\n",
            "Collecting gunicorn>=19.10.0\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (from paddlehub->-r requirements.txt (line 8)) (1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from paddlehub->-r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.0->paddlehub->-r requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.0->paddlehub->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.0->paddlehub->-r requirements.txt (line 8)) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.0->paddlehub->-r requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn>=19.10.0->paddlehub->-r requirements.txt (line 8)) (57.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.1.0->paddlehub->-r requirements.txt (line 8)) (2.0.1)\n",
            "Collecting onnx<=1.9.0\n",
            "  Downloading onnx-1.9.0-cp37-cp37m-manylinux2010_x86_64.whl (12.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2 MB 24.5 MB/s \n",
            "\u001b[?25h  Downloading onnx-1.8.1-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 30.8 MB/s \n",
            "\u001b[?25h  Downloading onnx-1.8.0-cp37-cp37m-manylinux2010_x86_64.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 28.8 MB/s \n",
            "\u001b[?25hCollecting paddlefsl\n",
            "  Downloading paddlefsl-1.1.0-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from paddlenlp>=2.0.0->paddlehub->-r requirements.txt (line 8)) (0.42.1)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from paddlenlp>=2.0.0->paddlehub->-r requirements.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from paddlenlp>=2.0.0->paddlehub->-r requirements.txt (line 8)) (0.70.12.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 40.1 MB/s \n",
            "\u001b[?25hCollecting Flask-Babel>=1.0.0\n",
            "  Downloading Flask_Babel-2.0.0-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from visualdl>=2.0.0->paddlehub->-r requirements.txt (line 8)) (1.3.5)\n",
            "Collecting bce-python-sdk\n",
            "  Downloading bce-python-sdk-0.8.64.tar.gz (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 48.5 MB/s \n",
            "\u001b[?25hCollecting shellcheck-py\n",
            "  Downloading shellcheck_py-0.8.0.4-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 27.2 MB/s \n",
            "\u001b[?25hCollecting flake8>=3.7.9\n",
            "  Downloading flake8-4.0.1-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting pre-commit\n",
            "  Downloading pre_commit-2.18.1-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 51.5 MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting importlib-metadata<4.3\n",
            "  Downloading importlib_metadata-4.2.0-py3-none-any.whl (16 kB)\n",
            "Collecting pycodestyle<2.9.0,>=2.8.0\n",
            "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 934 kB/s \n",
            "\u001b[?25hCollecting pyflakes<2.5.0,>=2.4.0\n",
            "  Downloading pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Babel>=2.3 in /usr/local/lib/python3.7/dist-packages (from Flask-Babel>=1.0.0->visualdl>=2.0.0->paddlehub->-r requirements.txt (line 8)) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from Flask-Babel>=1.0.0->visualdl>=2.0.0->paddlehub->-r requirements.txt (line 8)) (2022.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.3->flake8>=3.7.9->visualdl>=2.0.0->paddlehub->-r requirements.txt (line 8)) (3.8.0)\n",
            "Collecting paramiko\n",
            "  Downloading paramiko-2.10.4-py2.py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 54.8 MB/s \n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.6.8-cp37-cp37m-manylinux_2_24_x86_64.whl (253 kB)\n",
            "\u001b[K     |████████████████████████████████| 253 kB 51.7 MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.17.6-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 47.5 MB/s \n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.75.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 29.9 MB/s \n",
            "\u001b[?25hCollecting analytics-python\n",
            "  Downloading analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 49.6 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio->-r requirements.txt (line 9)) (21.4.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 36.1 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio->-r requirements.txt (line 9)) (2.0.12)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff==1.10.0\n",
            "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: future>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from bce-python-sdk->visualdl>=2.0.0->paddlehub->-r requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->paddlenlp>=2.0.0->paddlehub->-r requirements.txt (line 8)) (6.0.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->paddlenlp>=2.0.0->paddlehub->-r requirements.txt (line 8)) (0.3.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 39.6 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 49.1 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 51.3 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 54.0 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.18.2-py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 53.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.18.1-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 34.6 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.18.0-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 51.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 51.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 53.8 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.16.0-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 53.8 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 53.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.15.0-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 9.8 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 42.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 45.8 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.2-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 54.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 46.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0,>=0.0.18\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.13.0-py3-none-any.whl (285 kB)\n",
            "\u001b[K     |████████████████████████████████| 285 kB 30.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 51.0 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.12.0-py3-none-any.whl (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 53.1 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 54.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.10.2-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 46.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.10.1-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 39.7 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.10.0-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 48.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.9.0-py3-none-any.whl (262 kB)\n",
            "\u001b[K     |████████████████████████████████| 262 kB 51.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.8.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 54.0 MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting pyarrow<4.0.0,>=1.0.0\n",
            "  Downloading pyarrow-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (20.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.7 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.7.0-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 49.8 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
            "\u001b[K     |████████████████████████████████| 221 kB 52.2 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.6.1-py3-none-any.whl (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 14.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.6.0-py3-none-any.whl (202 kB)\n",
            "\u001b[K     |████████████████████████████████| 202 kB 52.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.5.0-py3-none-any.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 51.2 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\n",
            "\u001b[K     |████████████████████████████████| 186 kB 52.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.2\n",
            "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.4.0-py3-none-any.whl (186 kB)\n",
            "\u001b[K     |████████████████████████████████| 186 kB 55.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 52.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 54.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 51.2 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.1.3-py3-none-any.whl (153 kB)\n",
            "\u001b[K     |████████████████████████████████| 153 kB 53.1 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.1.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 45.1 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.1.1-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 50.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 56.3 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.0.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 8.8 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 36.0 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.0.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 37.1 MB/s \n",
            "\u001b[?25h  Downloading datasets-0.0.9.zip (4.0 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/cd/fe/4d2874473a753d59c83335691bd9532704f2605418a0d288a1d70fa003fc/datasets-0.0.9.zip#sha256=86d54441bab87aebb2aa3bf0853aa7fb7abed8c708f9bb08a88e86a498972010 (from https://pypi.org/simple/datasets/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "INFO: pip is looking at multiple versions of colorlog to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.5.0-py2.py3-none-any.whl (11 kB)\n",
            "  Downloading colorlog-6.4.1-py2.py3-none-any.whl (11 kB)\n",
            "  Downloading colorlog-6.4.0-py2.py3-none-any.whl (11 kB)\n",
            "  Downloading colorlog-5.0.1-py2.py3-none-any.whl (10 kB)\n",
            "  Downloading colorlog-5.0.0-py2.py3-none-any.whl (10 kB)\n",
            "  Downloading colorlog-4.8.0-py2.py3-none-any.whl (10 kB)\n",
            "  Downloading colorlog-4.7.2-py2.py3-none-any.whl (10 kB)\n",
            "INFO: pip is looking at multiple versions of colorlog to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading colorlog-4.7.0-py2.py3-none-any.whl (10 kB)\n",
            "  Downloading colorlog-4.6.2-py2.py3-none-any.whl (10.0 kB)\n",
            "  Downloading colorlog-4.6.0-py2.py3-none-any.whl (10.0 kB)\n",
            "  Downloading colorlog-4.5.0-py2.py3-none-any.whl (14 kB)\n",
            "  Downloading colorlog-4.4.0-py2.py3-none-any.whl (14 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading colorlog-4.3.0-py2.py3-none-any.whl (14 kB)\n",
            "  Downloading colorlog-4.2.1-py2.py3-none-any.whl (14 kB)\n",
            "  Downloading colorlog-4.2.0-py2.py3-none-any.whl (14 kB)\n",
            "  Downloading colorlog-4.1.0-py2.py3-none-any.whl (14 kB)\n",
            "  Downloading colorlog-4.0.2-py2.py3-none-any.whl (17 kB)\n",
            "  Downloading colorlog-4.0.0-py2.py3-none-any.whl (17 kB)\n",
            "  Downloading colorlog-3.2.0-py2.py3-none-any.whl (17 kB)\n",
            "  Downloading colorlog-3.1.4-py2.py3-none-any.whl (13 kB)\n",
            "  Downloading colorlog-3.1.3-py2.py3-none-any.whl (13 kB)\n",
            "  Downloading colorlog-3.1.2-py2.py3-none-any.whl (17 kB)\n",
            "  Downloading colorlog-3.1.1-py2.py3-none-any.whl (17 kB)\n",
            "  Downloading colorlog-3.1.0-py2.py3-none-any.whl (17 kB)\n",
            "  Downloading colorlog-3.0.1-py2.py3-none-any.whl (17 kB)\n",
            "  Downloading colorlog-3.0.0-py2.py3-none-any.whl (17 kB)\n",
            "  Downloading colorlog-2.10.0-py2.py3-none-any.whl (17 kB)\n",
            "  Downloading colorlog-2.9.0-py2.py3-none-any.whl (17 kB)\n",
            "  Downloading colorlog-2.8.0-py2.py3-none-any.whl (16 kB)\n",
            "  Downloading colorlog-2.7.0-py2.py3-none-any.whl (16 kB)\n",
            "  Downloading colorlog-2.6.3.tar.gz (9.6 kB)\n",
            "  Downloading colorlog-2.6.2-py2.py3-none-any.whl (16 kB)\n",
            "  Downloading colorlog-2.6.1-py2.py3-none-any.whl (11 kB)\n",
            "  Downloading colorlog-2.6.0-py2.py3-none-any.whl (11 kB)\n",
            "  Downloading colorlog-2.5.0-py2.py3-none-any.whl (9.7 kB)\n",
            "  Downloading colorlog-2.4.0-py2.py3-none-any.whl (9.3 kB)\n",
            "  Downloading colorlog-2.3.1-py2.py3-none-any.whl (9.3 kB)\n",
            "  Downloading colorlog-2.3.0.tar.gz (5.5 kB)\n",
            "  Downloading colorlog-2.2.0.tar.gz (5.2 kB)\n",
            "  Downloading colorlog-2.1.1.tar.gz (5.2 kB)\n",
            "  Downloading colorlog-2.1.0.tar.gz (5.2 kB)\n",
            "  Downloading colorlog-2.0.0.tar.gz (4.8 kB)\n",
            "  Downloading colorlog-1.8.tar.gz (4.5 kB)\n",
            "  Downloading colorlog-1.7.tar.gz (4.5 kB)\n",
            "  Downloading colorlog-1.6.tar.gz (4.3 kB)\n",
            "  Downloading colorlog-1.5.tar.gz (4.4 kB)\n",
            "  Downloading colorlog-1.4.tar.gz (4.3 kB)\n",
            "  Downloading colorlog-1.3.tar.gz (3.7 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/3d/6f/028645b3d7b212ed1a3ac4fa85458f4b34904e1e6d804ebce9faf7804bd1/colorlog-1.3.tar.gz#sha256=7b9d46e249a26ad06350ac019a1623749f822677812172139334efbe58a13774 (from https://pypi.org/simple/colorlog/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading colorlog-1.2.tar.gz (3.7 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/9c/a3/ef4fc0bf1f327ab35cba9da00c93f93c89e7ba5b62a45defcd220e9220a6/colorlog-1.2.tar.gz#sha256=1447f0a5ac6939943c1ed2975b17dc7dc9bb85f132c5f17e722a249c57cc70b7 (from https://pypi.org/simple/colorlog/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading colorlog-1.1.tar.gz (3.7 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/3e/56/d3ddf5bf04dc1f2048ea40e54845befad34f15a0d83483383f110acc56ad/colorlog-1.1.tar.gz#sha256=34394575191986e7f8d1d584741794b85a27c5f61be46c590bf3ad9d762af6c1 (from https://pypi.org/simple/colorlog/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading colorlog-1.0.tar.gz (3.7 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/5b/93/de1e2e9f234d2dd8e707d11c7733eaaafd28c577dcafebb74b51b2047558/colorlog-1.0.tar.gz#sha256=8d53135daaf3b3d21bff731dd2f24e1cc17c5a37fc6b93675e99d611b8f8c9f2 (from https://pypi.org/simple/colorlog/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading colorlog-0.6.tar.gz (3.2 kB)\n",
            "  Downloading colorlog-0.5.tar.gz (3.2 kB)\n",
            "  Downloading colorlog-0.4.tar.gz (2.6 kB)\n",
            "  Downloading colorlog-0.3.tar.gz (2.4 kB)\n",
            "  Downloading colorlog-0.2.tar.gz (2.4 kB)\n",
            "  Downloading colorlog-0.1.tar.gz (2.4 kB)\n",
            "INFO: pip is looking at multiple versions of colorama to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
            "  Downloading colorama-0.4.1-py2.py3-none-any.whl (15 kB)\n",
            "  Downloading colorama-0.4.0-py2.py3-none-any.whl (21 kB)\n",
            "  Downloading colorama-0.3.9-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading colorama-0.3.8-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading colorama-0.3.7-py2.py3-none-any.whl (19 kB)\n",
            "  Downloading colorama-0.3.6-py2.py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of colorama to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading colorama-0.3.5-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading colorama-0.3.4.zip (36 kB)\n",
            "  Downloading colorama-0.3.3.tar.gz (22 kB)\n",
            "  Downloading colorama-0.3.2.zip (32 kB)\n",
            "  Downloading colorama-0.3.1.zip (31 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading colorama-0.3.0.zip (24 kB)\n",
            "  Downloading colorama-0.2.7.zip (24 kB)\n",
            "  Downloading colorama-0.2.6.zip (25 kB)\n",
            "  Downloading colorama-0.2.5.zip (23 kB)\n",
            "  Downloading colorama-0.2.4.zip (21 kB)\n",
            "  Downloading colorama-0.2.3.zip (23 kB)\n",
            "  Downloading colorama-0.2.2.zip (23 kB)\n",
            "  Downloading colorama-0.2.1.zip (23 kB)\n",
            "  Downloading colorama-0.2.0.zip (16 kB)\n",
            "  Downloading colorama-0.1.18.zip (14 kB)\n",
            "  Downloading colorama-0.1.17.zip (15 kB)\n",
            "  Downloading colorama-0.1.16.zip (14 kB)\n",
            "  Downloading colorama-0.1.15.zip (4.6 kB)\n",
            "\u001b[31mERROR: File \"setup.py\" not found for legacy project colorama from https://files.pythonhosted.org/packages/ca/c2/6910ba3f0de55091bf88e96b94475bf72056d08aa0ce2e45f6a7c5d4e725/colorama-0.1.15.zip#sha256=fb55091e0dd982edcfa99f039f8ae85f7acd538626c15de71fed39a6b7faa3fd (from paddlehub->-r requirements.txt (line 8)).\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run the code to start the training.\n",
        "!python u2net_train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJdStc0eu1ZG",
        "outputId": "c4f18e20-e1cb-404e-dc92-3b7014ca2d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "[epoch:  99/1000, batch:     5/   15, ite: 1475] train loss: 0.258751, tar: 0.028184 \n",
            "l0: 0.018092, l1: 0.016984, l2: 0.018734, l3: 0.022676, l4: 0.029501, l5: 0.040564, l6: 0.046271\n",
            "\n",
            "[epoch:  99/1000, batch:     6/   15, ite: 1476] train loss: 0.257884, tar: 0.028052 \n",
            "l0: 0.020906, l1: 0.019904, l2: 0.021626, l3: 0.026201, l4: 0.029405, l5: 0.034258, l6: 0.046321\n",
            "\n",
            "[epoch:  99/1000, batch:     7/   15, ite: 1477] train loss: 0.257114, tar: 0.027959 \n",
            "l0: 0.034358, l1: 0.040506, l2: 0.037040, l3: 0.033591, l4: 0.041737, l5: 0.055217, l6: 0.076366\n",
            "\n",
            "[epoch:  99/1000, batch:     8/   15, ite: 1478] train loss: 0.257905, tar: 0.028041 \n",
            "l0: 0.049320, l1: 0.059079, l2: 0.055330, l3: 0.058285, l4: 0.039971, l5: 0.039021, l6: 0.049476\n",
            "\n",
            "[epoch:  99/1000, batch:     9/   15, ite: 1479] train loss: 0.259077, tar: 0.028310 \n",
            "l0: 0.036872, l1: 0.039156, l2: 0.038808, l3: 0.040572, l4: 0.044180, l5: 0.040626, l6: 0.057312\n",
            "\n",
            "[epoch:  99/1000, batch:    10/   15, ite: 1480] train loss: 0.259558, tar: 0.028417 \n",
            "l0: 0.040777, l1: 0.038931, l2: 0.041421, l3: 0.050113, l4: 0.056176, l5: 0.050754, l6: 0.064564\n",
            "\n",
            "[epoch:  99/1000, batch:    11/   15, ite: 1481] train loss: 0.260585, tar: 0.028570 \n",
            "l0: 0.022913, l1: 0.024827, l2: 0.023796, l3: 0.024889, l4: 0.028035, l5: 0.026311, l6: 0.037877\n",
            "\n",
            "[epoch:  99/1000, batch:    12/   15, ite: 1482] train loss: 0.259707, tar: 0.028501 \n",
            "l0: 0.017541, l1: 0.016885, l2: 0.020598, l3: 0.025603, l4: 0.027849, l5: 0.036030, l6: 0.063942\n",
            "\n",
            "[epoch:  99/1000, batch:    13/   15, ite: 1483] train loss: 0.259090, tar: 0.028369 \n",
            "l0: 0.022601, l1: 0.021934, l2: 0.023837, l3: 0.030074, l4: 0.031017, l5: 0.041897, l6: 0.050948\n",
            "\n",
            "[epoch:  99/1000, batch:    14/   15, ite: 1484] train loss: 0.258652, tar: 0.028300 \n",
            "l0: 0.029851, l1: 0.029297, l2: 0.032714, l3: 0.037576, l4: 0.038339, l5: 0.054736, l6: 0.081034\n",
            "\n",
            "[epoch:  99/1000, batch:    15/   15, ite: 1485] train loss: 0.259180, tar: 0.028318 \n",
            "l0: 0.028842, l1: 0.029962, l2: 0.029067, l3: 0.033747, l4: 0.041955, l5: 0.052030, l6: 0.048929\n",
            "\n",
            "[epoch: 100/1000, batch:     1/   15, ite: 1486] train loss: 0.259242, tar: 0.028324 \n",
            "l0: 0.027874, l1: 0.026820, l2: 0.030142, l3: 0.028824, l4: 0.035047, l5: 0.043503, l6: 0.054479\n",
            "\n",
            "[epoch: 100/1000, batch:     2/   15, ite: 1487] train loss: 0.259098, tar: 0.028319 \n",
            "l0: 0.026240, l1: 0.024588, l2: 0.024518, l3: 0.031969, l4: 0.033374, l5: 0.043482, l6: 0.064215\n",
            "\n",
            "[epoch: 100/1000, batch:     3/   15, ite: 1488] train loss: 0.258976, tar: 0.028296 \n",
            "l0: 0.024114, l1: 0.024161, l2: 0.025892, l3: 0.030071, l4: 0.029866, l5: 0.033454, l6: 0.049460\n",
            "\n",
            "[epoch: 100/1000, batch:     4/   15, ite: 1489] train loss: 0.258505, tar: 0.028249 \n",
            "l0: 0.034578, l1: 0.034962, l2: 0.036319, l3: 0.034389, l4: 0.036731, l5: 0.045176, l6: 0.065358\n",
            "\n",
            "[epoch: 100/1000, batch:     5/   15, ite: 1490] train loss: 0.258827, tar: 0.028319 \n",
            "l0: 0.032118, l1: 0.027838, l2: 0.034084, l3: 0.040491, l4: 0.047667, l5: 0.056218, l6: 0.076231\n",
            "\n",
            "[epoch: 100/1000, batch:     6/   15, ite: 1491] train loss: 0.259441, tar: 0.028361 \n",
            "l0: 0.042404, l1: 0.049966, l2: 0.050306, l3: 0.047861, l4: 0.044294, l5: 0.041562, l6: 0.050902\n",
            "\n",
            "[epoch: 100/1000, batch:     7/   15, ite: 1492] train loss: 0.260178, tar: 0.028513 \n",
            "l0: 0.019179, l1: 0.019909, l2: 0.021522, l3: 0.021837, l4: 0.025096, l5: 0.036281, l6: 0.061976\n",
            "\n",
            "[epoch: 100/1000, batch:     8/   15, ite: 1493] train loss: 0.259594, tar: 0.028413 \n",
            "l0: 0.027494, l1: 0.029405, l2: 0.032161, l3: 0.034853, l4: 0.036014, l5: 0.039464, l6: 0.056196\n",
            "\n",
            "[epoch: 100/1000, batch:     9/   15, ite: 1494] train loss: 0.259551, tar: 0.028403 \n",
            "l0: 0.027351, l1: 0.025675, l2: 0.032037, l3: 0.034403, l4: 0.038370, l5: 0.053267, l6: 0.066909\n",
            "\n",
            "[epoch: 100/1000, batch:    10/   15, ite: 1495] train loss: 0.259745, tar: 0.028392 \n",
            "l0: 0.025562, l1: 0.025573, l2: 0.027823, l3: 0.031426, l4: 0.032303, l5: 0.034905, l6: 0.044414\n",
            "\n",
            "[epoch: 100/1000, batch:    11/   15, ite: 1496] train loss: 0.259352, tar: 0.028363 \n",
            "l0: 0.020856, l1: 0.019891, l2: 0.025092, l3: 0.026853, l4: 0.030971, l5: 0.042294, l6: 0.061334\n",
            "\n",
            "[epoch: 100/1000, batch:    12/   15, ite: 1497] train loss: 0.259022, tar: 0.028285 \n",
            "l0: 0.022431, l1: 0.021644, l2: 0.025903, l3: 0.029152, l4: 0.036567, l5: 0.040420, l6: 0.066577\n",
            "\n",
            "[epoch: 100/1000, batch:    13/   15, ite: 1498] train loss: 0.258855, tar: 0.028226 \n",
            "l0: 0.017051, l1: 0.016231, l2: 0.019666, l3: 0.022071, l4: 0.026174, l5: 0.035446, l6: 0.046310\n",
            "\n",
            "[epoch: 100/1000, batch:    14/   15, ite: 1499] train loss: 0.258088, tar: 0.028113 \n",
            "l0: 0.045180, l1: 0.051562, l2: 0.045686, l3: 0.047299, l4: 0.051360, l5: 0.051803, l6: 0.057401\n",
            "\n",
            "[epoch: 100/1000, batch:    15/   15, ite: 1500] train loss: 0.259010, tar: 0.028283 \n",
            "l0: 0.024391, l1: 0.020466, l2: 0.025136, l3: 0.030556, l4: 0.034300, l5: 0.045703, l6: 0.055535\n",
            "\n",
            "[epoch: 101/1000, batch:     1/   15, ite: 1501] train loss: 0.236087, tar: 0.024391 \n",
            "l0: 0.023218, l1: 0.019293, l2: 0.027935, l3: 0.031048, l4: 0.036798, l5: 0.038040, l6: 0.056435\n",
            "\n",
            "[epoch: 101/1000, batch:     2/   15, ite: 1502] train loss: 0.234427, tar: 0.023804 \n",
            "l0: 0.031070, l1: 0.024525, l2: 0.029786, l3: 0.037366, l4: 0.041588, l5: 0.051038, l6: 0.073699\n",
            "\n",
            "[epoch: 101/1000, batch:     3/   15, ite: 1503] train loss: 0.252642, tar: 0.026226 \n",
            "l0: 0.028664, l1: 0.029759, l2: 0.031229, l3: 0.031982, l4: 0.047015, l5: 0.054297, l6: 0.059563\n",
            "\n",
            "[epoch: 101/1000, batch:     4/   15, ite: 1504] train loss: 0.260109, tar: 0.026836 \n",
            "l0: 0.029749, l1: 0.027696, l2: 0.032294, l3: 0.036531, l4: 0.037016, l5: 0.048518, l6: 0.065317\n",
            "\n",
            "[epoch: 101/1000, batch:     5/   15, ite: 1505] train loss: 0.263512, tar: 0.027418 \n",
            "l0: 0.021157, l1: 0.019041, l2: 0.020584, l3: 0.026086, l4: 0.034365, l5: 0.046352, l6: 0.053589\n",
            "\n",
            "[epoch: 101/1000, batch:     6/   15, ite: 1506] train loss: 0.256455, tar: 0.026375 \n",
            "l0: 0.022436, l1: 0.022852, l2: 0.025446, l3: 0.026211, l4: 0.032411, l5: 0.040583, l6: 0.057451\n",
            "\n",
            "[epoch: 101/1000, batch:     7/   15, ite: 1507] train loss: 0.252303, tar: 0.025812 \n",
            "l0: 0.038281, l1: 0.037902, l2: 0.048009, l3: 0.045115, l4: 0.042191, l5: 0.039390, l6: 0.059921\n",
            "\n",
            "[epoch: 101/1000, batch:     8/   15, ite: 1508] train loss: 0.259616, tar: 0.027371 \n",
            "l0: 0.030905, l1: 0.031759, l2: 0.030994, l3: 0.035333, l4: 0.036622, l5: 0.036994, l6: 0.039017\n",
            "\n",
            "[epoch: 101/1000, batch:     9/   15, ite: 1509] train loss: 0.257617, tar: 0.027763 \n",
            "l0: 0.018222, l1: 0.017487, l2: 0.019188, l3: 0.020998, l4: 0.025778, l5: 0.030584, l6: 0.039777\n",
            "\n",
            "[epoch: 101/1000, batch:    10/   15, ite: 1510] train loss: 0.249059, tar: 0.026809 \n",
            "l0: 0.044280, l1: 0.050954, l2: 0.041171, l3: 0.055106, l4: 0.051790, l5: 0.049079, l6: 0.069860\n",
            "\n",
            "[epoch: 101/1000, batch:    11/   15, ite: 1511] train loss: 0.259348, tar: 0.028398 \n",
            "l0: 0.027434, l1: 0.030952, l2: 0.034610, l3: 0.035087, l4: 0.034591, l5: 0.035897, l6: 0.060258\n",
            "\n",
            "[epoch: 101/1000, batch:    12/   15, ite: 1512] train loss: 0.259305, tar: 0.028317 \n",
            "l0: 0.029276, l1: 0.031534, l2: 0.031401, l3: 0.034598, l4: 0.041156, l5: 0.041992, l6: 0.059450\n",
            "\n",
            "[epoch: 101/1000, batch:    13/   15, ite: 1513] train loss: 0.260082, tar: 0.028391 \n",
            "l0: 0.022916, l1: 0.022133, l2: 0.025965, l3: 0.031280, l4: 0.035341, l5: 0.042572, l6: 0.058200\n",
            "\n",
            "[epoch: 101/1000, batch:    14/   15, ite: 1514] train loss: 0.258534, tar: 0.028000 \n",
            "l0: 0.022886, l1: 0.020980, l2: 0.027307, l3: 0.031861, l4: 0.033091, l5: 0.035186, l6: 0.047794\n",
            "\n",
            "[epoch: 101/1000, batch:    15/   15, ite: 1515] train loss: 0.255905, tar: 0.027659 \n",
            "l0: 0.022050, l1: 0.022863, l2: 0.022029, l3: 0.025973, l4: 0.027250, l5: 0.036282, l6: 0.050127\n",
            "\n",
            "[epoch: 102/1000, batch:     1/   15, ite: 1516] train loss: 0.252822, tar: 0.027308 \n",
            "l0: 0.034623, l1: 0.036713, l2: 0.034803, l3: 0.042052, l4: 0.046855, l5: 0.052148, l6: 0.072340\n",
            "\n",
            "[epoch: 102/1000, batch:     2/   15, ite: 1517] train loss: 0.256746, tar: 0.027739 \n",
            "l0: 0.025543, l1: 0.022368, l2: 0.026897, l3: 0.031172, l4: 0.035822, l5: 0.051054, l6: 0.061418\n",
            "\n",
            "[epoch: 102/1000, batch:     3/   15, ite: 1518] train loss: 0.256609, tar: 0.027617 \n",
            "l0: 0.039130, l1: 0.037853, l2: 0.036281, l3: 0.040537, l4: 0.049596, l5: 0.063293, l6: 0.072557\n",
            "\n",
            "[epoch: 102/1000, batch:     4/   15, ite: 1519] train loss: 0.260958, tar: 0.028223 \n",
            "l0: 0.020459, l1: 0.020066, l2: 0.023085, l3: 0.026464, l4: 0.028222, l5: 0.035054, l6: 0.048831\n",
            "\n",
            "[epoch: 102/1000, batch:     5/   15, ite: 1520] train loss: 0.258019, tar: 0.027835 \n",
            "l0: 0.058481, l1: 0.042449, l2: 0.051893, l3: 0.076744, l4: 0.080123, l5: 0.084845, l6: 0.092915\n",
            "\n",
            "[epoch: 102/1000, batch:     6/   15, ite: 1521] train loss: 0.268944, tar: 0.029294 \n",
            "l0: 0.025788, l1: 0.028373, l2: 0.028139, l3: 0.029279, l4: 0.032398, l5: 0.034634, l6: 0.049876\n",
            "\n",
            "[epoch: 102/1000, batch:     7/   15, ite: 1522] train loss: 0.267105, tar: 0.029135 \n",
            "l0: 0.030463, l1: 0.036865, l2: 0.028327, l3: 0.032957, l4: 0.031965, l5: 0.034318, l6: 0.045348\n",
            "\n",
            "[epoch: 102/1000, batch:     8/   15, ite: 1523] train loss: 0.265938, tar: 0.029192 \n",
            "l0: 0.029548, l1: 0.024424, l2: 0.028467, l3: 0.036126, l4: 0.035270, l5: 0.040204, l6: 0.052458\n",
            "\n",
            "[epoch: 102/1000, batch:     9/   15, ite: 1524] train loss: 0.265128, tar: 0.029207 \n",
            "l0: 0.025442, l1: 0.026989, l2: 0.026488, l3: 0.030875, l4: 0.031657, l5: 0.035634, l6: 0.047454\n",
            "\n",
            "[epoch: 102/1000, batch:    10/   15, ite: 1525] train loss: 0.263504, tar: 0.029057 \n",
            "l0: 0.024397, l1: 0.024339, l2: 0.028085, l3: 0.031408, l4: 0.040010, l5: 0.045469, l6: 0.062927\n",
            "\n",
            "[epoch: 102/1000, batch:    11/   15, ite: 1526] train loss: 0.263240, tar: 0.028877 \n",
            "l0: 0.021064, l1: 0.020720, l2: 0.023499, l3: 0.026803, l4: 0.028002, l5: 0.033825, l6: 0.042827\n",
            "\n",
            "[epoch: 102/1000, batch:    12/   15, ite: 1527] train loss: 0.260777, tar: 0.028588 \n",
            "l0: 0.031754, l1: 0.030861, l2: 0.034555, l3: 0.041594, l4: 0.045505, l5: 0.042789, l6: 0.061341\n",
            "\n",
            "[epoch: 102/1000, batch:    13/   15, ite: 1528] train loss: 0.261763, tar: 0.028701 \n",
            "l0: 0.031214, l1: 0.030190, l2: 0.032254, l3: 0.043000, l4: 0.047580, l5: 0.048182, l6: 0.057850\n",
            "\n",
            "[epoch: 102/1000, batch:    14/   15, ite: 1529] train loss: 0.262746, tar: 0.028788 \n",
            "l0: 0.018007, l1: 0.016325, l2: 0.019995, l3: 0.022458, l4: 0.025876, l5: 0.032042, l6: 0.040393\n",
            "\n",
            "[epoch: 102/1000, batch:    15/   15, ite: 1530] train loss: 0.259825, tar: 0.028428 \n",
            "l0: 0.028473, l1: 0.029066, l2: 0.030477, l3: 0.033316, l4: 0.038494, l5: 0.054945, l6: 0.076162\n",
            "\n",
            "[epoch: 103/1000, batch:     1/   15, ite: 1531] train loss: 0.260828, tar: 0.028430 \n",
            "l0: 0.026045, l1: 0.028434, l2: 0.029171, l3: 0.032559, l4: 0.028650, l5: 0.035096, l6: 0.065080\n",
            "\n",
            "[epoch: 103/1000, batch:     2/   15, ite: 1532] train loss: 0.260335, tar: 0.028355 \n",
            "l0: 0.019160, l1: 0.016821, l2: 0.020716, l3: 0.028664, l4: 0.029032, l5: 0.033954, l6: 0.052029\n",
            "\n",
            "[epoch: 103/1000, batch:     3/   15, ite: 1533] train loss: 0.258518, tar: 0.028077 \n",
            "l0: 0.024799, l1: 0.024129, l2: 0.025532, l3: 0.029682, l4: 0.035365, l5: 0.041688, l6: 0.056034\n",
            "\n",
            "[epoch: 103/1000, batch:     4/   15, ite: 1534] train loss: 0.257892, tar: 0.027980 \n",
            "l0: 0.017546, l1: 0.016549, l2: 0.017376, l3: 0.019797, l4: 0.027761, l5: 0.032646, l6: 0.045048\n",
            "\n",
            "[epoch: 103/1000, batch:     5/   15, ite: 1535] train loss: 0.255572, tar: 0.027682 \n",
            "l0: 0.028087, l1: 0.024502, l2: 0.030363, l3: 0.031106, l4: 0.039454, l5: 0.041386, l6: 0.056543\n",
            "\n",
            "[epoch: 103/1000, batch:     6/   15, ite: 1536] train loss: 0.255458, tar: 0.027693 \n",
            "l0: 0.041913, l1: 0.032150, l2: 0.042456, l3: 0.055253, l4: 0.064466, l5: 0.057897, l6: 0.068203\n",
            "\n",
            "[epoch: 103/1000, batch:     7/   15, ite: 1537] train loss: 0.258346, tar: 0.028078 \n",
            "l0: 0.024445, l1: 0.022668, l2: 0.027940, l3: 0.032485, l4: 0.039152, l5: 0.039162, l6: 0.053078\n",
            "\n",
            "[epoch: 103/1000, batch:     8/   15, ite: 1538] train loss: 0.257835, tar: 0.027982 \n",
            "l0: 0.030746, l1: 0.030877, l2: 0.033263, l3: 0.040742, l4: 0.048206, l5: 0.043724, l6: 0.076991\n",
            "\n",
            "[epoch: 103/1000, batch:     9/   15, ite: 1539] train loss: 0.259033, tar: 0.028053 \n",
            "l0: 0.036816, l1: 0.035622, l2: 0.035849, l3: 0.045308, l4: 0.056324, l5: 0.059776, l6: 0.080985\n",
            "\n",
            "[epoch: 103/1000, batch:    10/   15, ite: 1540] train loss: 0.261324, tar: 0.028272 \n",
            "l0: 0.029600, l1: 0.030613, l2: 0.036466, l3: 0.038730, l4: 0.044246, l5: 0.045364, l6: 0.052325\n",
            "\n",
            "[epoch: 103/1000, batch:    11/   15, ite: 1541] train loss: 0.261715, tar: 0.028304 \n",
            "l0: 0.028788, l1: 0.032299, l2: 0.035740, l3: 0.033840, l4: 0.032940, l5: 0.036237, l6: 0.049807\n",
            "\n",
            "[epoch: 103/1000, batch:    12/   15, ite: 1542] train loss: 0.261428, tar: 0.028316 \n",
            "l0: 0.020963, l1: 0.021582, l2: 0.021038, l3: 0.022080, l4: 0.024640, l5: 0.034110, l6: 0.045763\n",
            "\n",
            "[epoch: 103/1000, batch:    13/   15, ite: 1543] train loss: 0.259771, tar: 0.028145 \n",
            "l0: 0.024524, l1: 0.024564, l2: 0.027870, l3: 0.032560, l4: 0.035655, l5: 0.037787, l6: 0.052544\n",
            "\n",
            "[epoch: 103/1000, batch:    14/   15, ite: 1544] train loss: 0.259219, tar: 0.028063 \n",
            "l0: 0.025165, l1: 0.023119, l2: 0.026552, l3: 0.034174, l4: 0.038015, l5: 0.036149, l6: 0.062109\n",
            "\n",
            "[epoch: 103/1000, batch:    15/   15, ite: 1545] train loss: 0.258910, tar: 0.027998 \n",
            "l0: 0.019343, l1: 0.017842, l2: 0.020487, l3: 0.023171, l4: 0.026241, l5: 0.034172, l6: 0.051728\n",
            "\n",
            "[epoch: 104/1000, batch:     1/   15, ite: 1546] train loss: 0.257477, tar: 0.027810 \n",
            "l0: 0.031020, l1: 0.027255, l2: 0.036916, l3: 0.035877, l4: 0.038416, l5: 0.062212, l6: 0.067037\n",
            "\n",
            "[epoch: 104/1000, batch:     2/   15, ite: 1547] train loss: 0.258354, tar: 0.027878 \n",
            "l0: 0.039303, l1: 0.045242, l2: 0.043974, l3: 0.043773, l4: 0.037083, l5: 0.034518, l6: 0.042179\n",
            "\n",
            "[epoch: 104/1000, batch:     3/   15, ite: 1548] train loss: 0.258932, tar: 0.028116 \n",
            "l0: 0.027170, l1: 0.024027, l2: 0.032068, l3: 0.035593, l4: 0.044157, l5: 0.043282, l6: 0.045319\n",
            "\n",
            "[epoch: 104/1000, batch:     4/   15, ite: 1549] train loss: 0.258782, tar: 0.028097 \n",
            "l0: 0.019269, l1: 0.016741, l2: 0.022097, l3: 0.027551, l4: 0.033902, l5: 0.034803, l6: 0.057581\n",
            "\n",
            "[epoch: 104/1000, batch:     5/   15, ite: 1550] train loss: 0.257846, tar: 0.027921 \n",
            "l0: 0.023988, l1: 0.021379, l2: 0.026911, l3: 0.031768, l4: 0.035348, l5: 0.040108, l6: 0.062674\n",
            "\n",
            "[epoch: 104/1000, batch:     6/   15, ite: 1551] train loss: 0.257538, tar: 0.027843 \n",
            "l0: 0.021460, l1: 0.020003, l2: 0.024254, l3: 0.027613, l4: 0.029267, l5: 0.035967, l6: 0.052960\n",
            "\n",
            "[epoch: 104/1000, batch:     7/   15, ite: 1552] train loss: 0.256654, tar: 0.027721 \n",
            "l0: 0.038125, l1: 0.032261, l2: 0.048698, l3: 0.057708, l4: 0.061467, l5: 0.058564, l6: 0.071812\n",
            "\n",
            "[epoch: 104/1000, batch:     8/   15, ite: 1553] train loss: 0.258766, tar: 0.027917 \n",
            "l0: 0.032878, l1: 0.031125, l2: 0.038569, l3: 0.045296, l4: 0.044715, l5: 0.053096, l6: 0.055675\n",
            "\n",
            "[epoch: 104/1000, batch:     9/   15, ite: 1554] train loss: 0.259555, tar: 0.028009 \n",
            "l0: 0.024254, l1: 0.024330, l2: 0.025817, l3: 0.035246, l4: 0.036584, l5: 0.046190, l6: 0.065134\n",
            "\n",
            "[epoch: 104/1000, batch:    10/   15, ite: 1555] train loss: 0.259519, tar: 0.027941 \n",
            "l0: 0.017759, l1: 0.016238, l2: 0.019810, l3: 0.021520, l4: 0.025775, l5: 0.030233, l6: 0.041761\n",
            "\n",
            "[epoch: 104/1000, batch:    11/   15, ite: 1556] train loss: 0.257975, tar: 0.027759 \n",
            "l0: 0.022661, l1: 0.022562, l2: 0.028162, l3: 0.028361, l4: 0.033809, l5: 0.039268, l6: 0.054669\n",
            "\n",
            "[epoch: 104/1000, batch:    12/   15, ite: 1557] train loss: 0.257476, tar: 0.027669 \n",
            "l0: 0.029367, l1: 0.031616, l2: 0.032242, l3: 0.038922, l4: 0.039944, l5: 0.040780, l6: 0.059505\n",
            "\n",
            "[epoch: 104/1000, batch:    13/   15, ite: 1558] train loss: 0.257733, tar: 0.027699 \n",
            "l0: 0.022912, l1: 0.023638, l2: 0.022676, l3: 0.028125, l4: 0.028367, l5: 0.033966, l6: 0.044248\n",
            "\n",
            "[epoch: 104/1000, batch:    14/   15, ite: 1559] train loss: 0.256821, tar: 0.027617 \n",
            "l0: 0.019754, l1: 0.021644, l2: 0.020103, l3: 0.025904, l4: 0.030221, l5: 0.033404, l6: 0.047991\n",
            "\n",
            "[epoch: 104/1000, batch:    15/   15, ite: 1560] train loss: 0.255857, tar: 0.027486 \n",
            "l0: 0.021430, l1: 0.022554, l2: 0.022632, l3: 0.022633, l4: 0.031582, l5: 0.032417, l6: 0.043738\n",
            "\n",
            "[epoch: 105/1000, batch:     1/   15, ite: 1561] train loss: 0.254892, tar: 0.027387 \n",
            "l0: 0.028016, l1: 0.025399, l2: 0.028888, l3: 0.036904, l4: 0.045838, l5: 0.045734, l6: 0.058579\n",
            "\n",
            "[epoch: 105/1000, batch:     2/   15, ite: 1562] train loss: 0.255126, tar: 0.027397 \n",
            "l0: 0.020548, l1: 0.023185, l2: 0.023974, l3: 0.028783, l4: 0.029416, l5: 0.031948, l6: 0.043904\n",
            "\n",
            "[epoch: 105/1000, batch:     3/   15, ite: 1563] train loss: 0.254279, tar: 0.027289 \n",
            "l0: 0.035267, l1: 0.037340, l2: 0.037046, l3: 0.045801, l4: 0.045595, l5: 0.045731, l6: 0.058310\n",
            "\n",
            "[epoch: 105/1000, batch:     4/   15, ite: 1564] train loss: 0.255072, tar: 0.027413 \n",
            "l0: 0.033882, l1: 0.029479, l2: 0.027138, l3: 0.036865, l4: 0.039228, l5: 0.055786, l6: 0.066263\n",
            "\n",
            "[epoch: 105/1000, batch:     5/   15, ite: 1565] train loss: 0.255589, tar: 0.027513 \n",
            "l0: 0.026271, l1: 0.025409, l2: 0.025836, l3: 0.034304, l4: 0.038924, l5: 0.045934, l6: 0.066426\n",
            "\n",
            "[epoch: 105/1000, batch:     6/   15, ite: 1566] train loss: 0.255703, tar: 0.027494 \n",
            "l0: 0.018449, l1: 0.017049, l2: 0.020449, l3: 0.024478, l4: 0.030651, l5: 0.038713, l6: 0.047713\n",
            "\n",
            "[epoch: 105/1000, batch:     7/   15, ite: 1567] train loss: 0.254834, tar: 0.027359 \n",
            "l0: 0.027718, l1: 0.028071, l2: 0.027249, l3: 0.033864, l4: 0.032683, l5: 0.044209, l6: 0.049742\n",
            "\n",
            "[epoch: 105/1000, batch:     8/   15, ite: 1568] train loss: 0.254668, tar: 0.027364 \n",
            "l0: 0.032114, l1: 0.029141, l2: 0.033742, l3: 0.041857, l4: 0.037385, l5: 0.043169, l6: 0.058510\n",
            "\n",
            "[epoch: 105/1000, batch:     9/   15, ite: 1569] train loss: 0.254976, tar: 0.027433 \n",
            "l0: 0.021048, l1: 0.019675, l2: 0.020748, l3: 0.026872, l4: 0.026537, l5: 0.032494, l6: 0.044030\n",
            "\n",
            "[epoch: 105/1000, batch:    10/   15, ite: 1570] train loss: 0.254068, tar: 0.027342 \n",
            "l0: 0.023579, l1: 0.021391, l2: 0.027031, l3: 0.030215, l4: 0.030202, l5: 0.035082, l6: 0.048075\n",
            "\n",
            "[epoch: 105/1000, batch:    11/   15, ite: 1571] train loss: 0.253526, tar: 0.027289 \n",
            "l0: 0.034112, l1: 0.034140, l2: 0.041495, l3: 0.038717, l4: 0.039637, l5: 0.044337, l6: 0.053437\n",
            "\n",
            "[epoch: 105/1000, batch:    12/   15, ite: 1572] train loss: 0.253975, tar: 0.027384 \n",
            "l0: 0.019525, l1: 0.017542, l2: 0.021236, l3: 0.027733, l4: 0.031644, l5: 0.036010, l6: 0.051262\n",
            "\n",
            "[epoch: 105/1000, batch:    13/   15, ite: 1573] train loss: 0.253303, tar: 0.027276 \n",
            "l0: 0.040594, l1: 0.026391, l2: 0.036220, l3: 0.064761, l4: 0.061105, l5: 0.068434, l6: 0.086364\n",
            "\n",
            "[epoch: 105/1000, batch:    14/   15, ite: 1574] train loss: 0.255068, tar: 0.027456 \n",
            "l0: 0.025773, l1: 0.019335, l2: 0.027295, l3: 0.039281, l4: 0.050149, l5: 0.050931, l6: 0.062060\n",
            "\n",
            "[epoch: 105/1000, batch:    15/   15, ite: 1575] train loss: 0.255331, tar: 0.027433 \n",
            "l0: 0.028325, l1: 0.027616, l2: 0.033310, l3: 0.034977, l4: 0.032396, l5: 0.034640, l6: 0.054692\n",
            "\n",
            "[epoch: 106/1000, batch:     1/   15, ite: 1576] train loss: 0.255208, tar: 0.027445 \n",
            "l0: 0.030158, l1: 0.027505, l2: 0.031105, l3: 0.034892, l4: 0.040691, l5: 0.045073, l6: 0.043037\n",
            "\n",
            "[epoch: 106/1000, batch:     2/   15, ite: 1577] train loss: 0.255172, tar: 0.027480 \n",
            "l0: 0.022382, l1: 0.021043, l2: 0.025979, l3: 0.031270, l4: 0.030484, l5: 0.030474, l6: 0.041780\n",
            "\n",
            "[epoch: 106/1000, batch:     3/   15, ite: 1578] train loss: 0.254509, tar: 0.027415 \n",
            "l0: 0.036934, l1: 0.035715, l2: 0.040639, l3: 0.049319, l4: 0.053349, l5: 0.057797, l6: 0.078066\n",
            "\n",
            "[epoch: 106/1000, batch:     4/   15, ite: 1579] train loss: 0.255740, tar: 0.027536 \n",
            "l0: 0.034949, l1: 0.033404, l2: 0.038214, l3: 0.039208, l4: 0.057520, l5: 0.053108, l6: 0.059672\n",
            "\n",
            "[epoch: 106/1000, batch:     5/   15, ite: 1580] train loss: 0.256494, tar: 0.027628 \n",
            "l0: 0.017934, l1: 0.018920, l2: 0.019174, l3: 0.022598, l4: 0.027937, l5: 0.037012, l6: 0.051909\n",
            "\n",
            "[epoch: 106/1000, batch:     6/   15, ite: 1581] train loss: 0.255741, tar: 0.027509 \n",
            "l0: 0.036205, l1: 0.032182, l2: 0.036413, l3: 0.047560, l4: 0.062317, l5: 0.071055, l6: 0.083131\n",
            "\n",
            "[epoch: 106/1000, batch:     7/   15, ite: 1582] train loss: 0.257121, tar: 0.027615 \n",
            "l0: 0.027634, l1: 0.032476, l2: 0.037133, l3: 0.036124, l4: 0.038122, l5: 0.049019, l6: 0.064882\n",
            "\n",
            "[epoch: 106/1000, batch:     8/   15, ite: 1583] train loss: 0.257461, tar: 0.027615 \n",
            "l0: 0.042827, l1: 0.043115, l2: 0.051915, l3: 0.055243, l4: 0.063640, l5: 0.057138, l6: 0.069339\n",
            "\n",
            "[epoch: 106/1000, batch:     9/   15, ite: 1584] train loss: 0.258958, tar: 0.027796 \n",
            "l0: 0.034273, l1: 0.034645, l2: 0.039988, l3: 0.041674, l4: 0.046885, l5: 0.047638, l6: 0.062463\n",
            "\n",
            "[epoch: 106/1000, batch:    10/   15, ite: 1585] train loss: 0.259530, tar: 0.027872 \n",
            "l0: 0.018111, l1: 0.019468, l2: 0.019318, l3: 0.023939, l4: 0.027017, l5: 0.032427, l6: 0.052154\n",
            "\n",
            "[epoch: 106/1000, batch:    11/   15, ite: 1586] train loss: 0.258750, tar: 0.027759 \n",
            "l0: 0.027431, l1: 0.026664, l2: 0.033442, l3: 0.038496, l4: 0.034838, l5: 0.041784, l6: 0.056275\n",
            "\n",
            "[epoch: 106/1000, batch:    12/   15, ite: 1587] train loss: 0.258752, tar: 0.027755 \n",
            "l0: 0.023242, l1: 0.022699, l2: 0.023926, l3: 0.027307, l4: 0.030532, l5: 0.031400, l6: 0.040305\n",
            "\n",
            "[epoch: 106/1000, batch:    13/   15, ite: 1588] train loss: 0.258078, tar: 0.027704 \n",
            "l0: 0.022656, l1: 0.027518, l2: 0.024007, l3: 0.027019, l4: 0.031788, l5: 0.037605, l6: 0.046159\n",
            "\n",
            "[epoch: 106/1000, batch:    14/   15, ite: 1589] train loss: 0.257614, tar: 0.027647 \n",
            "l0: 0.025624, l1: 0.029470, l2: 0.029334, l3: 0.034637, l4: 0.029387, l5: 0.035585, l6: 0.055229\n",
            "\n",
            "[epoch: 106/1000, batch:    15/   15, ite: 1590] train loss: 0.257410, tar: 0.027624 \n",
            "l0: 0.021605, l1: 0.021083, l2: 0.024822, l3: 0.027468, l4: 0.034043, l5: 0.044411, l6: 0.053108\n",
            "\n",
            "[epoch: 107/1000, batch:     1/   15, ite: 1591] train loss: 0.257070, tar: 0.027558 \n",
            "l0: 0.041517, l1: 0.043657, l2: 0.046446, l3: 0.050242, l4: 0.052961, l5: 0.048448, l6: 0.063619\n",
            "\n",
            "[epoch: 107/1000, batch:     2/   15, ite: 1592] train loss: 0.258047, tar: 0.027710 \n",
            "l0: 0.029162, l1: 0.031386, l2: 0.032240, l3: 0.040402, l4: 0.046780, l5: 0.048182, l6: 0.056989\n",
            "\n",
            "[epoch: 107/1000, batch:     3/   15, ite: 1593] train loss: 0.258338, tar: 0.027726 \n",
            "l0: 0.057710, l1: 0.050326, l2: 0.055555, l3: 0.065650, l4: 0.080165, l5: 0.099000, l6: 0.081245\n",
            "\n",
            "[epoch: 107/1000, batch:     4/   15, ite: 1594] train loss: 0.260799, tar: 0.028045 \n",
            "l0: 0.031596, l1: 0.037172, l2: 0.037808, l3: 0.046057, l4: 0.054914, l5: 0.051837, l6: 0.076833\n",
            "\n",
            "[epoch: 107/1000, batch:     5/   15, ite: 1595] train loss: 0.261593, tar: 0.028082 \n",
            "l0: 0.019253, l1: 0.018182, l2: 0.023492, l3: 0.025357, l4: 0.031057, l5: 0.042940, l6: 0.062141\n",
            "\n",
            "[epoch: 107/1000, batch:     6/   15, ite: 1596] train loss: 0.261185, tar: 0.027990 \n",
            "l0: 0.026670, l1: 0.027205, l2: 0.030779, l3: 0.031816, l4: 0.040144, l5: 0.051419, l6: 0.073596\n",
            "\n",
            "[epoch: 107/1000, batch:     7/   15, ite: 1597] train loss: 0.261396, tar: 0.027976 \n",
            "l0: 0.022425, l1: 0.020487, l2: 0.027411, l3: 0.030891, l4: 0.034554, l5: 0.042438, l6: 0.049967\n",
            "\n",
            "[epoch: 107/1000, batch:     8/   15, ite: 1598] train loss: 0.261057, tar: 0.027920 \n",
            "l0: 0.018095, l1: 0.017253, l2: 0.020469, l3: 0.024652, l4: 0.032644, l5: 0.045986, l6: 0.060006\n",
            "\n",
            "[epoch: 107/1000, batch:     9/   15, ite: 1599] train loss: 0.260633, tar: 0.027820 \n",
            "l0: 0.021122, l1: 0.023113, l2: 0.023753, l3: 0.022140, l4: 0.021887, l5: 0.031665, l6: 0.042377\n",
            "\n",
            "[epoch: 107/1000, batch:    10/   15, ite: 1600] train loss: 0.259887, tar: 0.027754 \n",
            "l0: 0.023472, l1: 0.023187, l2: 0.024187, l3: 0.027701, l4: 0.031226, l5: 0.036731, l6: 0.054234\n",
            "\n",
            "[epoch: 107/1000, batch:    11/   15, ite: 1601] train loss: 0.220737, tar: 0.023472 \n",
            "l0: 0.025814, l1: 0.023224, l2: 0.026942, l3: 0.039251, l4: 0.043451, l5: 0.047280, l6: 0.059184\n",
            "\n",
            "[epoch: 107/1000, batch:    12/   15, ite: 1602] train loss: 0.242942, tar: 0.024643 \n",
            "l0: 0.026224, l1: 0.028138, l2: 0.032481, l3: 0.036690, l4: 0.041245, l5: 0.055874, l6: 0.066891\n",
            "\n",
            "[epoch: 107/1000, batch:    13/   15, ite: 1603] train loss: 0.257809, tar: 0.025170 \n",
            "l0: 0.024702, l1: 0.020106, l2: 0.026914, l3: 0.033712, l4: 0.039367, l5: 0.054810, l6: 0.070414\n",
            "\n",
            "[epoch: 107/1000, batch:    14/   15, ite: 1604] train loss: 0.260863, tar: 0.025053 \n",
            "l0: 0.023151, l1: 0.024530, l2: 0.026359, l3: 0.026897, l4: 0.029610, l5: 0.038856, l6: 0.055647\n",
            "\n",
            "[epoch: 107/1000, batch:    15/   15, ite: 1605] train loss: 0.253701, tar: 0.024672 \n",
            "l0: 0.016395, l1: 0.014403, l2: 0.017216, l3: 0.022027, l4: 0.024815, l5: 0.031293, l6: 0.051859\n",
            "\n",
            "[epoch: 108/1000, batch:     1/   15, ite: 1606] train loss: 0.241085, tar: 0.023293 \n",
            "l0: 0.028202, l1: 0.031989, l2: 0.031914, l3: 0.036858, l4: 0.033718, l5: 0.035540, l6: 0.059905\n",
            "\n",
            "[epoch: 108/1000, batch:     2/   15, ite: 1607] train loss: 0.243520, tar: 0.023994 \n",
            "l0: 0.031201, l1: 0.032934, l2: 0.033910, l3: 0.036384, l4: 0.040825, l5: 0.055082, l6: 0.058405\n",
            "\n",
            "[epoch: 108/1000, batch:     3/   15, ite: 1608] train loss: 0.249172, tar: 0.024895 \n",
            "l0: 0.023572, l1: 0.021451, l2: 0.027105, l3: 0.032702, l4: 0.034289, l5: 0.039278, l6: 0.054573\n",
            "\n",
            "[epoch: 108/1000, batch:     4/   15, ite: 1609] train loss: 0.247372, tar: 0.024748 \n",
            "l0: 0.024184, l1: 0.024536, l2: 0.026537, l3: 0.028671, l4: 0.033256, l5: 0.044703, l6: 0.066722\n",
            "\n",
            "[epoch: 108/1000, batch:     5/   15, ite: 1610] train loss: 0.247496, tar: 0.024692 \n",
            "l0: 0.025689, l1: 0.025932, l2: 0.027129, l3: 0.031399, l4: 0.034567, l5: 0.036184, l6: 0.050611\n",
            "\n",
            "[epoch: 108/1000, batch:     6/   15, ite: 1611] train loss: 0.246043, tar: 0.024782 \n",
            "l0: 0.020896, l1: 0.020472, l2: 0.024895, l3: 0.029515, l4: 0.030173, l5: 0.035738, l6: 0.050484\n",
            "\n",
            "[epoch: 108/1000, batch:     7/   15, ite: 1612] train loss: 0.243220, tar: 0.024458 \n",
            "l0: 0.032213, l1: 0.034737, l2: 0.030171, l3: 0.034901, l4: 0.033276, l5: 0.035712, l6: 0.049187\n",
            "\n",
            "[epoch: 108/1000, batch:     8/   15, ite: 1613] train loss: 0.243757, tar: 0.025055 \n",
            "l0: 0.022793, l1: 0.024484, l2: 0.022411, l3: 0.024294, l4: 0.025278, l5: 0.031429, l6: 0.041402\n",
            "\n",
            "[epoch: 108/1000, batch:     9/   15, ite: 1614] train loss: 0.240067, tar: 0.024893 \n",
            "l0: 0.035084, l1: 0.034250, l2: 0.039105, l3: 0.050086, l4: 0.055780, l5: 0.059505, l6: 0.066010\n",
            "\n",
            "[epoch: 108/1000, batch:    10/   15, ite: 1615] train loss: 0.246717, tar: 0.025573 \n",
            "l0: 0.034220, l1: 0.045998, l2: 0.042160, l3: 0.035780, l4: 0.034319, l5: 0.032569, l6: 0.041797\n",
            "\n",
            "[epoch: 108/1000, batch:    11/   15, ite: 1616] train loss: 0.247975, tar: 0.026113 \n",
            "l0: 0.043557, l1: 0.043347, l2: 0.043619, l3: 0.046487, l4: 0.051594, l5: 0.052435, l6: 0.057587\n",
            "\n",
            "[epoch: 108/1000, batch:    12/   15, ite: 1617] train loss: 0.253307, tar: 0.027139 \n",
            "l0: 0.034567, l1: 0.033008, l2: 0.035036, l3: 0.040879, l4: 0.043578, l5: 0.045740, l6: 0.058579\n",
            "\n",
            "[epoch: 108/1000, batch:    13/   15, ite: 1618] train loss: 0.255422, tar: 0.027552 \n",
            "l0: 0.035381, l1: 0.033281, l2: 0.037562, l3: 0.036210, l4: 0.048908, l5: 0.063848, l6: 0.076218\n",
            "\n",
            "[epoch: 108/1000, batch:    14/   15, ite: 1619] train loss: 0.259422, tar: 0.027964 \n",
            "l0: 0.037937, l1: 0.040554, l2: 0.041393, l3: 0.036940, l4: 0.046301, l5: 0.050700, l6: 0.069616\n",
            "\n",
            "[epoch: 108/1000, batch:    15/   15, ite: 1620] train loss: 0.262623, tar: 0.028463 \n",
            "l0: 0.035958, l1: 0.042564, l2: 0.034994, l3: 0.032053, l4: 0.033476, l5: 0.044554, l6: 0.059384\n",
            "\n",
            "[epoch: 109/1000, batch:     1/   15, ite: 1621] train loss: 0.263592, tar: 0.028820 \n",
            "l0: 0.024741, l1: 0.025164, l2: 0.022445, l3: 0.027976, l4: 0.034570, l5: 0.052275, l6: 0.084298\n",
            "\n",
            "[epoch: 109/1000, batch:     2/   15, ite: 1622] train loss: 0.263950, tar: 0.028634 \n",
            "l0: 0.041774, l1: 0.042200, l2: 0.044814, l3: 0.045997, l4: 0.051161, l5: 0.086021, l6: 0.110625\n",
            "\n",
            "[epoch: 109/1000, batch:     3/   15, ite: 1623] train loss: 0.270848, tar: 0.029205 \n",
            "l0: 0.027975, l1: 0.027361, l2: 0.030648, l3: 0.040021, l4: 0.040293, l5: 0.045349, l6: 0.056919\n",
            "\n",
            "[epoch: 109/1000, batch:     4/   15, ite: 1624] train loss: 0.270753, tar: 0.029154 \n",
            "l0: 0.016512, l1: 0.015500, l2: 0.017981, l3: 0.023399, l4: 0.025223, l5: 0.034915, l6: 0.052740\n",
            "\n",
            "[epoch: 109/1000, batch:     5/   15, ite: 1625] train loss: 0.267374, tar: 0.028648 \n",
            "l0: 0.030868, l1: 0.034088, l2: 0.032138, l3: 0.035125, l4: 0.034759, l5: 0.046329, l6: 0.068752\n",
            "\n",
            "[epoch: 109/1000, batch:     6/   15, ite: 1626] train loss: 0.267938, tar: 0.028734 \n",
            "l0: 0.028590, l1: 0.030337, l2: 0.030224, l3: 0.034693, l4: 0.035811, l5: 0.039909, l6: 0.065833\n",
            "\n",
            "[epoch: 109/1000, batch:     7/   15, ite: 1627] train loss: 0.267844, tar: 0.028729 \n",
            "l0: 0.024972, l1: 0.024186, l2: 0.032756, l3: 0.031031, l4: 0.032648, l5: 0.036112, l6: 0.047148\n",
            "\n",
            "[epoch: 109/1000, batch:     8/   15, ite: 1628] train loss: 0.266452, tar: 0.028594 \n",
            "l0: 0.017525, l1: 0.019685, l2: 0.022636, l3: 0.021642, l4: 0.025883, l5: 0.034720, l6: 0.047243\n",
            "\n",
            "[epoch: 109/1000, batch:     9/   15, ite: 1629] train loss: 0.263792, tar: 0.028213 \n",
            "l0: 0.031843, l1: 0.029652, l2: 0.035470, l3: 0.042809, l4: 0.047400, l5: 0.052038, l6: 0.068779\n",
            "\n",
            "[epoch: 109/1000, batch:    10/   15, ite: 1630] train loss: 0.265266, tar: 0.028334 \n",
            "l0: 0.027278, l1: 0.024369, l2: 0.026346, l3: 0.030239, l4: 0.039270, l5: 0.049077, l6: 0.070201\n",
            "\n",
            "[epoch: 109/1000, batch:    11/   15, ite: 1631] train loss: 0.265315, tar: 0.028300 \n",
            "l0: 0.047273, l1: 0.045243, l2: 0.041911, l3: 0.058691, l4: 0.072025, l5: 0.060436, l6: 0.070383\n",
            "\n",
            "[epoch: 109/1000, batch:    12/   15, ite: 1632] train loss: 0.269397, tar: 0.028893 \n",
            "l0: 0.038617, l1: 0.039793, l2: 0.037394, l3: 0.049386, l4: 0.059038, l5: 0.063629, l6: 0.090399\n",
            "\n",
            "[epoch: 109/1000, batch:    13/   15, ite: 1633] train loss: 0.272696, tar: 0.029187 \n",
            "l0: 0.030579, l1: 0.032164, l2: 0.033870, l3: 0.039530, l4: 0.042223, l5: 0.046211, l6: 0.061554\n",
            "\n",
            "[epoch: 109/1000, batch:    14/   15, ite: 1634] train loss: 0.273091, tar: 0.029228 \n",
            "l0: 0.023892, l1: 0.024799, l2: 0.027341, l3: 0.027879, l4: 0.027595, l5: 0.035361, l6: 0.049434\n",
            "\n",
            "[epoch: 109/1000, batch:    15/   15, ite: 1635] train loss: 0.271469, tar: 0.029076 \n",
            "l0: 0.027485, l1: 0.026769, l2: 0.034858, l3: 0.034529, l4: 0.038365, l5: 0.041845, l6: 0.053312\n",
            "\n",
            "[epoch: 110/1000, batch:     1/   15, ite: 1636] train loss: 0.271071, tar: 0.029031 \n",
            "l0: 0.025748, l1: 0.021471, l2: 0.031775, l3: 0.036651, l4: 0.035215, l5: 0.038048, l6: 0.061327\n",
            "\n",
            "[epoch: 110/1000, batch:     2/   15, ite: 1637] train loss: 0.270508, tar: 0.028943 \n",
            "l0: 0.029121, l1: 0.024788, l2: 0.034532, l3: 0.036496, l4: 0.046667, l5: 0.051608, l6: 0.065427\n",
            "\n",
            "[epoch: 110/1000, batch:     3/   15, ite: 1638] train loss: 0.270985, tar: 0.028947 \n",
            "l0: 0.067253, l1: 0.082013, l2: 0.086714, l3: 0.094848, l4: 0.076632, l5: 0.070976, l6: 0.062311\n",
            "\n",
            "[epoch: 110/1000, batch:     4/   15, ite: 1639] train loss: 0.277902, tar: 0.029930 \n",
            "l0: 0.023384, l1: 0.024625, l2: 0.025000, l3: 0.029485, l4: 0.032118, l5: 0.035818, l6: 0.052897\n",
            "\n",
            "[epoch: 110/1000, batch:     5/   15, ite: 1640] train loss: 0.276538, tar: 0.029766 \n",
            "l0: 0.021428, l1: 0.024269, l2: 0.024386, l3: 0.031251, l4: 0.027587, l5: 0.030829, l6: 0.040150\n",
            "\n",
            "[epoch: 110/1000, batch:     6/   15, ite: 1641] train loss: 0.274669, tar: 0.029563 \n",
            "l0: 0.044692, l1: 0.045795, l2: 0.045165, l3: 0.050522, l4: 0.056273, l5: 0.062547, l6: 0.088935\n",
            "\n",
            "[epoch: 110/1000, batch:     7/   15, ite: 1642] train loss: 0.277508, tar: 0.029923 \n",
            "l0: 0.045010, l1: 0.040280, l2: 0.048901, l3: 0.052056, l4: 0.057529, l5: 0.053784, l6: 0.073430\n",
            "\n",
            "[epoch: 110/1000, batch:     8/   15, ite: 1643] train loss: 0.279682, tar: 0.030274 \n",
            "l0: 0.028574, l1: 0.026505, l2: 0.030936, l3: 0.039414, l4: 0.036555, l5: 0.036289, l6: 0.051470\n",
            "\n",
            "[epoch: 110/1000, batch:     9/   15, ite: 1644] train loss: 0.279002, tar: 0.030235 \n",
            "l0: 0.094453, l1: 0.057487, l2: 0.061475, l3: 0.292031, l4: 0.215191, l5: 0.097386, l6: 0.104441\n",
            "\n",
            "[epoch: 110/1000, batch:    10/   15, ite: 1645] train loss: 0.293301, tar: 0.031662 \n",
            "l0: 0.018366, l1: 0.017389, l2: 0.024639, l3: 0.023777, l4: 0.025820, l5: 0.037082, l6: 0.083331\n",
            "\n",
            "[epoch: 110/1000, batch:    11/   15, ite: 1646] train loss: 0.291933, tar: 0.031373 \n",
            "l0: 0.022413, l1: 0.025707, l2: 0.028752, l3: 0.034101, l4: 0.029489, l5: 0.033238, l6: 0.043875\n",
            "\n",
            "[epoch: 110/1000, batch:    12/   15, ite: 1647] train loss: 0.290351, tar: 0.031182 \n",
            "l0: 0.035665, l1: 0.035344, l2: 0.041787, l3: 0.052071, l4: 0.055689, l5: 0.048516, l6: 0.044348\n",
            "\n",
            "[epoch: 110/1000, batch:    13/   15, ite: 1648] train loss: 0.290832, tar: 0.031276 \n",
            "l0: 0.040966, l1: 0.045455, l2: 0.050405, l3: 0.053989, l4: 0.064052, l5: 0.070654, l6: 0.072827\n",
            "\n",
            "[epoch: 110/1000, batch:    14/   15, ite: 1649] train loss: 0.293026, tar: 0.031474 \n",
            "l0: 0.024926, l1: 0.022701, l2: 0.030165, l3: 0.038130, l4: 0.041950, l5: 0.046001, l6: 0.054265\n",
            "\n",
            "[epoch: 110/1000, batch:    15/   15, ite: 1650] train loss: 0.292328, tar: 0.031343 \n",
            "l0: 0.028490, l1: 0.022383, l2: 0.031086, l3: 0.048439, l4: 0.073430, l5: 0.090005, l6: 0.093381\n",
            "\n",
            "[epoch: 111/1000, batch:     1/   15, ite: 1651] train loss: 0.294189, tar: 0.031287 \n",
            "l0: 0.064760, l1: 0.068366, l2: 0.133493, l3: 0.104336, l4: 0.105086, l5: 0.102697, l6: 0.091153\n",
            "\n",
            "[epoch: 111/1000, batch:     2/   15, ite: 1652] train loss: 0.301414, tar: 0.031930 \n",
            "l0: 0.029553, l1: 0.032804, l2: 0.031327, l3: 0.036585, l4: 0.035458, l5: 0.044114, l6: 0.054902\n",
            "\n",
            "[epoch: 111/1000, batch:     3/   15, ite: 1653] train loss: 0.300722, tar: 0.031886 \n",
            "l0: 0.026087, l1: 0.024345, l2: 0.032213, l3: 0.044849, l4: 0.048288, l5: 0.046004, l6: 0.059181\n",
            "\n",
            "[epoch: 111/1000, batch:     4/   15, ite: 1654] train loss: 0.300356, tar: 0.031778 \n",
            "l0: 0.059777, l1: 0.059335, l2: 0.067828, l3: 0.092454, l4: 0.081641, l5: 0.085290, l6: 0.093588\n",
            "\n",
            "[epoch: 111/1000, batch:     5/   15, ite: 1655] train loss: 0.304712, tar: 0.032287 \n",
            "l0: 0.024548, l1: 0.025961, l2: 0.030467, l3: 0.040826, l4: 0.048050, l5: 0.049900, l6: 0.065156\n",
            "\n",
            "[epoch: 111/1000, batch:     6/   15, ite: 1656] train loss: 0.304358, tar: 0.032149 \n",
            "l0: 0.040484, l1: 0.045630, l2: 0.046956, l3: 0.041973, l4: 0.055453, l5: 0.055136, l6: 0.055145\n",
            "\n",
            "[epoch: 111/1000, batch:     7/   15, ite: 1657] train loss: 0.304997, tar: 0.032295 \n",
            "l0: 0.029555, l1: 0.028059, l2: 0.031929, l3: 0.042776, l4: 0.054720, l5: 0.062103, l6: 0.069178\n",
            "\n",
            "[epoch: 111/1000, batch:     8/   15, ite: 1658] train loss: 0.305227, tar: 0.032248 \n",
            "l0: 0.065327, l1: 0.066154, l2: 0.063562, l3: 0.068924, l4: 0.104112, l5: 0.118012, l6: 0.107486\n",
            "\n",
            "[epoch: 111/1000, batch:     9/   15, ite: 1659] train loss: 0.310114, tar: 0.032809 \n",
            "l0: 0.037712, l1: 0.033353, l2: 0.039641, l3: 0.044742, l4: 0.045850, l5: 0.054434, l6: 0.063579\n",
            "\n",
            "[epoch: 111/1000, batch:    10/   15, ite: 1660] train loss: 0.310267, tar: 0.032890 \n",
            "l0: 0.061963, l1: 0.067847, l2: 0.071828, l3: 0.071705, l4: 0.079238, l5: 0.166709, l6: 0.163850\n",
            "\n",
            "[epoch: 111/1000, batch:    11/   15, ite: 1661] train loss: 0.316380, tar: 0.033367 \n",
            "l0: 0.044675, l1: 0.049116, l2: 0.044629, l3: 0.048922, l4: 0.039363, l5: 0.050605, l6: 0.062482\n",
            "\n",
            "[epoch: 111/1000, batch:    12/   15, ite: 1662] train loss: 0.316758, tar: 0.033549 \n",
            "l0: 0.035966, l1: 0.039777, l2: 0.040660, l3: 0.046851, l4: 0.053325, l5: 0.049879, l6: 0.071723\n",
            "\n",
            "[epoch: 111/1000, batch:    13/   15, ite: 1663] train loss: 0.317098, tar: 0.033588 \n",
            "l0: 0.034804, l1: 0.029113, l2: 0.034250, l3: 0.045352, l4: 0.068802, l5: 0.062000, l6: 0.065070\n",
            "\n",
            "[epoch: 111/1000, batch:    14/   15, ite: 1664] train loss: 0.317446, tar: 0.033607 \n",
            "l0: 0.059399, l1: 0.072708, l2: 0.064370, l3: 0.054278, l4: 0.068038, l5: 0.086907, l6: 0.120639\n",
            "\n",
            "[epoch: 111/1000, batch:    15/   15, ite: 1665] train loss: 0.320660, tar: 0.034004 \n",
            "l0: 0.038744, l1: 0.038110, l2: 0.044637, l3: 0.048039, l4: 0.062923, l5: 0.074298, l6: 0.105879\n",
            "\n",
            "[epoch: 112/1000, batch:     1/   15, ite: 1666] train loss: 0.322053, tar: 0.034075 \n",
            "l0: 0.062538, l1: 0.072586, l2: 0.074899, l3: 0.065429, l4: 0.087185, l5: 0.079360, l6: 0.084683\n",
            "\n",
            "[epoch: 112/1000, batch:     2/   15, ite: 1667] train loss: 0.325107, tar: 0.034500 \n",
            "l0: 0.039122, l1: 0.037014, l2: 0.038826, l3: 0.051508, l4: 0.046996, l5: 0.047546, l6: 0.069858\n",
            "\n",
            "[epoch: 112/1000, batch:     3/   15, ite: 1668] train loss: 0.325192, tar: 0.034568 \n",
            "l0: 0.044385, l1: 0.043430, l2: 0.053061, l3: 0.052539, l4: 0.064557, l5: 0.060611, l6: 0.068970\n",
            "\n",
            "[epoch: 112/1000, batch:     4/   15, ite: 1669] train loss: 0.326096, tar: 0.034710 \n",
            "l0: 0.030548, l1: 0.027144, l2: 0.038336, l3: 0.042289, l4: 0.045816, l5: 0.046464, l6: 0.071353\n",
            "\n",
            "[epoch: 112/1000, batch:     5/   15, ite: 1670] train loss: 0.325751, tar: 0.034651 \n",
            "l0: 0.042523, l1: 0.043400, l2: 0.048923, l3: 0.051785, l4: 0.049523, l5: 0.065259, l6: 0.076806\n",
            "\n",
            "[epoch: 112/1000, batch:     6/   15, ite: 1671] train loss: 0.326490, tar: 0.034762 \n",
            "l0: 0.026451, l1: 0.026308, l2: 0.030766, l3: 0.033069, l4: 0.036167, l5: 0.040747, l6: 0.052312\n",
            "\n",
            "[epoch: 112/1000, batch:     7/   15, ite: 1672] train loss: 0.325370, tar: 0.034646 \n",
            "l0: 0.086734, l1: 0.086256, l2: 0.092859, l3: 0.105418, l4: 0.090373, l5: 0.102809, l6: 0.097247\n",
            "\n",
            "[epoch: 112/1000, batch:     8/   15, ite: 1673] train loss: 0.329977, tar: 0.035360 \n",
            "l0: 0.033418, l1: 0.033044, l2: 0.038019, l3: 0.045985, l4: 0.061401, l5: 0.076455, l6: 0.084286\n",
            "\n",
            "[epoch: 112/1000, batch:     9/   15, ite: 1674] train loss: 0.330553, tar: 0.035334 \n",
            "l0: 0.032393, l1: 0.033376, l2: 0.040145, l3: 0.036005, l4: 0.039262, l5: 0.050431, l6: 0.073107\n",
            "\n",
            "[epoch: 112/1000, batch:    10/   15, ite: 1675] train loss: 0.330208, tar: 0.035295 \n",
            "l0: 0.030786, l1: 0.030664, l2: 0.041483, l3: 0.034076, l4: 0.037440, l5: 0.042607, l6: 0.073692\n",
            "\n",
            "[epoch: 112/1000, batch:    11/   15, ite: 1676] train loss: 0.329689, tar: 0.035235 \n",
            "l0: 0.104778, l1: 0.089011, l2: 0.109946, l3: 0.112962, l4: 0.155043, l5: 0.188739, l6: 0.170973\n",
            "\n",
            "[epoch: 112/1000, batch:    12/   15, ite: 1677] train loss: 0.337504, tar: 0.036138 \n",
            "l0: 0.040604, l1: 0.042237, l2: 0.049124, l3: 0.051445, l4: 0.047042, l5: 0.052615, l6: 0.055633\n",
            "\n",
            "[epoch: 112/1000, batch:    13/   15, ite: 1678] train loss: 0.337520, tar: 0.036196 \n",
            "l0: 0.062594, l1: 0.061725, l2: 0.066054, l3: 0.094498, l4: 0.101911, l5: 0.091872, l6: 0.104713\n",
            "\n",
            "[epoch: 112/1000, batch:    14/   15, ite: 1679] train loss: 0.340632, tar: 0.036530 \n",
            "l0: 0.036109, l1: 0.030555, l2: 0.041598, l3: 0.047143, l4: 0.057648, l5: 0.079945, l6: 0.078865\n",
            "\n",
            "[epoch: 112/1000, batch:    15/   15, ite: 1680] train loss: 0.341022, tar: 0.036524 \n",
            "l0: 0.044658, l1: 0.050496, l2: 0.054558, l3: 0.042291, l4: 0.060640, l5: 0.103253, l6: 0.106460\n",
            "\n",
            "[epoch: 113/1000, batch:     1/   15, ite: 1681] train loss: 0.342520, tar: 0.036625 \n",
            "l0: 0.047353, l1: 0.049159, l2: 0.056311, l3: 0.054514, l4: 0.058033, l5: 0.054542, l6: 0.047354\n",
            "\n",
            "[epoch: 113/1000, batch:     2/   15, ite: 1682] train loss: 0.342822, tar: 0.036756 \n",
            "l0: 0.048983, l1: 0.036462, l2: 0.050416, l3: 0.066267, l4: 0.096326, l5: 0.113668, l6: 0.136512\n",
            "\n",
            "[epoch: 113/1000, batch:     3/   15, ite: 1683] train loss: 0.345301, tar: 0.036903 \n",
            "l0: 0.142430, l1: 0.135450, l2: 0.176661, l3: 0.199676, l4: 0.248712, l5: 0.162287, l6: 0.271921\n",
            "\n",
            "[epoch: 113/1000, batch:     4/   15, ite: 1684] train loss: 0.357109, tar: 0.038159 \n",
            "l0: 0.025957, l1: 0.025984, l2: 0.025115, l3: 0.038941, l4: 0.052719, l5: 0.075970, l6: 0.096719\n",
            "\n",
            "[epoch: 113/1000, batch:     5/   15, ite: 1685] train loss: 0.356924, tar: 0.038016 \n",
            "l0: 0.036142, l1: 0.037892, l2: 0.039853, l3: 0.058479, l4: 0.055012, l5: 0.053800, l6: 0.073546\n",
            "\n",
            "[epoch: 113/1000, batch:     6/   15, ite: 1686] train loss: 0.356899, tar: 0.037994 \n",
            "l0: 0.046583, l1: 0.047342, l2: 0.051926, l3: 0.054484, l4: 0.053915, l5: 0.074073, l6: 0.076642\n",
            "\n",
            "[epoch: 113/1000, batch:     7/   15, ite: 1687] train loss: 0.357451, tar: 0.038093 \n",
            "l0: 0.234104, l1: 0.197806, l2: 0.326729, l3: 0.297864, l4: 0.212535, l5: 0.169642, l6: 0.136086\n",
            "\n",
            "[epoch: 113/1000, batch:     8/   15, ite: 1688] train loss: 0.371284, tar: 0.040320 \n",
            "l0: 0.039483, l1: 0.035977, l2: 0.039131, l3: 0.066320, l4: 0.094066, l5: 0.073998, l6: 0.088915\n",
            "\n",
            "[epoch: 113/1000, batch:     9/   15, ite: 1689] train loss: 0.372033, tar: 0.040311 \n",
            "l0: 0.154659, l1: 0.105156, l2: 0.163863, l3: 0.257043, l4: 0.187249, l5: 0.293270, l6: 0.364535\n",
            "\n",
            "[epoch: 113/1000, batch:    10/   15, ite: 1690] train loss: 0.384852, tar: 0.041581 \n",
            "l0: 0.061213, l1: 0.071277, l2: 0.060979, l3: 0.075701, l4: 0.085444, l5: 0.113634, l6: 0.087058\n",
            "\n",
            "[epoch: 113/1000, batch:    11/   15, ite: 1691] train loss: 0.386725, tar: 0.041797 \n",
            "l0: 0.054279, l1: 0.062798, l2: 0.050411, l3: 0.055274, l4: 0.105725, l5: 0.164461, l6: 0.087374\n",
            "\n",
            "[epoch: 113/1000, batch:    12/   15, ite: 1692] train loss: 0.388829, tar: 0.041933 \n",
            "l0: 0.083278, l1: 0.100740, l2: 0.085157, l3: 0.100976, l4: 0.134272, l5: 0.160275, l6: 0.091513\n",
            "\n",
            "[epoch: 113/1000, batch:    13/   15, ite: 1693] train loss: 0.392780, tar: 0.042377 \n",
            "l0: 0.162205, l1: 0.213512, l2: 0.149648, l3: 0.129715, l4: 0.154738, l5: 0.229120, l6: 0.207947\n",
            "\n",
            "[epoch: 113/1000, batch:    14/   15, ite: 1694] train loss: 0.401866, tar: 0.043652 \n",
            "l0: 0.053154, l1: 0.057386, l2: 0.041836, l3: 0.065411, l4: 0.071136, l5: 0.095827, l6: 0.086267\n",
            "\n",
            "[epoch: 113/1000, batch:    15/   15, ite: 1695] train loss: 0.402594, tar: 0.043752 \n",
            "l0: 0.035968, l1: 0.033249, l2: 0.034033, l3: 0.053382, l4: 0.057659, l5: 0.088493, l6: 0.074278\n",
            "\n",
            "[epoch: 114/1000, batch:     1/   15, ite: 1696] train loss: 0.402328, tar: 0.043671 \n",
            "l0: 0.107685, l1: 0.131877, l2: 0.138796, l3: 0.142247, l4: 0.112951, l5: 0.139363, l6: 0.148767\n",
            "\n",
            "[epoch: 114/1000, batch:     2/   15, ite: 1697] train loss: 0.407682, tar: 0.044331 \n",
            "l0: 0.090541, l1: 0.162527, l2: 0.111821, l3: 0.091261, l4: 0.098525, l5: 0.084821, l6: 0.077411\n",
            "\n",
            "[epoch: 114/1000, batch:     3/   15, ite: 1698] train loss: 0.410837, tar: 0.044802 \n",
            "l0: 0.081946, l1: 0.092682, l2: 0.068835, l3: 0.095297, l4: 0.109464, l5: 0.100501, l6: 0.081673\n",
            "\n",
            "[epoch: 114/1000, batch:     4/   15, ite: 1699] train loss: 0.413055, tar: 0.045178 \n",
            "l0: 0.141309, l1: 0.130362, l2: 0.177340, l3: 0.217684, l4: 0.267704, l5: 0.294143, l6: 0.304065\n",
            "\n",
            "[epoch: 114/1000, batch:     5/   15, ite: 1700] train loss: 0.424251, tar: 0.046139 \n",
            "l0: 0.289619, l1: 0.236414, l2: 0.381307, l3: 0.444372, l4: 0.290132, l5: 0.379878, l6: 0.353336\n",
            "\n",
            "[epoch: 114/1000, batch:     6/   15, ite: 1701] train loss: 2.375058, tar: 0.289619 \n",
            "l0: 0.076729, l1: 0.112771, l2: 0.086836, l3: 0.097026, l4: 0.101011, l5: 0.056596, l6: 0.072707\n",
            "\n",
            "[epoch: 114/1000, batch:     7/   15, ite: 1702] train loss: 1.489367, tar: 0.183174 \n",
            "l0: 0.083649, l1: 0.103108, l2: 0.077398, l3: 0.130849, l4: 0.101850, l5: 0.099938, l6: 0.112303\n",
            "\n",
            "[epoch: 114/1000, batch:     8/   15, ite: 1703] train loss: 1.229277, tar: 0.149999 \n",
            "l0: 0.142870, l1: 0.178390, l2: 0.153783, l3: 0.155242, l4: 0.180684, l5: 0.183180, l6: 0.159499\n",
            "\n",
            "[epoch: 114/1000, batch:     9/   15, ite: 1704] train loss: 1.210370, tar: 0.148217 \n",
            "l0: 0.105865, l1: 0.128510, l2: 0.140338, l3: 0.137733, l4: 0.116488, l5: 0.189624, l6: 0.191640\n",
            "\n",
            "[epoch: 114/1000, batch:    10/   15, ite: 1705] train loss: 1.170335, tar: 0.139747 \n",
            "l0: 0.117146, l1: 0.154897, l2: 0.292137, l3: 0.131237, l4: 0.156175, l5: 0.157369, l6: 0.096465\n",
            "\n",
            "[epoch: 114/1000, batch:    11/   15, ite: 1706] train loss: 1.159517, tar: 0.135980 \n",
            "l0: 0.119032, l1: 0.120516, l2: 0.181825, l3: 0.138946, l4: 0.137323, l5: 0.210959, l6: 0.263248\n",
            "\n",
            "[epoch: 114/1000, batch:    12/   15, ite: 1707] train loss: 1.161279, tar: 0.133559 \n",
            "l0: 0.099518, l1: 0.119696, l2: 0.106539, l3: 0.115838, l4: 0.088417, l5: 0.136147, l6: 0.120105\n",
            "\n",
            "[epoch: 114/1000, batch:    13/   15, ite: 1708] train loss: 1.114401, tar: 0.129304 \n",
            "l0: 0.153195, l1: 0.255431, l2: 0.228446, l3: 0.159122, l4: 0.161991, l5: 0.162113, l6: 0.143874\n",
            "\n",
            "[epoch: 114/1000, batch:    14/   15, ite: 1709] train loss: 1.131043, tar: 0.131958 \n",
            "l0: 0.198886, l1: 0.296026, l2: 0.228564, l3: 0.169135, l4: 0.157492, l5: 0.226657, l6: 0.175020\n",
            "\n",
            "[epoch: 114/1000, batch:    15/   15, ite: 1710] train loss: 1.163116, tar: 0.138651 \n",
            "l0: 0.073501, l1: 0.071360, l2: 0.085956, l3: 0.086234, l4: 0.073877, l5: 0.099733, l6: 0.085681\n",
            "\n",
            "[epoch: 115/1000, batch:     1/   15, ite: 1711] train loss: 1.109773, tar: 0.132728 \n",
            "l0: 0.095861, l1: 0.102197, l2: 0.102262, l3: 0.095095, l4: 0.105653, l5: 0.121264, l6: 0.095894\n",
            "\n",
            "[epoch: 115/1000, batch:     2/   15, ite: 1712] train loss: 1.077144, tar: 0.129656 \n",
            "l0: 0.133672, l1: 0.138838, l2: 0.124379, l3: 0.146110, l4: 0.126431, l5: 0.188468, l6: 0.228916\n",
            "\n",
            "[epoch: 115/1000, batch:     3/   15, ite: 1713] train loss: 1.077888, tar: 0.129965 \n",
            "l0: 0.075211, l1: 0.065590, l2: 0.100537, l3: 0.132999, l4: 0.075664, l5: 0.075171, l6: 0.088407\n",
            "\n",
            "[epoch: 115/1000, batch:     4/   15, ite: 1714] train loss: 1.044723, tar: 0.126054 \n",
            "l0: 0.070725, l1: 0.087057, l2: 0.062074, l3: 0.069758, l4: 0.069780, l5: 0.091780, l6: 0.118731\n",
            "\n",
            "[epoch: 115/1000, batch:     5/   15, ite: 1715] train loss: 1.013069, tar: 0.122365 \n",
            "l0: 0.099590, l1: 0.115118, l2: 0.149672, l3: 0.150964, l4: 0.087985, l5: 0.099971, l6: 0.148761\n",
            "\n",
            "[epoch: 115/1000, batch:     6/   15, ite: 1716] train loss: 1.003006, tar: 0.120942 \n",
            "l0: 0.059117, l1: 0.058039, l2: 0.090168, l3: 0.093408, l4: 0.058260, l5: 0.082402, l6: 0.104509\n",
            "\n",
            "[epoch: 115/1000, batch:     7/   15, ite: 1717] train loss: 0.976117, tar: 0.117305 \n",
            "l0: 0.072506, l1: 0.084017, l2: 0.125411, l3: 0.109941, l4: 0.081663, l5: 0.085431, l6: 0.117803\n",
            "\n",
            "[epoch: 115/1000, batch:     8/   15, ite: 1718] train loss: 0.959487, tar: 0.114816 \n",
            "l0: 0.068125, l1: 0.077465, l2: 0.071943, l3: 0.101818, l4: 0.086336, l5: 0.089738, l6: 0.095996\n",
            "\n",
            "[epoch: 115/1000, batch:     9/   15, ite: 1719] train loss: 0.940115, tar: 0.112359 \n",
            "l0: 0.206728, l1: 0.227820, l2: 0.204035, l3: 0.219924, l4: 0.223103, l5: 0.273581, l6: 0.180012\n",
            "\n",
            "[epoch: 115/1000, batch:    10/   15, ite: 1720] train loss: 0.969870, tar: 0.117077 \n",
            "l0: 0.154735, l1: 0.174755, l2: 0.177099, l3: 0.200808, l4: 0.128215, l5: 0.165868, l6: 0.140576\n",
            "\n",
            "[epoch: 115/1000, batch:    11/   15, ite: 1721] train loss: 0.978069, tar: 0.118871 \n",
            "l0: 0.058063, l1: 0.087606, l2: 0.090233, l3: 0.070988, l4: 0.063015, l5: 0.070070, l6: 0.071061\n",
            "\n",
            "[epoch: 115/1000, batch:    12/   15, ite: 1722] train loss: 0.956840, tar: 0.116107 \n",
            "l0: 0.207984, l1: 0.224751, l2: 0.225464, l3: 0.251424, l4: 0.154137, l5: 0.240209, l6: 0.260226\n",
            "\n",
            "[epoch: 115/1000, batch:    13/   15, ite: 1723] train loss: 0.983247, tar: 0.120101 \n",
            "l0: 0.086957, l1: 0.071561, l2: 0.127429, l3: 0.112494, l4: 0.151919, l5: 0.145974, l6: 0.167639\n",
            "\n",
            "[epoch: 115/1000, batch:    14/   15, ite: 1724] train loss: 0.978277, tar: 0.118720 \n",
            "l0: 0.084784, l1: 0.079778, l2: 0.078738, l3: 0.088446, l4: 0.085897, l5: 0.130759, l6: 0.128959\n",
            "\n",
            "[epoch: 115/1000, batch:    15/   15, ite: 1725] train loss: 0.966240, tar: 0.117363 \n",
            "l0: 0.123809, l1: 0.124241, l2: 0.133624, l3: 0.102807, l4: 0.137802, l5: 0.182048, l6: 0.238217\n",
            "\n",
            "[epoch: 116/1000, batch:     1/   15, ite: 1726] train loss: 0.969175, tar: 0.117611 \n",
            "l0: 0.067778, l1: 0.074788, l2: 0.086455, l3: 0.080355, l4: 0.073210, l5: 0.073103, l6: 0.098084\n",
            "\n",
            "[epoch: 116/1000, batch:     2/   15, ite: 1727] train loss: 0.953790, tar: 0.115765 \n",
            "l0: 0.077132, l1: 0.086300, l2: 0.098292, l3: 0.087984, l4: 0.072032, l5: 0.068933, l6: 0.084669\n",
            "\n",
            "[epoch: 116/1000, batch:     3/   15, ite: 1728] train loss: 0.940274, tar: 0.114385 \n",
            "l0: 0.196121, l1: 0.336722, l2: 0.132246, l3: 0.181481, l4: 0.190390, l5: 0.169224, l6: 0.119320\n",
            "\n",
            "[epoch: 116/1000, batch:     4/   15, ite: 1729] train loss: 0.953558, tar: 0.117204 \n",
            "l0: 0.065377, l1: 0.063106, l2: 0.065810, l3: 0.070221, l4: 0.093998, l5: 0.126186, l6: 0.088206\n",
            "\n",
            "[epoch: 116/1000, batch:     5/   15, ite: 1730] train loss: 0.940869, tar: 0.115476 \n",
            "l0: 0.462006, l1: 0.665953, l2: 0.456621, l3: 0.330004, l4: 0.298293, l5: 0.267790, l6: 0.172118\n",
            "\n",
            "[epoch: 116/1000, batch:     6/   15, ite: 1731] train loss: 0.996093, tar: 0.126655 \n",
            "l0: 0.072312, l1: 0.097232, l2: 0.090851, l3: 0.078994, l4: 0.081575, l5: 0.094940, l6: 0.119590\n",
            "\n",
            "[epoch: 116/1000, batch:     7/   15, ite: 1732] train loss: 0.984824, tar: 0.124956 \n",
            "l0: 0.078004, l1: 0.093541, l2: 0.101500, l3: 0.093617, l4: 0.105300, l5: 0.121259, l6: 0.166853\n",
            "\n",
            "[epoch: 116/1000, batch:     8/   15, ite: 1733] train loss: 0.978013, tar: 0.123534 \n",
            "l0: 0.106756, l1: 0.105559, l2: 0.128135, l3: 0.161241, l4: 0.152019, l5: 0.176964, l6: 0.185615\n",
            "\n",
            "[epoch: 116/1000, batch:     9/   15, ite: 1734] train loss: 0.979139, tar: 0.123040 \n",
            "l0: 0.153278, l1: 0.099412, l2: 0.184691, l3: 0.301015, l4: 0.424612, l5: 0.360042, l6: 0.292031\n",
            "\n",
            "[epoch: 116/1000, batch:    10/   15, ite: 1735] train loss: 1.003023, tar: 0.123904 \n",
            "l0: 0.092803, l1: 0.102742, l2: 0.099324, l3: 0.116824, l4: 0.103649, l5: 0.156971, l6: 0.114494\n",
            "\n",
            "[epoch: 116/1000, batch:    11/   15, ite: 1736] train loss: 0.997017, tar: 0.123040 \n",
            "l0: 0.160110, l1: 0.168800, l2: 0.176240, l3: 0.195136, l4: 0.225304, l5: 0.254426, l6: 0.206062\n",
            "\n",
            "[epoch: 116/1000, batch:    12/   15, ite: 1737] train loss: 1.007532, tar: 0.124042 \n",
            "l0: 0.057940, l1: 0.049630, l2: 0.054042, l3: 0.074326, l4: 0.100284, l5: 0.102203, l6: 0.133736\n",
            "\n",
            "[epoch: 116/1000, batch:    13/   15, ite: 1738] train loss: 0.996075, tar: 0.122303 \n",
            "l0: 0.095658, l1: 0.090817, l2: 0.095621, l3: 0.116108, l4: 0.128537, l5: 0.157053, l6: 0.159636\n",
            "\n",
            "[epoch: 116/1000, batch:    14/   15, ite: 1739] train loss: 0.992161, tar: 0.121619 \n",
            "l0: 0.102391, l1: 0.121708, l2: 0.102482, l3: 0.111233, l4: 0.097469, l5: 0.105000, l6: 0.152644\n",
            "\n",
            "[epoch: 116/1000, batch:    15/   15, ite: 1740] train loss: 0.987180, tar: 0.121139 \n",
            "l0: 0.128426, l1: 0.143618, l2: 0.126080, l3: 0.135316, l4: 0.131174, l5: 0.134109, l6: 0.140372\n",
            "\n",
            "[epoch: 117/1000, batch:     1/   15, ite: 1741] train loss: 0.986007, tar: 0.121316 \n",
            "l0: 0.065159, l1: 0.070748, l2: 0.064250, l3: 0.065791, l4: 0.106676, l5: 0.112711, l6: 0.090517\n",
            "\n",
            "[epoch: 117/1000, batch:     2/   15, ite: 1742] train loss: 0.976242, tar: 0.119979 \n",
            "l0: 0.054866, l1: 0.052172, l2: 0.061982, l3: 0.061811, l4: 0.130381, l5: 0.098372, l6: 0.073462\n",
            "\n",
            "[epoch: 117/1000, batch:     3/   15, ite: 1743] train loss: 0.965935, tar: 0.118465 \n",
            "l0: 0.054172, l1: 0.062289, l2: 0.061080, l3: 0.058144, l4: 0.072656, l5: 0.066281, l6: 0.096625\n",
            "\n",
            "[epoch: 117/1000, batch:     4/   15, ite: 1744] train loss: 0.954692, tar: 0.117004 \n",
            "l0: 0.077923, l1: 0.093229, l2: 0.099106, l3: 0.097626, l4: 0.099617, l5: 0.093469, l6: 0.113921\n",
            "\n",
            "[epoch: 117/1000, batch:     5/   15, ite: 1745] train loss: 0.948474, tar: 0.116135 \n",
            "l0: 0.101347, l1: 0.108875, l2: 0.130861, l3: 0.109702, l4: 0.183309, l5: 0.168998, l6: 0.219738\n",
            "\n",
            "[epoch: 117/1000, batch:     6/   15, ite: 1746] train loss: 0.950091, tar: 0.115814 \n",
            "l0: 0.074920, l1: 0.079267, l2: 0.088475, l3: 0.087133, l4: 0.126100, l5: 0.134533, l6: 0.151503\n",
            "\n",
            "[epoch: 117/1000, batch:     7/   15, ite: 1747] train loss: 0.945662, tar: 0.114944 \n",
            "l0: 0.064221, l1: 0.098029, l2: 0.081000, l3: 0.070718, l4: 0.076666, l5: 0.066657, l6: 0.063211\n",
            "\n",
            "[epoch: 117/1000, batch:     8/   15, ite: 1748] train loss: 0.936804, tar: 0.113887 \n",
            "l0: 0.080414, l1: 0.064115, l2: 0.077560, l3: 0.093911, l4: 0.092685, l5: 0.178176, l6: 0.170303\n",
            "\n",
            "[epoch: 117/1000, batch:     9/   15, ite: 1749] train loss: 0.933138, tar: 0.113204 \n",
            "l0: 0.157555, l1: 0.162072, l2: 0.159331, l3: 0.173567, l4: 0.148260, l5: 0.227364, l6: 0.147466\n",
            "\n",
            "[epoch: 117/1000, batch:    10/   15, ite: 1750] train loss: 0.937988, tar: 0.114091 \n",
            "l0: 0.047970, l1: 0.055453, l2: 0.050253, l3: 0.052071, l4: 0.066026, l5: 0.079972, l6: 0.080673\n",
            "\n",
            "[epoch: 117/1000, batch:    11/   15, ite: 1751] train loss: 0.928074, tar: 0.112794 \n",
            "l0: 0.063951, l1: 0.059924, l2: 0.056153, l3: 0.090391, l4: 0.085366, l5: 0.105349, l6: 0.121129\n",
            "\n",
            "[epoch: 117/1000, batch:    12/   15, ite: 1752] train loss: 0.921424, tar: 0.111855 \n",
            "l0: 0.044526, l1: 0.049063, l2: 0.050381, l3: 0.053771, l4: 0.064960, l5: 0.097881, l6: 0.140957\n",
            "\n",
            "[epoch: 117/1000, batch:    13/   15, ite: 1753] train loss: 0.913502, tar: 0.110585 \n",
            "l0: 0.079094, l1: 0.081599, l2: 0.082460, l3: 0.089025, l4: 0.100519, l5: 0.142038, l6: 0.126757\n",
            "\n",
            "[epoch: 117/1000, batch:    14/   15, ite: 1754] train loss: 0.909576, tar: 0.110002 \n",
            "l0: 0.074342, l1: 0.089844, l2: 0.084335, l3: 0.115567, l4: 0.090796, l5: 0.095303, l6: 0.120540\n",
            "\n",
            "[epoch: 117/1000, batch:    15/   15, ite: 1755] train loss: 0.905233, tar: 0.109353 \n",
            "l0: 0.068918, l1: 0.088808, l2: 0.078360, l3: 0.072822, l4: 0.081313, l5: 0.131340, l6: 0.155799\n",
            "\n",
            "[epoch: 118/1000, batch:     1/   15, ite: 1756] train loss: 0.901164, tar: 0.108631 \n",
            "l0: 0.034805, l1: 0.026281, l2: 0.041318, l3: 0.043647, l4: 0.061476, l5: 0.079083, l6: 0.067647\n",
            "\n",
            "[epoch: 118/1000, batch:     2/   15, ite: 1757] train loss: 0.891569, tar: 0.107336 \n",
            "l0: 0.055066, l1: 0.061823, l2: 0.067958, l3: 0.055103, l4: 0.060419, l5: 0.065558, l6: 0.109470\n",
            "\n",
            "[epoch: 118/1000, batch:     3/   15, ite: 1758] train loss: 0.884394, tar: 0.106435 \n",
            "l0: 0.132740, l1: 0.108062, l2: 0.139829, l3: 0.166658, l4: 0.214328, l5: 0.180056, l6: 0.186046\n",
            "\n",
            "[epoch: 118/1000, batch:     4/   15, ite: 1759] train loss: 0.888518, tar: 0.106881 \n",
            "l0: 0.062496, l1: 0.061956, l2: 0.075493, l3: 0.099230, l4: 0.062299, l5: 0.059740, l6: 0.071897\n",
            "\n",
            "[epoch: 118/1000, batch:     5/   15, ite: 1760] train loss: 0.881928, tar: 0.106141 \n",
            "l0: 0.083003, l1: 0.090644, l2: 0.104400, l3: 0.095631, l4: 0.066280, l5: 0.079339, l6: 0.081139\n",
            "\n",
            "[epoch: 118/1000, batch:     6/   15, ite: 1761] train loss: 0.877313, tar: 0.105762 \n",
            "l0: 0.117061, l1: 0.128173, l2: 0.125286, l3: 0.129258, l4: 0.190297, l5: 0.173754, l6: 0.134941\n",
            "\n",
            "[epoch: 118/1000, batch:     7/   15, ite: 1762] train loss: 0.879272, tar: 0.105944 \n",
            "l0: 0.052057, l1: 0.071723, l2: 0.056154, l3: 0.052673, l4: 0.070464, l5: 0.103888, l6: 0.096185\n",
            "\n",
            "[epoch: 118/1000, batch:     8/   15, ite: 1763] train loss: 0.873302, tar: 0.105089 \n",
            "l0: 0.073846, l1: 0.085309, l2: 0.078739, l3: 0.074697, l4: 0.097146, l5: 0.151060, l6: 0.129209\n",
            "\n",
            "[epoch: 118/1000, batch:     9/   15, ite: 1764] train loss: 0.870438, tar: 0.104600 \n",
            "l0: 0.042436, l1: 0.042300, l2: 0.047225, l3: 0.048968, l4: 0.058446, l5: 0.085122, l6: 0.066685\n",
            "\n",
            "[epoch: 118/1000, batch:    10/   15, ite: 1765] train loss: 0.863065, tar: 0.103644 \n",
            "l0: 0.068463, l1: 0.064123, l2: 0.067385, l3: 0.070983, l4: 0.074599, l5: 0.129104, l6: 0.111990\n",
            "\n",
            "[epoch: 118/1000, batch:    11/   15, ite: 1766] train loss: 0.858876, tar: 0.103111 \n",
            "l0: 0.049918, l1: 0.054043, l2: 0.055771, l3: 0.062246, l4: 0.055385, l5: 0.071074, l6: 0.076607\n",
            "\n",
            "[epoch: 118/1000, batch:    12/   15, ite: 1767] train loss: 0.852401, tar: 0.102317 \n",
            "l0: 0.068741, l1: 0.070856, l2: 0.066391, l3: 0.087996, l4: 0.110287, l5: 0.122090, l6: 0.207610\n",
            "\n",
            "[epoch: 118/1000, batch:    13/   15, ite: 1768] train loss: 0.850660, tar: 0.101823 \n",
            "l0: 0.050742, l1: 0.045172, l2: 0.047639, l3: 0.058859, l4: 0.073629, l5: 0.110751, l6: 0.292375\n",
            "\n",
            "[epoch: 118/1000, batch:    14/   15, ite: 1769] train loss: 0.848174, tar: 0.101083 \n",
            "l0: 0.047395, l1: 0.048938, l2: 0.047649, l3: 0.052045, l4: 0.061854, l5: 0.067513, l6: 0.094811\n",
            "\n",
            "[epoch: 118/1000, batch:    15/   15, ite: 1770] train loss: 0.842061, tar: 0.100316 \n",
            "l0: 0.091725, l1: 0.118775, l2: 0.125160, l3: 0.122472, l4: 0.112347, l5: 0.085961, l6: 0.111859\n",
            "\n",
            "[epoch: 119/1000, batch:     1/   15, ite: 1771] train loss: 0.841022, tar: 0.100195 \n",
            "l0: 0.038113, l1: 0.029491, l2: 0.034677, l3: 0.046898, l4: 0.061172, l5: 0.072428, l6: 0.067144\n",
            "\n",
            "[epoch: 119/1000, batch:     2/   15, ite: 1772] train loss: 0.834201, tar: 0.099333 \n",
            "l0: 0.065365, l1: 0.072363, l2: 0.075343, l3: 0.082815, l4: 0.082798, l5: 0.080666, l6: 0.093862\n",
            "\n",
            "[epoch: 119/1000, batch:     3/   15, ite: 1773] train loss: 0.830352, tar: 0.098867 \n",
            "l0: 0.059539, l1: 0.070326, l2: 0.071646, l3: 0.081007, l4: 0.064046, l5: 0.062716, l6: 0.080892\n",
            "\n",
            "[epoch: 119/1000, batch:     4/   15, ite: 1774] train loss: 0.825755, tar: 0.098336 \n",
            "l0: 0.053095, l1: 0.052734, l2: 0.063046, l3: 0.066424, l4: 0.056326, l5: 0.060451, l6: 0.079108\n",
            "\n",
            "[epoch: 119/1000, batch:     5/   15, ite: 1775] train loss: 0.820494, tar: 0.097733 \n",
            "l0: 0.042364, l1: 0.041591, l2: 0.045541, l3: 0.056674, l4: 0.059390, l5: 0.073286, l6: 0.074369\n",
            "\n",
            "[epoch: 119/1000, batch:     6/   15, ite: 1776] train loss: 0.814872, tar: 0.097004 \n",
            "l0: 0.081958, l1: 0.072736, l2: 0.080791, l3: 0.096071, l4: 0.087977, l5: 0.137567, l6: 0.119795\n",
            "\n",
            "[epoch: 119/1000, batch:     7/   15, ite: 1777] train loss: 0.813080, tar: 0.096809 \n",
            "l0: 0.059959, l1: 0.074887, l2: 0.068326, l3: 0.059900, l4: 0.061920, l5: 0.084604, l6: 0.080014\n",
            "\n",
            "[epoch: 119/1000, batch:     8/   15, ite: 1778] train loss: 0.808933, tar: 0.096336 \n",
            "l0: 0.053267, l1: 0.046979, l2: 0.061637, l3: 0.071264, l4: 0.076220, l5: 0.075798, l6: 0.087927\n",
            "\n",
            "[epoch: 119/1000, batch:     9/   15, ite: 1779] train loss: 0.804681, tar: 0.095791 \n",
            "l0: 0.059887, l1: 0.076272, l2: 0.076015, l3: 0.064105, l4: 0.053619, l5: 0.063280, l6: 0.067548\n",
            "\n",
            "[epoch: 119/1000, batch:    10/   15, ite: 1780] train loss: 0.800382, tar: 0.095342 \n",
            "l0: 0.049296, l1: 0.056092, l2: 0.066644, l3: 0.066122, l4: 0.051505, l5: 0.061913, l6: 0.073371\n",
            "\n",
            "[epoch: 119/1000, batch:    11/   15, ite: 1781] train loss: 0.795747, tar: 0.094774 \n",
            "l0: 0.041505, l1: 0.038460, l2: 0.053043, l3: 0.055700, l4: 0.048757, l5: 0.049437, l6: 0.066986\n",
            "\n",
            "[epoch: 119/1000, batch:    12/   15, ite: 1782] train loss: 0.790358, tar: 0.094124 \n",
            "l0: 0.101087, l1: 0.100751, l2: 0.126407, l3: 0.134398, l4: 0.153412, l5: 0.143047, l6: 0.131669\n",
            "\n",
            "[epoch: 119/1000, batch:    13/   15, ite: 1783] train loss: 0.791568, tar: 0.094208 \n",
            "l0: 0.038838, l1: 0.040643, l2: 0.039749, l3: 0.043993, l4: 0.053648, l5: 0.064581, l6: 0.075110\n",
            "\n",
            "[epoch: 119/1000, batch:    14/   15, ite: 1784] train loss: 0.786390, tar: 0.093549 \n",
            "l0: 0.053845, l1: 0.054788, l2: 0.055497, l3: 0.053868, l4: 0.055359, l5: 0.066276, l6: 0.091719\n",
            "\n",
            "[epoch: 119/1000, batch:    15/   15, ite: 1785] train loss: 0.782213, tar: 0.093082 \n",
            "l0: 0.053034, l1: 0.053195, l2: 0.057996, l3: 0.071797, l4: 0.070472, l5: 0.082449, l6: 0.079793\n",
            "\n",
            "[epoch: 120/1000, batch:     1/   15, ite: 1786] train loss: 0.778568, tar: 0.092616 \n",
            "l0: 0.046012, l1: 0.047129, l2: 0.050660, l3: 0.054087, l4: 0.052579, l5: 0.079773, l6: 0.106627\n",
            "\n",
            "[epoch: 120/1000, batch:     2/   15, ite: 1787] train loss: 0.774640, tar: 0.092081 \n",
            "l0: 0.032133, l1: 0.035482, l2: 0.037209, l3: 0.043283, l4: 0.039439, l5: 0.042798, l6: 0.065172\n",
            "\n",
            "[epoch: 120/1000, batch:     3/   15, ite: 1788] train loss: 0.769195, tar: 0.091399 \n",
            "l0: 0.035948, l1: 0.041147, l2: 0.040527, l3: 0.041196, l4: 0.040588, l5: 0.047095, l6: 0.056990\n",
            "\n",
            "[epoch: 120/1000, batch:     4/   15, ite: 1789] train loss: 0.763963, tar: 0.090776 \n",
            "l0: 0.048089, l1: 0.044032, l2: 0.053270, l3: 0.059578, l4: 0.064698, l5: 0.062066, l6: 0.081805\n",
            "\n",
            "[epoch: 120/1000, batch:     5/   15, ite: 1790] train loss: 0.760069, tar: 0.090302 \n",
            "l0: 0.052830, l1: 0.060980, l2: 0.060335, l3: 0.048347, l4: 0.053533, l5: 0.072728, l6: 0.086942\n",
            "\n",
            "[epoch: 120/1000, batch:     6/   15, ite: 1791] train loss: 0.756505, tar: 0.089890 \n",
            "l0: 0.027710, l1: 0.026870, l2: 0.031929, l3: 0.029788, l4: 0.038988, l5: 0.037247, l6: 0.078585\n",
            "\n",
            "[epoch: 120/1000, batch:     7/   15, ite: 1792] train loss: 0.751229, tar: 0.089214 \n",
            "l0: 0.037749, l1: 0.040504, l2: 0.044385, l3: 0.045849, l4: 0.049169, l5: 0.044511, l6: 0.054545\n",
            "\n",
            "[epoch: 120/1000, batch:     8/   15, ite: 1793] train loss: 0.746557, tar: 0.088661 \n",
            "l0: 0.077833, l1: 0.092976, l2: 0.093985, l3: 0.113330, l4: 0.087464, l5: 0.073250, l6: 0.107181\n",
            "\n",
            "[epoch: 120/1000, batch:     9/   15, ite: 1794] train loss: 0.745487, tar: 0.088546 \n",
            "l0: 0.037242, l1: 0.035044, l2: 0.041032, l3: 0.055645, l4: 0.049727, l5: 0.054169, l6: 0.061582\n",
            "\n",
            "[epoch: 120/1000, batch:    10/   15, ite: 1795] train loss: 0.741160, tar: 0.088006 \n",
            "l0: 0.131553, l1: 0.138419, l2: 0.138063, l3: 0.148330, l4: 0.142600, l5: 0.141018, l6: 0.182501\n",
            "\n",
            "[epoch: 120/1000, batch:    11/   15, ite: 1796] train loss: 0.744091, tar: 0.088459 \n",
            "l0: 0.057369, l1: 0.077363, l2: 0.071604, l3: 0.069170, l4: 0.061987, l5: 0.060256, l6: 0.094259\n",
            "\n",
            "[epoch: 120/1000, batch:    12/   15, ite: 1797] train loss: 0.741492, tar: 0.088139 \n",
            "l0: 0.038324, l1: 0.040395, l2: 0.038883, l3: 0.037172, l4: 0.066451, l5: 0.066329, l6: 0.059079\n",
            "\n",
            "[epoch: 120/1000, batch:    13/   15, ite: 1798] train loss: 0.737463, tar: 0.087631 \n",
            "l0: 0.055938, l1: 0.051442, l2: 0.059285, l3: 0.063642, l4: 0.069497, l5: 0.084967, l6: 0.095289\n",
            "\n",
            "[epoch: 120/1000, batch:    14/   15, ite: 1799] train loss: 0.734863, tar: 0.087310 \n",
            "l0: 0.049910, l1: 0.053190, l2: 0.047916, l3: 0.052916, l4: 0.068340, l5: 0.063614, l6: 0.069846\n",
            "\n",
            "[epoch: 120/1000, batch:    15/   15, ite: 1800] train loss: 0.731571, tar: 0.086936 \n",
            "l0: 0.040967, l1: 0.042873, l2: 0.041720, l3: 0.041735, l4: 0.053325, l5: 0.052924, l6: 0.056256\n",
            "\n",
            "[epoch: 121/1000, batch:     1/   15, ite: 1801] train loss: 0.329800, tar: 0.040967 \n",
            "l0: 0.067803, l1: 0.083940, l2: 0.077442, l3: 0.078655, l4: 0.066723, l5: 0.060242, l6: 0.062615\n",
            "\n",
            "[epoch: 121/1000, batch:     2/   15, ite: 1802] train loss: 0.413610, tar: 0.054385 \n",
            "l0: 0.061017, l1: 0.073639, l2: 0.071048, l3: 0.068920, l4: 0.073031, l5: 0.080894, l6: 0.070980\n",
            "\n",
            "[epoch: 121/1000, batch:     3/   15, ite: 1803] train loss: 0.442250, tar: 0.056596 \n",
            "l0: 0.052484, l1: 0.057694, l2: 0.058556, l3: 0.059790, l4: 0.066619, l5: 0.067624, l6: 0.074881\n",
            "\n",
            "[epoch: 121/1000, batch:     4/   15, ite: 1804] train loss: 0.441099, tar: 0.055568 \n",
            "l0: 0.029372, l1: 0.030317, l2: 0.030017, l3: 0.035177, l4: 0.031191, l5: 0.031047, l6: 0.047994\n",
            "\n",
            "[epoch: 121/1000, batch:     5/   15, ite: 1805] train loss: 0.399902, tar: 0.050329 \n",
            "l0: 0.047931, l1: 0.051269, l2: 0.051531, l3: 0.048517, l4: 0.054404, l5: 0.077847, l6: 0.133758\n",
            "\n",
            "[epoch: 121/1000, batch:     6/   15, ite: 1806] train loss: 0.410795, tar: 0.049929 \n",
            "l0: 0.036187, l1: 0.036960, l2: 0.041317, l3: 0.043689, l4: 0.051356, l5: 0.058589, l6: 0.089043\n",
            "\n",
            "[epoch: 121/1000, batch:     7/   15, ite: 1807] train loss: 0.403130, tar: 0.047966 \n",
            "l0: 0.048799, l1: 0.048273, l2: 0.046990, l3: 0.051171, l4: 0.054187, l5: 0.066830, l6: 0.092806\n",
            "\n",
            "[epoch: 121/1000, batch:     8/   15, ite: 1808] train loss: 0.403871, tar: 0.048070 \n",
            "l0: 0.039041, l1: 0.040908, l2: 0.043595, l3: 0.043573, l4: 0.045999, l5: 0.042766, l6: 0.075316\n",
            "\n",
            "[epoch: 121/1000, batch:     9/   15, ite: 1809] train loss: 0.395796, tar: 0.047067 \n",
            "l0: 0.042130, l1: 0.043829, l2: 0.047166, l3: 0.057084, l4: 0.057883, l5: 0.068250, l6: 0.087757\n",
            "\n",
            "[epoch: 121/1000, batch:    10/   15, ite: 1810] train loss: 0.396626, tar: 0.046573 \n",
            "l0: 0.033784, l1: 0.030841, l2: 0.037105, l3: 0.045314, l4: 0.050498, l5: 0.058977, l6: 0.069432\n",
            "\n",
            "[epoch: 121/1000, batch:    11/   15, ite: 1811] train loss: 0.390201, tar: 0.045411 \n",
            "l0: 0.029969, l1: 0.031752, l2: 0.033973, l3: 0.040152, l4: 0.046700, l5: 0.046230, l6: 0.061035\n",
            "\n",
            "[epoch: 121/1000, batch:    12/   15, ite: 1812] train loss: 0.381835, tar: 0.044124 \n",
            "l0: 0.033800, l1: 0.032916, l2: 0.036392, l3: 0.043459, l4: 0.046028, l5: 0.055967, l6: 0.071319\n",
            "\n",
            "[epoch: 121/1000, batch:    13/   15, ite: 1813] train loss: 0.377070, tar: 0.043330 \n",
            "l0: 0.032138, l1: 0.034341, l2: 0.036643, l3: 0.037685, l4: 0.045965, l5: 0.046826, l6: 0.055751\n",
            "\n",
            "[epoch: 121/1000, batch:    14/   15, ite: 1814] train loss: 0.370804, tar: 0.042530 \n",
            "l0: 0.045220, l1: 0.054510, l2: 0.057515, l3: 0.061705, l4: 0.046291, l5: 0.049554, l6: 0.060457\n",
            "\n",
            "[epoch: 121/1000, batch:    15/   15, ite: 1815] train loss: 0.371100, tar: 0.042710 \n",
            "l0: 0.039474, l1: 0.047412, l2: 0.049623, l3: 0.054187, l4: 0.042849, l5: 0.044088, l6: 0.063106\n",
            "\n",
            "[epoch: 122/1000, batch:     1/   15, ite: 1816] train loss: 0.369203, tar: 0.042507 \n",
            "l0: 0.024620, l1: 0.024804, l2: 0.025995, l3: 0.026001, l4: 0.031711, l5: 0.042980, l6: 0.065196\n",
            "\n",
            "[epoch: 122/1000, batch:     2/   15, ite: 1817] train loss: 0.361679, tar: 0.041455 \n",
            "l0: 0.048158, l1: 0.054802, l2: 0.053850, l3: 0.058789, l4: 0.083918, l5: 0.097659, l6: 0.091990\n",
            "\n",
            "[epoch: 122/1000, batch:     3/   15, ite: 1818] train loss: 0.368762, tar: 0.041827 \n",
            "l0: 0.042346, l1: 0.045612, l2: 0.043698, l3: 0.037670, l4: 0.043380, l5: 0.063120, l6: 0.081198\n",
            "\n",
            "[epoch: 122/1000, batch:     4/   15, ite: 1819] train loss: 0.368144, tar: 0.041855 \n",
            "l0: 0.035156, l1: 0.035311, l2: 0.040346, l3: 0.049944, l4: 0.052723, l5: 0.052956, l6: 0.064347\n",
            "\n",
            "[epoch: 122/1000, batch:     5/   15, ite: 1820] train loss: 0.366276, tar: 0.041520 \n",
            "l0: 0.042564, l1: 0.036904, l2: 0.042749, l3: 0.055452, l4: 0.059688, l5: 0.072767, l6: 0.077947\n",
            "\n",
            "[epoch: 122/1000, batch:     6/   15, ite: 1821] train loss: 0.367314, tar: 0.041570 \n",
            "l0: 0.040703, l1: 0.041089, l2: 0.047654, l3: 0.043020, l4: 0.046785, l5: 0.054646, l6: 0.056215\n",
            "\n",
            "[epoch: 122/1000, batch:     7/   15, ite: 1822] train loss: 0.365623, tar: 0.041530 \n",
            "l0: 0.043022, l1: 0.042111, l2: 0.051203, l3: 0.049869, l4: 0.049939, l5: 0.055510, l6: 0.056606\n",
            "\n",
            "[epoch: 122/1000, batch:     8/   15, ite: 1823] train loss: 0.364868, tar: 0.041595 \n",
            "l0: 0.029070, l1: 0.033794, l2: 0.034576, l3: 0.037413, l4: 0.036754, l5: 0.037594, l6: 0.058608\n",
            "\n",
            "[epoch: 122/1000, batch:     9/   15, ite: 1824] train loss: 0.360824, tar: 0.041073 \n",
            "l0: 0.035140, l1: 0.034177, l2: 0.040953, l3: 0.038442, l4: 0.039725, l5: 0.046596, l6: 0.048748\n",
            "\n",
            "[epoch: 122/1000, batch:    10/   15, ite: 1825] train loss: 0.357742, tar: 0.040836 \n",
            "l0: 0.066288, l1: 0.069208, l2: 0.071592, l3: 0.081990, l4: 0.091929, l5: 0.077980, l6: 0.094260\n",
            "\n",
            "[epoch: 122/1000, batch:    11/   15, ite: 1826] train loss: 0.365262, tar: 0.041815 \n",
            "l0: 0.025176, l1: 0.026808, l2: 0.030419, l3: 0.028563, l4: 0.028873, l5: 0.036789, l6: 0.048612\n",
            "\n",
            "[epoch: 122/1000, batch:    12/   15, ite: 1827] train loss: 0.360076, tar: 0.041198 \n",
            "l0: 0.055852, l1: 0.054493, l2: 0.063389, l3: 0.065753, l4: 0.058066, l5: 0.060502, l6: 0.077182\n",
            "\n",
            "[epoch: 122/1000, batch:    13/   15, ite: 1828] train loss: 0.362760, tar: 0.041722 \n",
            "l0: 0.031762, l1: 0.033027, l2: 0.033252, l3: 0.037283, l4: 0.036213, l5: 0.037638, l6: 0.059148\n",
            "\n",
            "[epoch: 122/1000, batch:    14/   15, ite: 1829] train loss: 0.359503, tar: 0.041378 \n",
            "l0: 0.050945, l1: 0.050430, l2: 0.060197, l3: 0.083111, l4: 0.087728, l5: 0.078755, l6: 0.069019\n",
            "\n",
            "[epoch: 122/1000, batch:    15/   15, ite: 1830] train loss: 0.363526, tar: 0.041697 \n",
            "l0: 0.032485, l1: 0.030064, l2: 0.034448, l3: 0.038604, l4: 0.044436, l5: 0.054444, l6: 0.061969\n",
            "\n",
            "[epoch: 123/1000, batch:     1/   15, ite: 1831] train loss: 0.361362, tar: 0.041400 \n",
            "l0: 0.045413, l1: 0.045557, l2: 0.054734, l3: 0.074041, l4: 0.068872, l5: 0.061917, l6: 0.059462\n",
            "\n",
            "[epoch: 123/1000, batch:     2/   15, ite: 1832] train loss: 0.362882, tar: 0.041525 \n",
            "l0: 0.031726, l1: 0.032577, l2: 0.033068, l3: 0.037356, l4: 0.041566, l5: 0.046155, l6: 0.057132\n",
            "\n",
            "[epoch: 123/1000, batch:     3/   15, ite: 1833] train loss: 0.360358, tar: 0.041229 \n",
            "l0: 0.030238, l1: 0.036607, l2: 0.032352, l3: 0.038196, l4: 0.035756, l5: 0.044738, l6: 0.048076\n",
            "\n",
            "[epoch: 123/1000, batch:     4/   15, ite: 1834] train loss: 0.357582, tar: 0.040905 \n",
            "l0: 0.033681, l1: 0.035134, l2: 0.038082, l3: 0.038617, l4: 0.043987, l5: 0.046367, l6: 0.068752\n",
            "\n",
            "[epoch: 123/1000, batch:     5/   15, ite: 1835] train loss: 0.356068, tar: 0.040699 \n",
            "l0: 0.027970, l1: 0.028886, l2: 0.027944, l3: 0.033998, l4: 0.034532, l5: 0.035770, l6: 0.054019\n",
            "\n",
            "[epoch: 123/1000, batch:     6/   15, ite: 1836] train loss: 0.352931, tar: 0.040345 \n",
            "l0: 0.031076, l1: 0.027675, l2: 0.032011, l3: 0.034609, l4: 0.037855, l5: 0.047986, l6: 0.057570\n",
            "\n",
            "[epoch: 123/1000, batch:     7/   15, ite: 1837] train loss: 0.350657, tar: 0.040095 \n",
            "l0: 0.048502, l1: 0.051741, l2: 0.058267, l3: 0.062203, l4: 0.059335, l5: 0.057076, l6: 0.083058\n",
            "\n",
            "[epoch: 123/1000, batch:     8/   15, ite: 1838] train loss: 0.352486, tar: 0.040316 \n",
            "l0: 0.034435, l1: 0.036757, l2: 0.040791, l3: 0.038826, l4: 0.042465, l5: 0.038551, l6: 0.054435\n",
            "\n",
            "[epoch: 123/1000, batch:     9/   15, ite: 1839] train loss: 0.350788, tar: 0.040165 \n",
            "l0: 0.019906, l1: 0.019457, l2: 0.020998, l3: 0.025432, l4: 0.030387, l5: 0.034232, l6: 0.044962\n",
            "\n",
            "[epoch: 123/1000, batch:    10/   15, ite: 1840] train loss: 0.346903, tar: 0.039659 \n",
            "l0: 0.027654, l1: 0.027029, l2: 0.033477, l3: 0.036619, l4: 0.040423, l5: 0.038858, l6: 0.043598\n",
            "\n",
            "[epoch: 123/1000, batch:    11/   15, ite: 1841] train loss: 0.344482, tar: 0.039366 \n",
            "l0: 0.038617, l1: 0.038926, l2: 0.045966, l3: 0.048585, l4: 0.057483, l5: 0.063021, l6: 0.081270\n",
            "\n",
            "[epoch: 123/1000, batch:    12/   15, ite: 1842] train loss: 0.345182, tar: 0.039348 \n",
            "l0: 0.040041, l1: 0.039056, l2: 0.057655, l3: 0.054836, l4: 0.041913, l5: 0.043995, l6: 0.075678\n",
            "\n",
            "[epoch: 123/1000, batch:    13/   15, ite: 1843] train loss: 0.345368, tar: 0.039364 \n",
            "l0: 0.040417, l1: 0.052533, l2: 0.046615, l3: 0.039697, l4: 0.047232, l5: 0.047761, l6: 0.052469\n",
            "\n",
            "[epoch: 123/1000, batch:    14/   15, ite: 1844] train loss: 0.344944, tar: 0.039388 \n",
            "l0: 0.022294, l1: 0.023481, l2: 0.022653, l3: 0.025934, l4: 0.027793, l5: 0.032477, l6: 0.040461\n",
            "\n",
            "[epoch: 123/1000, batch:    15/   15, ite: 1845] train loss: 0.341614, tar: 0.039008 \n",
            "l0: 0.030340, l1: 0.033388, l2: 0.032181, l3: 0.037332, l4: 0.043142, l5: 0.041759, l6: 0.052402\n",
            "\n",
            "[epoch: 124/1000, batch:     1/   15, ite: 1846] train loss: 0.340069, tar: 0.038820 \n",
            "l0: 0.032367, l1: 0.041847, l2: 0.039366, l3: 0.034985, l4: 0.038162, l5: 0.035682, l6: 0.045179\n",
            "\n",
            "[epoch: 124/1000, batch:     2/   15, ite: 1847] train loss: 0.338527, tar: 0.038683 \n",
            "l0: 0.027902, l1: 0.029711, l2: 0.032758, l3: 0.034523, l4: 0.036995, l5: 0.041402, l6: 0.048368\n",
            "\n",
            "[epoch: 124/1000, batch:     3/   15, ite: 1848] train loss: 0.336717, tar: 0.038458 \n",
            "l0: 0.031477, l1: 0.027782, l2: 0.032203, l3: 0.035812, l4: 0.041567, l5: 0.052360, l6: 0.067781\n",
            "\n",
            "[epoch: 124/1000, batch:     4/   15, ite: 1849] train loss: 0.335743, tar: 0.038316 \n",
            "l0: 0.028980, l1: 0.030474, l2: 0.034072, l3: 0.035695, l4: 0.041947, l5: 0.040380, l6: 0.051509\n",
            "\n",
            "[epoch: 124/1000, batch:     5/   15, ite: 1850] train loss: 0.334289, tar: 0.038129 \n",
            "l0: 0.034323, l1: 0.041189, l2: 0.041438, l3: 0.044076, l4: 0.041204, l5: 0.041701, l6: 0.061958\n",
            "\n",
            "[epoch: 124/1000, batch:     6/   15, ite: 1851] train loss: 0.333732, tar: 0.038054 \n",
            "l0: 0.046013, l1: 0.050824, l2: 0.053926, l3: 0.060986, l4: 0.065499, l5: 0.051280, l6: 0.066636\n",
            "\n",
            "[epoch: 124/1000, batch:     7/   15, ite: 1852] train loss: 0.334914, tar: 0.038207 \n",
            "l0: 0.024404, l1: 0.023260, l2: 0.026882, l3: 0.026546, l4: 0.029621, l5: 0.036225, l6: 0.048482\n",
            "\n",
            "[epoch: 124/1000, batch:     8/   15, ite: 1853] train loss: 0.332659, tar: 0.037947 \n",
            "l0: 0.031682, l1: 0.030651, l2: 0.032645, l3: 0.033606, l4: 0.033439, l5: 0.040988, l6: 0.062405\n",
            "\n",
            "[epoch: 124/1000, batch:     9/   15, ite: 1854] train loss: 0.331414, tar: 0.037831 \n",
            "l0: 0.043227, l1: 0.037532, l2: 0.055547, l3: 0.053896, l4: 0.045390, l5: 0.049924, l6: 0.074240\n",
            "\n",
            "[epoch: 124/1000, batch:    10/   15, ite: 1855] train loss: 0.331929, tar: 0.037929 \n",
            "l0: 0.032047, l1: 0.029926, l2: 0.035149, l3: 0.042124, l4: 0.043771, l5: 0.046140, l6: 0.052354\n",
            "\n",
            "[epoch: 124/1000, batch:    11/   15, ite: 1856] train loss: 0.331029, tar: 0.037824 \n",
            "l0: 0.052667, l1: 0.050983, l2: 0.054704, l3: 0.055805, l4: 0.059135, l5: 0.074142, l6: 0.072833\n",
            "\n",
            "[epoch: 124/1000, batch:    12/   15, ite: 1857] train loss: 0.332594, tar: 0.038084 \n",
            "l0: 0.027839, l1: 0.030356, l2: 0.032022, l3: 0.034814, l4: 0.036278, l5: 0.036668, l6: 0.042461\n",
            "\n",
            "[epoch: 124/1000, batch:    13/   15, ite: 1858] train loss: 0.331006, tar: 0.037908 \n",
            "l0: 0.031372, l1: 0.027822, l2: 0.034745, l3: 0.038090, l4: 0.044544, l5: 0.052678, l6: 0.062560\n",
            "\n",
            "[epoch: 124/1000, batch:    14/   15, ite: 1859] train loss: 0.330341, tar: 0.037797 \n",
            "l0: 0.024373, l1: 0.025956, l2: 0.025294, l3: 0.030275, l4: 0.030049, l5: 0.045576, l6: 0.052477\n",
            "\n",
            "[epoch: 124/1000, batch:    15/   15, ite: 1860] train loss: 0.328736, tar: 0.037573 \n",
            "l0: 0.041611, l1: 0.047136, l2: 0.044760, l3: 0.038513, l4: 0.040377, l5: 0.048912, l6: 0.058932\n",
            "\n",
            "[epoch: 125/1000, batch:     1/   15, ite: 1861] train loss: 0.328596, tar: 0.037639 \n",
            "l0: 0.028378, l1: 0.025760, l2: 0.030237, l3: 0.037378, l4: 0.041481, l5: 0.050296, l6: 0.055251\n",
            "\n",
            "[epoch: 125/1000, batch:     2/   15, ite: 1862] train loss: 0.327632, tar: 0.037490 \n",
            "l0: 0.029322, l1: 0.028703, l2: 0.034402, l3: 0.038055, l4: 0.035901, l5: 0.036902, l6: 0.054398\n",
            "\n",
            "[epoch: 125/1000, batch:     3/   15, ite: 1863] train loss: 0.326521, tar: 0.037360 \n",
            "l0: 0.045793, l1: 0.053387, l2: 0.051722, l3: 0.044444, l4: 0.051036, l5: 0.062112, l6: 0.064064\n",
            "\n",
            "[epoch: 125/1000, batch:     4/   15, ite: 1864] train loss: 0.327241, tar: 0.037492 \n",
            "l0: 0.035790, l1: 0.035542, l2: 0.039890, l3: 0.046056, l4: 0.047638, l5: 0.044877, l6: 0.054889\n",
            "\n",
            "[epoch: 125/1000, batch:     5/   15, ite: 1865] train loss: 0.326894, tar: 0.037466 \n",
            "l0: 0.038197, l1: 0.040812, l2: 0.042249, l3: 0.046320, l4: 0.045148, l5: 0.055395, l6: 0.070692\n",
            "\n",
            "[epoch: 125/1000, batch:     6/   15, ite: 1866] train loss: 0.327074, tar: 0.037477 \n",
            "l0: 0.029193, l1: 0.027590, l2: 0.036116, l3: 0.038025, l4: 0.041969, l5: 0.045672, l6: 0.052448\n",
            "\n",
            "[epoch: 125/1000, batch:     7/   15, ite: 1867] train loss: 0.326237, tar: 0.037353 \n",
            "l0: 0.059756, l1: 0.064751, l2: 0.063644, l3: 0.061551, l4: 0.067165, l5: 0.074075, l6: 0.085506\n",
            "\n",
            "[epoch: 125/1000, batch:     8/   15, ite: 1868] train loss: 0.328446, tar: 0.037683 \n",
            "l0: 0.024621, l1: 0.024578, l2: 0.025496, l3: 0.028193, l4: 0.031554, l5: 0.043730, l6: 0.055627\n",
            "\n",
            "[epoch: 125/1000, batch:     9/   15, ite: 1869] train loss: 0.327075, tar: 0.037493 \n",
            "l0: 0.024309, l1: 0.024155, l2: 0.024696, l3: 0.027123, l4: 0.027992, l5: 0.029633, l6: 0.037532\n",
            "\n",
            "[epoch: 125/1000, batch:    10/   15, ite: 1870] train loss: 0.325194, tar: 0.037305 \n",
            "l0: 0.027402, l1: 0.024272, l2: 0.026476, l3: 0.030819, l4: 0.034039, l5: 0.039627, l6: 0.051230\n",
            "\n",
            "[epoch: 125/1000, batch:    11/   15, ite: 1871] train loss: 0.323908, tar: 0.037166 \n",
            "l0: 0.047429, l1: 0.042564, l2: 0.045121, l3: 0.051170, l4: 0.054388, l5: 0.062446, l6: 0.071388\n",
            "\n",
            "[epoch: 125/1000, batch:    12/   15, ite: 1872] train loss: 0.324611, tar: 0.037308 \n",
            "l0: 0.029645, l1: 0.031044, l2: 0.033236, l3: 0.035937, l4: 0.034040, l5: 0.042866, l6: 0.063732\n",
            "\n",
            "[epoch: 125/1000, batch:    13/   15, ite: 1873] train loss: 0.323869, tar: 0.037203 \n",
            "l0: 0.052838, l1: 0.055100, l2: 0.057497, l3: 0.057213, l4: 0.055221, l5: 0.049022, l6: 0.059442\n",
            "\n",
            "[epoch: 125/1000, batch:    14/   15, ite: 1874] train loss: 0.324713, tar: 0.037414 \n",
            "l0: 0.039060, l1: 0.037978, l2: 0.049661, l3: 0.045588, l4: 0.041565, l5: 0.044310, l6: 0.052062\n",
            "\n",
            "[epoch: 125/1000, batch:    15/   15, ite: 1875] train loss: 0.324520, tar: 0.037436 \n",
            "l0: 0.017905, l1: 0.017933, l2: 0.018700, l3: 0.021143, l4: 0.024572, l5: 0.029881, l6: 0.044176\n",
            "\n",
            "[epoch: 126/1000, batch:     1/   15, ite: 1876] train loss: 0.322544, tar: 0.037179 \n",
            "l0: 0.030918, l1: 0.036217, l2: 0.043240, l3: 0.041169, l4: 0.037130, l5: 0.036718, l6: 0.050865\n",
            "\n",
            "[epoch: 126/1000, batch:     2/   15, ite: 1877] train loss: 0.321943, tar: 0.037098 \n",
            "l0: 0.028702, l1: 0.028447, l2: 0.031654, l3: 0.036046, l4: 0.041830, l5: 0.047506, l6: 0.068432\n",
            "\n",
            "[epoch: 126/1000, batch:     3/   15, ite: 1878] train loss: 0.321438, tar: 0.036990 \n",
            "l0: 0.024213, l1: 0.024427, l2: 0.023903, l3: 0.027350, l4: 0.033335, l5: 0.041348, l6: 0.056747\n",
            "\n",
            "[epoch: 126/1000, batch:     4/   15, ite: 1879] train loss: 0.320298, tar: 0.036829 \n",
            "l0: 0.037434, l1: 0.042104, l2: 0.040954, l3: 0.044351, l4: 0.040510, l5: 0.043581, l6: 0.060590\n",
            "\n",
            "[epoch: 126/1000, batch:     5/   15, ite: 1880] train loss: 0.320163, tar: 0.036836 \n",
            "l0: 0.029535, l1: 0.030773, l2: 0.030986, l3: 0.037174, l4: 0.041958, l5: 0.049984, l6: 0.051060\n",
            "\n",
            "[epoch: 126/1000, batch:     6/   15, ite: 1881] train loss: 0.319562, tar: 0.036746 \n",
            "l0: 0.034586, l1: 0.038344, l2: 0.037399, l3: 0.038885, l4: 0.040589, l5: 0.051907, l6: 0.062454\n",
            "\n",
            "[epoch: 126/1000, batch:     7/   15, ite: 1882] train loss: 0.319374, tar: 0.036720 \n",
            "l0: 0.025837, l1: 0.025850, l2: 0.028195, l3: 0.033586, l4: 0.034361, l5: 0.032047, l6: 0.048445\n",
            "\n",
            "[epoch: 126/1000, batch:     8/   15, ite: 1883] train loss: 0.318277, tar: 0.036589 \n",
            "l0: 0.028109, l1: 0.025171, l2: 0.032294, l3: 0.032500, l4: 0.034489, l5: 0.039256, l6: 0.041125\n",
            "\n",
            "[epoch: 126/1000, batch:     9/   15, ite: 1884] train loss: 0.317261, tar: 0.036488 \n",
            "l0: 0.034378, l1: 0.038635, l2: 0.038291, l3: 0.036867, l4: 0.038048, l5: 0.036327, l6: 0.048037\n",
            "\n",
            "[epoch: 126/1000, batch:    10/   15, ite: 1885] train loss: 0.316712, tar: 0.036463 \n",
            "l0: 0.025939, l1: 0.027423, l2: 0.029899, l3: 0.030831, l4: 0.033039, l5: 0.035256, l6: 0.052714\n",
            "\n",
            "[epoch: 126/1000, batch:    11/   15, ite: 1886] train loss: 0.315763, tar: 0.036341 \n",
            "l0: 0.032259, l1: 0.039608, l2: 0.035829, l3: 0.036353, l4: 0.033621, l5: 0.031251, l6: 0.039593\n",
            "\n",
            "[epoch: 126/1000, batch:    12/   15, ite: 1887] train loss: 0.314990, tar: 0.036294 \n",
            "l0: 0.038168, l1: 0.034852, l2: 0.041222, l3: 0.043387, l4: 0.044603, l5: 0.052271, l6: 0.058675\n",
            "\n",
            "[epoch: 126/1000, batch:    13/   15, ite: 1888] train loss: 0.314970, tar: 0.036315 \n",
            "l0: 0.044405, l1: 0.044000, l2: 0.051748, l3: 0.058144, l4: 0.054958, l5: 0.058110, l6: 0.085479\n",
            "\n",
            "[epoch: 126/1000, batch:    14/   15, ite: 1889] train loss: 0.315890, tar: 0.036406 \n",
            "l0: 0.040803, l1: 0.047637, l2: 0.049240, l3: 0.046584, l4: 0.053805, l5: 0.047063, l6: 0.066775\n",
            "\n",
            "[epoch: 126/1000, batch:    15/   15, ite: 1890] train loss: 0.316290, tar: 0.036455 \n",
            "l0: 0.048859, l1: 0.053310, l2: 0.052991, l3: 0.057473, l4: 0.061615, l5: 0.061495, l6: 0.067006\n",
            "\n",
            "[epoch: 127/1000, batch:     1/   15, ite: 1891] train loss: 0.317240, tar: 0.036591 \n",
            "l0: 0.023078, l1: 0.023833, l2: 0.023872, l3: 0.026839, l4: 0.035515, l5: 0.043574, l6: 0.051271\n",
            "\n",
            "[epoch: 127/1000, batch:     2/   15, ite: 1892] train loss: 0.316270, tar: 0.036444 \n",
            "l0: 0.027909, l1: 0.029036, l2: 0.032100, l3: 0.032548, l4: 0.035649, l5: 0.033815, l6: 0.046754\n",
            "\n",
            "[epoch: 127/1000, batch:     3/   15, ite: 1893] train loss: 0.315426, tar: 0.036352 \n",
            "l0: 0.029857, l1: 0.030047, l2: 0.032245, l3: 0.031629, l4: 0.037878, l5: 0.035862, l6: 0.053019\n",
            "\n",
            "[epoch: 127/1000, batch:     4/   15, ite: 1894] train loss: 0.314736, tar: 0.036283 \n",
            "l0: 0.046438, l1: 0.044396, l2: 0.048351, l3: 0.051572, l4: 0.055196, l5: 0.058528, l6: 0.066511\n",
            "\n",
            "[epoch: 127/1000, batch:     5/   15, ite: 1895] train loss: 0.315328, tar: 0.036390 \n",
            "l0: 0.022775, l1: 0.021252, l2: 0.024759, l3: 0.028776, l4: 0.028334, l5: 0.028838, l6: 0.035705\n",
            "\n",
            "[epoch: 127/1000, batch:     6/   15, ite: 1896] train loss: 0.314027, tar: 0.036248 \n",
            "l0: 0.017007, l1: 0.016723, l2: 0.017671, l3: 0.019546, l4: 0.025343, l5: 0.027299, l6: 0.043196\n",
            "\n",
            "[epoch: 127/1000, batch:     7/   15, ite: 1897] train loss: 0.312509, tar: 0.036050 \n",
            "l0: 0.024821, l1: 0.023610, l2: 0.027420, l3: 0.031939, l4: 0.035349, l5: 0.039407, l6: 0.058620\n",
            "\n",
            "[epoch: 127/1000, batch:     8/   15, ite: 1898] train loss: 0.311781, tar: 0.035935 \n",
            "l0: 0.025343, l1: 0.026696, l2: 0.027538, l3: 0.031990, l4: 0.031682, l5: 0.041391, l6: 0.043113\n",
            "\n",
            "[epoch: 127/1000, batch:     9/   15, ite: 1899] train loss: 0.310932, tar: 0.035828 \n",
            "l0: 0.036463, l1: 0.040804, l2: 0.037957, l3: 0.040196, l4: 0.039597, l5: 0.048231, l6: 0.053114\n",
            "\n",
            "[epoch: 127/1000, batch:    10/   15, ite: 1900] train loss: 0.310787, tar: 0.035835 \n",
            "l0: 0.034261, l1: 0.036966, l2: 0.048928, l3: 0.040745, l4: 0.036024, l5: 0.035127, l6: 0.054361\n",
            "\n",
            "[epoch: 127/1000, batch:    11/   15, ite: 1901] train loss: 0.286412, tar: 0.034261 \n",
            "l0: 0.037768, l1: 0.041722, l2: 0.047140, l3: 0.054968, l4: 0.049359, l5: 0.045795, l6: 0.052108\n",
            "\n",
            "[epoch: 127/1000, batch:    12/   15, ite: 1902] train loss: 0.307636, tar: 0.036014 \n",
            "l0: 0.019318, l1: 0.019417, l2: 0.023090, l3: 0.025038, l4: 0.026136, l5: 0.032164, l6: 0.054152\n",
            "\n",
            "[epoch: 127/1000, batch:    13/   15, ite: 1903] train loss: 0.271530, tar: 0.030449 \n",
            "l0: 0.024358, l1: 0.022982, l2: 0.028910, l3: 0.031628, l4: 0.032895, l5: 0.034002, l6: 0.039084\n",
            "\n",
            "[epoch: 127/1000, batch:    14/   15, ite: 1904] train loss: 0.257112, tar: 0.028926 \n",
            "l0: 0.027495, l1: 0.026962, l2: 0.031996, l3: 0.034463, l4: 0.035537, l5: 0.045593, l6: 0.063293\n",
            "\n",
            "[epoch: 127/1000, batch:    15/   15, ite: 1905] train loss: 0.258757, tar: 0.028640 \n",
            "l0: 0.024951, l1: 0.025904, l2: 0.027523, l3: 0.029151, l4: 0.033245, l5: 0.042665, l6: 0.046902\n",
            "\n",
            "[epoch: 128/1000, batch:     1/   15, ite: 1906] train loss: 0.254021, tar: 0.028025 \n",
            "l0: 0.018091, l1: 0.017043, l2: 0.018945, l3: 0.020136, l4: 0.025722, l5: 0.030299, l6: 0.040221\n",
            "\n",
            "[epoch: 128/1000, batch:     2/   15, ite: 1907] train loss: 0.242084, tar: 0.026606 \n",
            "l0: 0.018278, l1: 0.017735, l2: 0.019948, l3: 0.021418, l4: 0.024448, l5: 0.025280, l6: 0.035989\n",
            "\n",
            "[epoch: 128/1000, batch:     3/   15, ite: 1908] train loss: 0.232210, tar: 0.025565 \n",
            "l0: 0.049670, l1: 0.047972, l2: 0.044373, l3: 0.048499, l4: 0.047008, l5: 0.060787, l6: 0.077851\n",
            "\n",
            "[epoch: 128/1000, batch:     4/   15, ite: 1909] train loss: 0.248205, tar: 0.028243 \n",
            "l0: 0.030326, l1: 0.026883, l2: 0.030480, l3: 0.035251, l4: 0.038425, l5: 0.043797, l6: 0.051121\n",
            "\n",
            "[epoch: 128/1000, batch:     5/   15, ite: 1910] train loss: 0.249012, tar: 0.028452 \n",
            "l0: 0.029409, l1: 0.035835, l2: 0.032253, l3: 0.031232, l4: 0.034152, l5: 0.032349, l6: 0.043461\n",
            "\n",
            "[epoch: 128/1000, batch:     6/   15, ite: 1911] train loss: 0.248074, tar: 0.028539 \n",
            "l0: 0.036835, l1: 0.034761, l2: 0.041190, l3: 0.039499, l4: 0.044711, l5: 0.045474, l6: 0.051926\n",
            "\n",
            "[epoch: 128/1000, batch:     7/   15, ite: 1912] train loss: 0.251934, tar: 0.029230 \n",
            "l0: 0.018705, l1: 0.019269, l2: 0.020285, l3: 0.023034, l4: 0.027269, l5: 0.035815, l6: 0.044154\n",
            "\n",
            "[epoch: 128/1000, batch:     8/   15, ite: 1913] train loss: 0.247057, tar: 0.028420 \n",
            "l0: 0.023543, l1: 0.022746, l2: 0.024177, l3: 0.029685, l4: 0.035405, l5: 0.039562, l6: 0.055334\n",
            "\n",
            "[epoch: 128/1000, batch:     9/   15, ite: 1914] train loss: 0.245871, tar: 0.028072 \n",
            "l0: 0.025425, l1: 0.027092, l2: 0.027099, l3: 0.031210, l4: 0.032574, l5: 0.036777, l6: 0.048358\n",
            "\n",
            "[epoch: 128/1000, batch:    10/   15, ite: 1915] train loss: 0.244715, tar: 0.027895 \n",
            "l0: 0.027594, l1: 0.025704, l2: 0.029508, l3: 0.034524, l4: 0.032991, l5: 0.039919, l6: 0.054219\n",
            "\n",
            "[epoch: 128/1000, batch:    11/   15, ite: 1916] train loss: 0.244699, tar: 0.027877 \n",
            "l0: 0.028620, l1: 0.030470, l2: 0.033755, l3: 0.033816, l4: 0.035998, l5: 0.039516, l6: 0.061524\n",
            "\n",
            "[epoch: 128/1000, batch:    12/   15, ite: 1917] train loss: 0.245817, tar: 0.027920 \n",
            "l0: 0.026469, l1: 0.022894, l2: 0.030475, l3: 0.033231, l4: 0.039536, l5: 0.047161, l6: 0.056600\n",
            "\n",
            "[epoch: 128/1000, batch:    13/   15, ite: 1918] train loss: 0.246403, tar: 0.027840 \n",
            "l0: 0.034961, l1: 0.038113, l2: 0.040826, l3: 0.040449, l4: 0.042584, l5: 0.046783, l6: 0.062745\n",
            "\n",
            "[epoch: 128/1000, batch:    14/   15, ite: 1919] train loss: 0.249564, tar: 0.028214 \n",
            "l0: 0.030179, l1: 0.031854, l2: 0.034933, l3: 0.040075, l4: 0.040451, l5: 0.040779, l6: 0.053843\n",
            "\n",
            "[epoch: 128/1000, batch:    15/   15, ite: 1920] train loss: 0.250691, tar: 0.028313 \n",
            "l0: 0.026058, l1: 0.024123, l2: 0.031921, l3: 0.033569, l4: 0.036326, l5: 0.037076, l6: 0.047820\n",
            "\n",
            "[epoch: 129/1000, batch:     1/   15, ite: 1921] train loss: 0.250034, tar: 0.028205 \n",
            "l0: 0.018881, l1: 0.016890, l2: 0.020837, l3: 0.024779, l4: 0.024156, l5: 0.026937, l6: 0.050451\n",
            "\n",
            "[epoch: 129/1000, batch:     2/   15, ite: 1922] train loss: 0.246984, tar: 0.027782 \n",
            "l0: 0.025481, l1: 0.027649, l2: 0.026313, l3: 0.025760, l4: 0.028887, l5: 0.036227, l6: 0.052017\n",
            "\n",
            "[epoch: 129/1000, batch:     3/   15, ite: 1923] train loss: 0.245912, tar: 0.027681 \n",
            "l0: 0.026076, l1: 0.024551, l2: 0.026933, l3: 0.030842, l4: 0.034146, l5: 0.037129, l6: 0.055390\n",
            "\n",
            "[epoch: 129/1000, batch:     4/   15, ite: 1924] train loss: 0.245461, tar: 0.027615 \n",
            "l0: 0.031759, l1: 0.029775, l2: 0.039721, l3: 0.045840, l4: 0.051189, l5: 0.048502, l6: 0.061789\n",
            "\n",
            "[epoch: 129/1000, batch:     5/   15, ite: 1925] train loss: 0.247985, tar: 0.027780 \n",
            "l0: 0.029705, l1: 0.033430, l2: 0.032300, l3: 0.031315, l4: 0.035964, l5: 0.037638, l6: 0.045489\n",
            "\n",
            "[epoch: 129/1000, batch:     6/   15, ite: 1926] train loss: 0.247903, tar: 0.027854 \n",
            "l0: 0.033518, l1: 0.034370, l2: 0.033203, l3: 0.032485, l4: 0.036818, l5: 0.048624, l6: 0.065611\n",
            "\n",
            "[epoch: 129/1000, batch:     7/   15, ite: 1927] train loss: 0.249263, tar: 0.028064 \n",
            "l0: 0.029936, l1: 0.023548, l2: 0.031219, l3: 0.039698, l4: 0.041360, l5: 0.045895, l6: 0.075831\n",
            "\n",
            "[epoch: 129/1000, batch:     8/   15, ite: 1928] train loss: 0.250628, tar: 0.028131 \n",
            "l0: 0.024685, l1: 0.023493, l2: 0.023457, l3: 0.027541, l4: 0.034612, l5: 0.042268, l6: 0.059353\n",
            "\n",
            "[epoch: 129/1000, batch:     9/   15, ite: 1929] train loss: 0.250103, tar: 0.028012 \n",
            "l0: 0.044425, l1: 0.036590, l2: 0.055305, l3: 0.054555, l4: 0.056961, l5: 0.061122, l6: 0.079267\n",
            "\n",
            "[epoch: 129/1000, batch:    10/   15, ite: 1930] train loss: 0.254707, tar: 0.028559 \n",
            "l0: 0.040494, l1: 0.041857, l2: 0.049091, l3: 0.052412, l4: 0.050974, l5: 0.044013, l6: 0.068528\n",
            "\n",
            "[epoch: 129/1000, batch:    11/   15, ite: 1931] train loss: 0.257696, tar: 0.028944 \n",
            "l0: 0.035216, l1: 0.032819, l2: 0.037900, l3: 0.041583, l4: 0.048035, l5: 0.041965, l6: 0.056663\n",
            "\n",
            "[epoch: 129/1000, batch:    12/   15, ite: 1932] train loss: 0.258836, tar: 0.029140 \n",
            "l0: 0.026985, l1: 0.032669, l2: 0.031220, l3: 0.033465, l4: 0.029713, l5: 0.031613, l6: 0.045001\n",
            "\n",
            "[epoch: 129/1000, batch:    13/   15, ite: 1933] train loss: 0.257983, tar: 0.029075 \n",
            "l0: 0.018798, l1: 0.019335, l2: 0.022094, l3: 0.023018, l4: 0.024590, l5: 0.028765, l6: 0.051982\n",
            "\n",
            "[epoch: 129/1000, batch:    14/   15, ite: 1934] train loss: 0.255942, tar: 0.028773 \n",
            "l0: 0.023959, l1: 0.023817, l2: 0.021775, l3: 0.023216, l4: 0.031467, l5: 0.051068, l6: 0.062345\n",
            "\n",
            "[epoch: 129/1000, batch:    15/   15, ite: 1935] train loss: 0.255419, tar: 0.028635 \n",
            "l0: 0.034498, l1: 0.033406, l2: 0.037995, l3: 0.040329, l4: 0.038294, l5: 0.051651, l6: 0.071120\n",
            "\n",
            "[epoch: 130/1000, batch:     1/   15, ite: 1936] train loss: 0.256860, tar: 0.028798 \n",
            "l0: 0.042287, l1: 0.039406, l2: 0.049107, l3: 0.057498, l4: 0.061851, l5: 0.056581, l6: 0.080197\n",
            "\n",
            "[epoch: 130/1000, batch:     2/   15, ite: 1937] train loss: 0.260375, tar: 0.029163 \n",
            "l0: 0.020278, l1: 0.017557, l2: 0.024551, l3: 0.026815, l4: 0.030622, l5: 0.041616, l6: 0.056623\n",
            "\n",
            "[epoch: 130/1000, batch:     3/   15, ite: 1938] train loss: 0.259262, tar: 0.028929 \n",
            "l0: 0.023126, l1: 0.025857, l2: 0.027325, l3: 0.029787, l4: 0.031197, l5: 0.034768, l6: 0.040538\n",
            "\n",
            "[epoch: 130/1000, batch:     4/   15, ite: 1939] train loss: 0.258065, tar: 0.028780 \n",
            "l0: 0.018816, l1: 0.019448, l2: 0.018544, l3: 0.020322, l4: 0.023958, l5: 0.031903, l6: 0.041655\n",
            "\n",
            "[epoch: 130/1000, batch:     5/   15, ite: 1940] train loss: 0.255980, tar: 0.028531 \n",
            "l0: 0.026121, l1: 0.025047, l2: 0.027193, l3: 0.033282, l4: 0.039639, l5: 0.038580, l6: 0.061181\n",
            "\n",
            "[epoch: 130/1000, batch:     6/   15, ite: 1941] train loss: 0.255859, tar: 0.028472 \n",
            "l0: 0.022700, l1: 0.023483, l2: 0.026462, l3: 0.026247, l4: 0.030942, l5: 0.035723, l6: 0.044639\n",
            "\n",
            "[epoch: 130/1000, batch:     7/   15, ite: 1942] train loss: 0.254772, tar: 0.028335 \n",
            "l0: 0.023181, l1: 0.025163, l2: 0.026853, l3: 0.031564, l4: 0.028536, l5: 0.032394, l6: 0.040966\n",
            "\n",
            "[epoch: 130/1000, batch:     8/   15, ite: 1943] train loss: 0.253700, tar: 0.028215 \n",
            "l0: 0.025588, l1: 0.022838, l2: 0.026721, l3: 0.032460, l4: 0.037102, l5: 0.049997, l6: 0.071274\n",
            "\n",
            "[epoch: 130/1000, batch:     9/   15, ite: 1944] train loss: 0.253979, tar: 0.028155 \n",
            "l0: 0.024882, l1: 0.025003, l2: 0.026195, l3: 0.031084, l4: 0.034120, l5: 0.036012, l6: 0.059679\n",
            "\n",
            "[epoch: 130/1000, batch:    10/   15, ite: 1945] train loss: 0.253601, tar: 0.028082 \n",
            "l0: 0.024919, l1: 0.023337, l2: 0.024600, l3: 0.027792, l4: 0.030710, l5: 0.034016, l6: 0.049430\n",
            "\n",
            "[epoch: 130/1000, batch:    11/   15, ite: 1946] train loss: 0.252757, tar: 0.028014 \n",
            "l0: 0.020951, l1: 0.019649, l2: 0.022992, l3: 0.026067, l4: 0.035053, l5: 0.035247, l6: 0.040380\n",
            "\n",
            "[epoch: 130/1000, batch:    12/   15, ite: 1947] train loss: 0.251642, tar: 0.027863 \n",
            "l0: 0.024145, l1: 0.024112, l2: 0.026740, l3: 0.030043, l4: 0.032666, l5: 0.035716, l6: 0.047668\n",
            "\n",
            "[epoch: 130/1000, batch:    13/   15, ite: 1948] train loss: 0.251006, tar: 0.027786 \n",
            "l0: 0.027846, l1: 0.033692, l2: 0.029474, l3: 0.033453, l4: 0.038295, l5: 0.037976, l6: 0.047910\n",
            "\n",
            "[epoch: 130/1000, batch:    14/   15, ite: 1949] train loss: 0.250958, tar: 0.027787 \n",
            "l0: 0.029106, l1: 0.031653, l2: 0.030780, l3: 0.034133, l4: 0.033566, l5: 0.044113, l6: 0.058334\n",
            "\n",
            "[epoch: 130/1000, batch:    15/   15, ite: 1950] train loss: 0.251172, tar: 0.027813 \n",
            "l0: 0.023996, l1: 0.020650, l2: 0.026343, l3: 0.032338, l4: 0.033326, l5: 0.042091, l6: 0.079797\n",
            "\n",
            "[epoch: 131/1000, batch:     1/   15, ite: 1951] train loss: 0.251317, tar: 0.027739 \n",
            "l0: 0.029695, l1: 0.032652, l2: 0.032941, l3: 0.030473, l4: 0.035928, l5: 0.035696, l6: 0.051152\n",
            "\n",
            "[epoch: 131/1000, batch:     2/   15, ite: 1952] train loss: 0.251263, tar: 0.027776 \n",
            "l0: 0.025720, l1: 0.029845, l2: 0.029898, l3: 0.028699, l4: 0.031193, l5: 0.036761, l6: 0.042631\n",
            "\n",
            "[epoch: 131/1000, batch:     3/   15, ite: 1953] train loss: 0.250763, tar: 0.027737 \n",
            "l0: 0.023054, l1: 0.022274, l2: 0.026989, l3: 0.032507, l4: 0.035385, l5: 0.034180, l6: 0.043858\n",
            "\n",
            "[epoch: 131/1000, batch:     4/   15, ite: 1954] train loss: 0.250161, tar: 0.027651 \n",
            "l0: 0.024515, l1: 0.023820, l2: 0.025782, l3: 0.030615, l4: 0.030955, l5: 0.033964, l6: 0.051106\n",
            "\n",
            "[epoch: 131/1000, batch:     5/   15, ite: 1955] train loss: 0.249626, tar: 0.027594 \n",
            "l0: 0.023628, l1: 0.028404, l2: 0.027296, l3: 0.030245, l4: 0.030535, l5: 0.026497, l6: 0.033936\n",
            "\n",
            "[epoch: 131/1000, batch:     6/   15, ite: 1956] train loss: 0.248750, tar: 0.027523 \n",
            "l0: 0.055434, l1: 0.062613, l2: 0.056263, l3: 0.060495, l4: 0.067413, l5: 0.062343, l6: 0.072306\n",
            "\n",
            "[epoch: 131/1000, batch:     7/   15, ite: 1957] train loss: 0.252050, tar: 0.028013 \n",
            "l0: 0.022346, l1: 0.019824, l2: 0.021982, l3: 0.025262, l4: 0.028179, l5: 0.035226, l6: 0.047237\n",
            "\n",
            "[epoch: 131/1000, batch:     8/   15, ite: 1958] train loss: 0.251153, tar: 0.027915 \n",
            "l0: 0.021942, l1: 0.020345, l2: 0.021233, l3: 0.023727, l4: 0.027594, l5: 0.035191, l6: 0.062768\n",
            "\n",
            "[epoch: 131/1000, batch:     9/   15, ite: 1959] train loss: 0.250503, tar: 0.027814 \n",
            "l0: 0.024153, l1: 0.024858, l2: 0.026144, l3: 0.026827, l4: 0.026783, l5: 0.034702, l6: 0.051042\n",
            "\n",
            "[epoch: 131/1000, batch:    10/   15, ite: 1960] train loss: 0.249903, tar: 0.027753 \n",
            "l0: 0.016922, l1: 0.016328, l2: 0.018443, l3: 0.022682, l4: 0.025673, l5: 0.028614, l6: 0.037815\n",
            "\n",
            "[epoch: 131/1000, batch:    11/   15, ite: 1961] train loss: 0.248536, tar: 0.027575 \n",
            "l0: 0.030935, l1: 0.031679, l2: 0.034362, l3: 0.040299, l4: 0.042049, l5: 0.040650, l6: 0.057200\n",
            "\n",
            "[epoch: 131/1000, batch:    12/   15, ite: 1962] train loss: 0.248998, tar: 0.027629 \n",
            "l0: 0.027668, l1: 0.029068, l2: 0.033719, l3: 0.033281, l4: 0.033576, l5: 0.033072, l6: 0.050967\n",
            "\n",
            "[epoch: 131/1000, batch:    13/   15, ite: 1963] train loss: 0.248876, tar: 0.027630 \n",
            "l0: 0.026132, l1: 0.025607, l2: 0.028670, l3: 0.028705, l4: 0.033625, l5: 0.039369, l6: 0.052783\n",
            "\n",
            "[epoch: 131/1000, batch:    14/   15, ite: 1964] train loss: 0.248658, tar: 0.027606 \n",
            "l0: 0.024755, l1: 0.023618, l2: 0.025274, l3: 0.028826, l4: 0.032740, l5: 0.035640, l6: 0.049412\n",
            "\n",
            "[epoch: 131/1000, batch:    15/   15, ite: 1965] train loss: 0.248221, tar: 0.027563 \n",
            "l0: 0.031313, l1: 0.032765, l2: 0.040411, l3: 0.034884, l4: 0.039560, l5: 0.037650, l6: 0.053405\n",
            "\n",
            "[epoch: 132/1000, batch:     1/   15, ite: 1966] train loss: 0.248551, tar: 0.027619 \n",
            "l0: 0.047087, l1: 0.046985, l2: 0.052091, l3: 0.055522, l4: 0.052953, l5: 0.057104, l6: 0.089007\n",
            "\n",
            "[epoch: 132/1000, batch:     2/   15, ite: 1967] train loss: 0.250822, tar: 0.027910 \n",
            "l0: 0.025307, l1: 0.025860, l2: 0.031392, l3: 0.032670, l4: 0.032547, l5: 0.031535, l6: 0.051010\n",
            "\n",
            "[epoch: 132/1000, batch:     3/   15, ite: 1968] train loss: 0.250521, tar: 0.027872 \n",
            "l0: 0.019863, l1: 0.018122, l2: 0.019882, l3: 0.021839, l4: 0.030075, l5: 0.032867, l6: 0.040702\n",
            "\n",
            "[epoch: 132/1000, batch:     4/   15, ite: 1969] train loss: 0.249547, tar: 0.027756 \n",
            "l0: 0.024804, l1: 0.024991, l2: 0.025301, l3: 0.030668, l4: 0.035442, l5: 0.040789, l6: 0.041811\n",
            "\n",
            "[epoch: 132/1000, batch:     5/   15, ite: 1970] train loss: 0.249180, tar: 0.027713 \n",
            "l0: 0.020812, l1: 0.020615, l2: 0.021822, l3: 0.025132, l4: 0.027190, l5: 0.032723, l6: 0.040849\n",
            "\n",
            "[epoch: 132/1000, batch:     6/   15, ite: 1971] train loss: 0.248334, tar: 0.027616 \n",
            "l0: 0.025417, l1: 0.024137, l2: 0.025283, l3: 0.032226, l4: 0.033349, l5: 0.036681, l6: 0.054292\n",
            "\n",
            "[epoch: 132/1000, batch:     7/   15, ite: 1972] train loss: 0.248099, tar: 0.027586 \n",
            "l0: 0.015882, l1: 0.015700, l2: 0.017819, l3: 0.021566, l4: 0.024742, l5: 0.028870, l6: 0.034575\n",
            "\n",
            "[epoch: 132/1000, batch:     8/   15, ite: 1973] train loss: 0.246880, tar: 0.027425 \n",
            "l0: 0.042587, l1: 0.039207, l2: 0.046954, l3: 0.049283, l4: 0.047193, l5: 0.050377, l6: 0.069941\n",
            "\n",
            "[epoch: 132/1000, batch:     9/   15, ite: 1974] train loss: 0.248214, tar: 0.027630 \n",
            "l0: 0.025485, l1: 0.025572, l2: 0.030545, l3: 0.030834, l4: 0.034800, l5: 0.044806, l6: 0.050713\n",
            "\n",
            "[epoch: 132/1000, batch:    10/   15, ite: 1975] train loss: 0.248141, tar: 0.027602 \n",
            "l0: 0.020095, l1: 0.018407, l2: 0.022773, l3: 0.027303, l4: 0.026654, l5: 0.029567, l6: 0.046516\n",
            "\n",
            "[epoch: 132/1000, batch:    11/   15, ite: 1976] train loss: 0.247393, tar: 0.027503 \n",
            "l0: 0.028151, l1: 0.032128, l2: 0.030473, l3: 0.033755, l4: 0.037556, l5: 0.044462, l6: 0.053631\n",
            "\n",
            "[epoch: 132/1000, batch:    12/   15, ite: 1977] train loss: 0.247559, tar: 0.027511 \n",
            "l0: 0.020641, l1: 0.020519, l2: 0.022472, l3: 0.028127, l4: 0.035592, l5: 0.039954, l6: 0.053693\n",
            "\n",
            "[epoch: 132/1000, batch:    13/   15, ite: 1978] train loss: 0.247218, tar: 0.027423 \n",
            "l0: 0.024567, l1: 0.026476, l2: 0.026013, l3: 0.027467, l4: 0.030219, l5: 0.031780, l6: 0.050764\n",
            "\n",
            "[epoch: 132/1000, batch:    14/   15, ite: 1979] train loss: 0.246839, tar: 0.027387 \n",
            "l0: 0.028179, l1: 0.028821, l2: 0.030907, l3: 0.036226, l4: 0.031963, l5: 0.033895, l6: 0.045224\n",
            "\n",
            "[epoch: 132/1000, batch:    15/   15, ite: 1980] train loss: 0.246694, tar: 0.027397 \n",
            "l0: 0.017262, l1: 0.016636, l2: 0.019727, l3: 0.023265, l4: 0.022186, l5: 0.029190, l6: 0.047412\n",
            "\n",
            "[epoch: 133/1000, batch:     1/   15, ite: 1981] train loss: 0.245817, tar: 0.027272 \n",
            "l0: 0.027271, l1: 0.026708, l2: 0.036789, l3: 0.037485, l4: 0.035931, l5: 0.032863, l6: 0.054305\n",
            "\n",
            "[epoch: 133/1000, batch:     2/   15, ite: 1982] train loss: 0.245885, tar: 0.027272 \n",
            "l0: 0.025227, l1: 0.025132, l2: 0.030713, l3: 0.033498, l4: 0.034263, l5: 0.035247, l6: 0.047807\n",
            "\n",
            "[epoch: 133/1000, batch:     3/   15, ite: 1983] train loss: 0.245716, tar: 0.027247 \n",
            "l0: 0.022419, l1: 0.024869, l2: 0.026377, l3: 0.028402, l4: 0.033045, l5: 0.038011, l6: 0.048786\n",
            "\n",
            "[epoch: 133/1000, batch:     4/   15, ite: 1984] train loss: 0.245433, tar: 0.027190 \n",
            "l0: 0.024464, l1: 0.020373, l2: 0.024898, l3: 0.031929, l4: 0.035282, l5: 0.049696, l6: 0.072880\n",
            "\n",
            "[epoch: 133/1000, batch:     5/   15, ite: 1985] train loss: 0.245599, tar: 0.027158 \n",
            "l0: 0.034896, l1: 0.033517, l2: 0.035438, l3: 0.038526, l4: 0.049346, l5: 0.051612, l6: 0.074074\n",
            "\n",
            "[epoch: 133/1000, batch:     6/   15, ite: 1986] train loss: 0.246434, tar: 0.027248 \n",
            "l0: 0.029638, l1: 0.028910, l2: 0.030521, l3: 0.037104, l4: 0.035443, l5: 0.038749, l6: 0.052732\n",
            "\n",
            "[epoch: 133/1000, batch:     7/   15, ite: 1987] train loss: 0.246510, tar: 0.027275 \n",
            "l0: 0.030650, l1: 0.028623, l2: 0.034858, l3: 0.037324, l4: 0.039974, l5: 0.043939, l6: 0.063316\n",
            "\n",
            "[epoch: 133/1000, batch:     8/   15, ite: 1988] train loss: 0.246876, tar: 0.027313 \n",
            "l0: 0.026540, l1: 0.024412, l2: 0.030427, l3: 0.032020, l4: 0.035797, l5: 0.034566, l6: 0.057076\n",
            "\n",
            "[epoch: 133/1000, batch:     9/   15, ite: 1989] train loss: 0.246808, tar: 0.027305 \n",
            "l0: 0.023065, l1: 0.022337, l2: 0.024099, l3: 0.027045, l4: 0.030437, l5: 0.030728, l6: 0.048184\n",
            "\n",
            "[epoch: 133/1000, batch:    10/   15, ite: 1990] train loss: 0.246353, tar: 0.027258 \n",
            "l0: 0.030980, l1: 0.029138, l2: 0.031762, l3: 0.034721, l4: 0.033813, l5: 0.040055, l6: 0.044504\n",
            "\n",
            "[epoch: 133/1000, batch:    11/   15, ite: 1991] train loss: 0.246338, tar: 0.027299 \n",
            "l0: 0.020564, l1: 0.018986, l2: 0.020480, l3: 0.023595, l4: 0.024973, l5: 0.031253, l6: 0.036048\n",
            "\n",
            "[epoch: 133/1000, batch:    12/   15, ite: 1992] train loss: 0.245573, tar: 0.027225 \n",
            "l0: 0.032176, l1: 0.032100, l2: 0.037779, l3: 0.040253, l4: 0.034737, l5: 0.040305, l6: 0.042977\n",
            "\n",
            "[epoch: 133/1000, batch:    13/   15, ite: 1993] train loss: 0.245731, tar: 0.027279 \n",
            "l0: 0.025512, l1: 0.027071, l2: 0.028679, l3: 0.030026, l4: 0.033280, l5: 0.039622, l6: 0.049276\n",
            "\n",
            "[epoch: 133/1000, batch:    14/   15, ite: 1994] train loss: 0.245601, tar: 0.027260 \n",
            "l0: 0.022565, l1: 0.023605, l2: 0.023944, l3: 0.027273, l4: 0.030129, l5: 0.037419, l6: 0.052992\n",
            "\n",
            "[epoch: 133/1000, batch:    15/   15, ite: 1995] train loss: 0.245309, tar: 0.027210 \n",
            "l0: 0.026478, l1: 0.033453, l2: 0.031965, l3: 0.027640, l4: 0.024971, l5: 0.030298, l6: 0.048691\n",
            "\n",
            "[epoch: 134/1000, batch:     1/   15, ite: 1996] train loss: 0.245082, tar: 0.027203 \n",
            "l0: 0.038837, l1: 0.042261, l2: 0.047309, l3: 0.042334, l4: 0.051793, l5: 0.052365, l6: 0.090015\n",
            "\n",
            "[epoch: 134/1000, batch:     2/   15, ite: 1997] train loss: 0.246318, tar: 0.027323 \n",
            "l0: 0.018337, l1: 0.025299, l2: 0.017399, l3: 0.018638, l4: 0.019809, l5: 0.029153, l6: 0.032241\n",
            "\n",
            "[epoch: 134/1000, batch:     3/   15, ite: 1998] train loss: 0.245446, tar: 0.027231 \n",
            "l0: 0.031520, l1: 0.038416, l2: 0.033415, l3: 0.033727, l4: 0.035415, l5: 0.042339, l6: 0.051576\n",
            "\n",
            "[epoch: 134/1000, batch:     4/   15, ite: 1999] train loss: 0.245657, tar: 0.027274 \n",
            "l0: 0.027321, l1: 0.029311, l2: 0.031510, l3: 0.032312, l4: 0.034661, l5: 0.037773, l6: 0.051225\n",
            "\n",
            "[epoch: 134/1000, batch:     5/   15, ite: 2000] train loss: 0.245642, tar: 0.027275 \n",
            "l0: 0.051109, l1: 0.051648, l2: 0.055980, l3: 0.058761, l4: 0.068897, l5: 0.064969, l6: 0.067566\n",
            "\n",
            "[epoch: 134/1000, batch:     6/   15, ite: 2001] train loss: 0.418932, tar: 0.051109 \n",
            "l0: 0.016824, l1: 0.015914, l2: 0.016650, l3: 0.018384, l4: 0.022455, l5: 0.027545, l6: 0.043606\n",
            "\n",
            "[epoch: 134/1000, batch:     7/   15, ite: 2002] train loss: 0.290155, tar: 0.033967 \n",
            "l0: 0.030887, l1: 0.031328, l2: 0.032125, l3: 0.030567, l4: 0.045967, l5: 0.056048, l6: 0.058526\n",
            "\n",
            "[epoch: 134/1000, batch:     8/   15, ite: 2003] train loss: 0.288586, tar: 0.032940 \n",
            "l0: 0.030277, l1: 0.033185, l2: 0.036035, l3: 0.034012, l4: 0.035561, l5: 0.043555, l6: 0.073024\n",
            "\n",
            "[epoch: 134/1000, batch:     9/   15, ite: 2004] train loss: 0.287852, tar: 0.032274 \n",
            "l0: 0.027783, l1: 0.027261, l2: 0.029068, l3: 0.031383, l4: 0.033222, l5: 0.040173, l6: 0.059426\n",
            "\n",
            "[epoch: 134/1000, batch:    10/   15, ite: 2005] train loss: 0.279945, tar: 0.031376 \n",
            "l0: 0.032186, l1: 0.032471, l2: 0.035014, l3: 0.030964, l4: 0.035277, l5: 0.038233, l6: 0.048964\n",
            "\n",
            "[epoch: 134/1000, batch:    11/   15, ite: 2006] train loss: 0.275472, tar: 0.031511 \n",
            "l0: 0.035337, l1: 0.038025, l2: 0.033000, l3: 0.037412, l4: 0.046977, l5: 0.050550, l6: 0.056787\n",
            "\n",
            "[epoch: 134/1000, batch:    12/   15, ite: 2007] train loss: 0.278703, tar: 0.032058 \n",
            "l0: 0.024574, l1: 0.023324, l2: 0.024315, l3: 0.028585, l4: 0.029756, l5: 0.034555, l6: 0.044621\n",
            "\n",
            "[epoch: 134/1000, batch:    13/   15, ite: 2008] train loss: 0.270081, tar: 0.031122 \n",
            "l0: 0.033481, l1: 0.035339, l2: 0.033002, l3: 0.036130, l4: 0.039432, l5: 0.036391, l6: 0.042263\n",
            "\n",
            "[epoch: 134/1000, batch:    14/   15, ite: 2009] train loss: 0.268520, tar: 0.031384 \n",
            "l0: 0.024605, l1: 0.022974, l2: 0.039358, l3: 0.035066, l4: 0.031662, l5: 0.029490, l6: 0.049743\n",
            "\n",
            "[epoch: 134/1000, batch:    15/   15, ite: 2010] train loss: 0.264958, tar: 0.030706 \n",
            "l0: 0.043279, l1: 0.040818, l2: 0.047961, l3: 0.054141, l4: 0.055583, l5: 0.046062, l6: 0.058282\n",
            "\n",
            "[epoch: 135/1000, batch:     1/   15, ite: 2011] train loss: 0.272337, tar: 0.031849 \n",
            "l0: 0.033413, l1: 0.034207, l2: 0.040629, l3: 0.042377, l4: 0.039843, l5: 0.041068, l6: 0.065754\n",
            "\n",
            "[epoch: 135/1000, batch:     2/   15, ite: 2012] train loss: 0.274417, tar: 0.031980 \n",
            "l0: 0.019838, l1: 0.020956, l2: 0.022830, l3: 0.028116, l4: 0.026913, l5: 0.028261, l6: 0.050743\n",
            "\n",
            "[epoch: 135/1000, batch:     3/   15, ite: 2013] train loss: 0.268512, tar: 0.031046 \n",
            "l0: 0.021355, l1: 0.022490, l2: 0.024645, l3: 0.029234, l4: 0.032556, l5: 0.041018, l6: 0.057949\n",
            "\n",
            "[epoch: 135/1000, batch:     4/   15, ite: 2014] train loss: 0.265707, tar: 0.030353 \n",
            "l0: 0.024234, l1: 0.025633, l2: 0.026921, l3: 0.023650, l4: 0.026594, l5: 0.039779, l6: 0.052459\n",
            "\n",
            "[epoch: 135/1000, batch:     5/   15, ite: 2015] train loss: 0.262611, tar: 0.029945 \n",
            "l0: 0.026279, l1: 0.025727, l2: 0.032325, l3: 0.030757, l4: 0.035357, l5: 0.040734, l6: 0.063856\n",
            "\n",
            "[epoch: 135/1000, batch:     6/   15, ite: 2016] train loss: 0.262138, tar: 0.029716 \n",
            "l0: 0.024003, l1: 0.027456, l2: 0.031140, l3: 0.027793, l4: 0.030811, l5: 0.038744, l6: 0.058802\n",
            "\n",
            "[epoch: 135/1000, batch:     7/   15, ite: 2017] train loss: 0.260762, tar: 0.029380 \n",
            "l0: 0.049729, l1: 0.061970, l2: 0.066044, l3: 0.049861, l4: 0.051812, l5: 0.048083, l6: 0.047835\n",
            "\n",
            "[epoch: 135/1000, batch:     8/   15, ite: 2018] train loss: 0.267127, tar: 0.030511 \n",
            "l0: 0.026712, l1: 0.027633, l2: 0.034717, l3: 0.035346, l4: 0.041778, l5: 0.037556, l6: 0.048725\n",
            "\n",
            "[epoch: 135/1000, batch:     9/   15, ite: 2019] train loss: 0.266356, tar: 0.030311 \n",
            "l0: 0.027483, l1: 0.033076, l2: 0.031327, l3: 0.031643, l4: 0.033324, l5: 0.036415, l6: 0.044772\n",
            "\n",
            "[epoch: 135/1000, batch:    10/   15, ite: 2020] train loss: 0.264940, tar: 0.030169 \n",
            "l0: 0.034662, l1: 0.049668, l2: 0.031584, l3: 0.037207, l4: 0.031395, l5: 0.036322, l6: 0.062855\n",
            "\n",
            "[epoch: 135/1000, batch:    11/   15, ite: 2021] train loss: 0.265833, tar: 0.030383 \n",
            "l0: 0.048473, l1: 0.040516, l2: 0.052222, l3: 0.056652, l4: 0.062536, l5: 0.068081, l6: 0.089011\n",
            "\n",
            "[epoch: 135/1000, batch:    12/   15, ite: 2022] train loss: 0.272726, tar: 0.031206 \n",
            "l0: 0.019019, l1: 0.019643, l2: 0.021684, l3: 0.022897, l4: 0.024951, l5: 0.029926, l6: 0.044186\n",
            "\n",
            "[epoch: 135/1000, batch:    13/   15, ite: 2023] train loss: 0.268795, tar: 0.030676 \n",
            "l0: 0.016926, l1: 0.017205, l2: 0.017472, l3: 0.020494, l4: 0.022834, l5: 0.028697, l6: 0.040326\n",
            "\n",
            "[epoch: 135/1000, batch:    14/   15, ite: 2024] train loss: 0.264427, tar: 0.030103 \n",
            "l0: 0.031329, l1: 0.034484, l2: 0.034799, l3: 0.034353, l4: 0.036548, l5: 0.037554, l6: 0.083595\n",
            "\n",
            "[epoch: 135/1000, batch:    15/   15, ite: 2025] train loss: 0.265556, tar: 0.030152 \n",
            "l0: 0.038013, l1: 0.039602, l2: 0.043686, l3: 0.045786, l4: 0.052359, l5: 0.050928, l6: 0.075395\n",
            "\n",
            "[epoch: 136/1000, batch:     1/   15, ite: 2026] train loss: 0.268641, tar: 0.030454 \n",
            "l0: 0.031793, l1: 0.031585, l2: 0.036148, l3: 0.035197, l4: 0.039201, l5: 0.039104, l6: 0.050810\n",
            "\n",
            "[epoch: 136/1000, batch:     2/   15, ite: 2027] train loss: 0.268463, tar: 0.030504 \n",
            "l0: 0.026002, l1: 0.026108, l2: 0.029969, l3: 0.029826, l4: 0.032863, l5: 0.035360, l6: 0.052767\n",
            "\n",
            "[epoch: 136/1000, batch:     3/   15, ite: 2028] train loss: 0.267193, tar: 0.030343 \n",
            "l0: 0.058068, l1: 0.049402, l2: 0.053684, l3: 0.062940, l4: 0.086810, l5: 0.082705, l6: 0.083048\n",
            "\n",
            "[epoch: 136/1000, batch:     4/   15, ite: 2029] train loss: 0.274416, tar: 0.031299 \n",
            "l0: 0.087143, l1: 0.094069, l2: 0.085779, l3: 0.095376, l4: 0.077090, l5: 0.066267, l6: 0.051967\n",
            "\n",
            "[epoch: 136/1000, batch:     5/   15, ite: 2030] train loss: 0.283858, tar: 0.033161 \n",
            "l0: 0.018029, l1: 0.017315, l2: 0.017202, l3: 0.020973, l4: 0.024565, l5: 0.032693, l6: 0.071386\n",
            "\n",
            "[epoch: 136/1000, batch:     6/   15, ite: 2031] train loss: 0.281223, tar: 0.032672 \n",
            "l0: 0.022409, l1: 0.021098, l2: 0.023362, l3: 0.025801, l4: 0.029850, l5: 0.039900, l6: 0.076194\n",
            "\n",
            "[epoch: 136/1000, batch:     7/   15, ite: 2032] train loss: 0.279892, tar: 0.032352 \n",
            "l0: 0.038196, l1: 0.039083, l2: 0.039850, l3: 0.046344, l4: 0.054579, l5: 0.061835, l6: 0.093183\n",
            "\n",
            "[epoch: 136/1000, batch:     8/   15, ite: 2033] train loss: 0.282715, tar: 0.032529 \n",
            "l0: 0.045242, l1: 0.044207, l2: 0.046145, l3: 0.055096, l4: 0.065371, l5: 0.085305, l6: 0.118743\n",
            "\n",
            "[epoch: 136/1000, batch:     9/   15, ite: 2034] train loss: 0.287933, tar: 0.032903 \n",
            "l0: 0.044110, l1: 0.035021, l2: 0.054519, l3: 0.056816, l4: 0.070358, l5: 0.075644, l6: 0.069022\n",
            "\n",
            "[epoch: 136/1000, batch:    10/   15, ite: 2035] train loss: 0.291291, tar: 0.033223 \n",
            "l0: 0.040658, l1: 0.036986, l2: 0.045416, l3: 0.051918, l4: 0.067071, l5: 0.060673, l6: 0.075985\n",
            "\n",
            "[epoch: 136/1000, batch:    11/   15, ite: 2036] train loss: 0.293720, tar: 0.033429 \n",
            "l0: 0.028878, l1: 0.027090, l2: 0.029266, l3: 0.033235, l4: 0.042430, l5: 0.052667, l6: 0.056425\n",
            "\n",
            "[epoch: 136/1000, batch:    12/   15, ite: 2037] train loss: 0.293078, tar: 0.033306 \n",
            "l0: 0.022973, l1: 0.022603, l2: 0.030533, l3: 0.029051, l4: 0.031837, l5: 0.033234, l6: 0.052577\n",
            "\n",
            "[epoch: 136/1000, batch:    13/   15, ite: 2038] train loss: 0.291229, tar: 0.033034 \n",
            "l0: 0.027282, l1: 0.025995, l2: 0.026764, l3: 0.028930, l4: 0.042393, l5: 0.048683, l6: 0.063756\n",
            "\n",
            "[epoch: 136/1000, batch:    14/   15, ite: 2039] train loss: 0.290526, tar: 0.032887 \n",
            "l0: 0.027743, l1: 0.030248, l2: 0.033348, l3: 0.033805, l4: 0.035297, l5: 0.041762, l6: 0.053280\n",
            "\n",
            "[epoch: 136/1000, batch:    15/   15, ite: 2040] train loss: 0.289650, tar: 0.032758 \n",
            "l0: 0.022418, l1: 0.022208, l2: 0.025467, l3: 0.028293, l4: 0.029250, l5: 0.030008, l6: 0.063470\n",
            "\n",
            "[epoch: 137/1000, batch:     1/   15, ite: 2041] train loss: 0.287978, tar: 0.032506 \n",
            "l0: 0.058166, l1: 0.050871, l2: 0.056041, l3: 0.063306, l4: 0.075379, l5: 0.087182, l6: 0.124969\n",
            "\n",
            "[epoch: 137/1000, batch:     2/   15, ite: 2042] train loss: 0.293405, tar: 0.033117 \n",
            "l0: 0.029903, l1: 0.034586, l2: 0.032919, l3: 0.032385, l4: 0.038906, l5: 0.045827, l6: 0.062333\n",
            "\n",
            "[epoch: 137/1000, batch:     3/   15, ite: 2043] train loss: 0.293020, tar: 0.033042 \n",
            "l0: 0.035914, l1: 0.034670, l2: 0.047072, l3: 0.043941, l4: 0.040551, l5: 0.041518, l6: 0.065095\n",
            "\n",
            "[epoch: 137/1000, batch:     4/   15, ite: 2044] train loss: 0.293378, tar: 0.033108 \n",
            "l0: 0.042972, l1: 0.043432, l2: 0.046957, l3: 0.054326, l4: 0.055554, l5: 0.051974, l6: 0.074002\n",
            "\n",
            "[epoch: 137/1000, batch:     5/   15, ite: 2045] train loss: 0.295063, tar: 0.033327 \n",
            "l0: 0.029448, l1: 0.023715, l2: 0.028587, l3: 0.035419, l4: 0.045240, l5: 0.057152, l6: 0.064981\n",
            "\n",
            "[epoch: 137/1000, batch:     6/   15, ite: 2046] train loss: 0.294835, tar: 0.033243 \n",
            "l0: 0.027891, l1: 0.031736, l2: 0.030122, l3: 0.030359, l4: 0.030963, l5: 0.044680, l6: 0.072036\n",
            "\n",
            "[epoch: 137/1000, batch:     7/   15, ite: 2047] train loss: 0.294259, tar: 0.033129 \n",
            "l0: 0.028833, l1: 0.025649, l2: 0.038543, l3: 0.041642, l4: 0.039269, l5: 0.041623, l6: 0.050806\n",
            "\n",
            "[epoch: 137/1000, batch:     8/   15, ite: 2048] train loss: 0.293678, tar: 0.033039 \n",
            "l0: 0.020549, l1: 0.020684, l2: 0.023259, l3: 0.024032, l4: 0.030130, l5: 0.039649, l6: 0.063777\n",
            "\n",
            "[epoch: 137/1000, batch:     9/   15, ite: 2049] train loss: 0.292217, tar: 0.032784 \n",
            "l0: 0.018858, l1: 0.018011, l2: 0.022678, l3: 0.025845, l4: 0.024966, l5: 0.027059, l6: 0.057336\n",
            "\n",
            "[epoch: 137/1000, batch:    10/   15, ite: 2050] train loss: 0.290268, tar: 0.032506 \n",
            "l0: 0.052944, l1: 0.052144, l2: 0.054543, l3: 0.065651, l4: 0.071985, l5: 0.078071, l6: 0.093197\n",
            "\n",
            "[epoch: 137/1000, batch:    11/   15, ite: 2051] train loss: 0.293763, tar: 0.032907 \n",
            "l0: 0.034601, l1: 0.038805, l2: 0.039524, l3: 0.038100, l4: 0.037873, l5: 0.039321, l6: 0.064278\n",
            "\n",
            "[epoch: 137/1000, batch:    12/   15, ite: 2052] train loss: 0.293739, tar: 0.032939 \n",
            "l0: 0.088866, l1: 0.095596, l2: 0.093831, l3: 0.083026, l4: 0.075706, l5: 0.093644, l6: 0.105593\n",
            "\n",
            "[epoch: 137/1000, batch:    13/   15, ite: 2053] train loss: 0.300202, tar: 0.033994 \n",
            "l0: 0.033608, l1: 0.032993, l2: 0.036226, l3: 0.040715, l4: 0.040839, l5: 0.040649, l6: 0.051711\n",
            "\n",
            "[epoch: 137/1000, batch:    14/   15, ite: 2054] train loss: 0.299767, tar: 0.033987 \n",
            "l0: 0.025291, l1: 0.025313, l2: 0.025841, l3: 0.027589, l4: 0.029087, l5: 0.035706, l6: 0.049223\n",
            "\n",
            "[epoch: 137/1000, batch:    15/   15, ite: 2055] train loss: 0.298281, tar: 0.033829 \n",
            "l0: 0.033459, l1: 0.030901, l2: 0.037090, l3: 0.043298, l4: 0.048365, l5: 0.052680, l6: 0.075656\n",
            "\n",
            "[epoch: 138/1000, batch:     1/   15, ite: 2056] train loss: 0.298695, tar: 0.033822 \n",
            "l0: 0.023565, l1: 0.024004, l2: 0.027312, l3: 0.029915, l4: 0.031963, l5: 0.035149, l6: 0.045330\n",
            "\n",
            "[epoch: 138/1000, batch:     2/   15, ite: 2057] train loss: 0.297266, tar: 0.033643 \n",
            "l0: 0.023414, l1: 0.023495, l2: 0.027574, l3: 0.031455, l4: 0.034513, l5: 0.037950, l6: 0.061664\n",
            "\n",
            "[epoch: 138/1000, batch:     3/   15, ite: 2058] train loss: 0.296280, tar: 0.033466 \n",
            "l0: 0.020582, l1: 0.020621, l2: 0.022750, l3: 0.025816, l4: 0.028808, l5: 0.035528, l6: 0.058784\n",
            "\n",
            "[epoch: 138/1000, batch:     4/   15, ite: 2059] train loss: 0.294866, tar: 0.033248 \n",
            "l0: 0.029828, l1: 0.029284, l2: 0.028362, l3: 0.030978, l4: 0.044852, l5: 0.057506, l6: 0.062304\n",
            "\n",
            "[epoch: 138/1000, batch:     5/   15, ite: 2060] train loss: 0.294671, tar: 0.033191 \n",
            "l0: 0.063747, l1: 0.071810, l2: 0.063331, l3: 0.065531, l4: 0.063162, l5: 0.077255, l6: 0.093431\n",
            "\n",
            "[epoch: 138/1000, batch:     6/   15, ite: 2061] train loss: 0.298008, tar: 0.033692 \n",
            "l0: 0.023811, l1: 0.023522, l2: 0.033306, l3: 0.031756, l4: 0.033445, l5: 0.034710, l6: 0.041891\n",
            "\n",
            "[epoch: 138/1000, batch:     7/   15, ite: 2062] train loss: 0.296789, tar: 0.033532 \n",
            "l0: 0.033764, l1: 0.029428, l2: 0.046811, l3: 0.046430, l4: 0.045246, l5: 0.050807, l6: 0.071481\n",
            "\n",
            "[epoch: 138/1000, batch:     8/   15, ite: 2063] train loss: 0.297221, tar: 0.033536 \n",
            "l0: 0.028544, l1: 0.027649, l2: 0.036593, l3: 0.040031, l4: 0.038918, l5: 0.043153, l6: 0.062267\n",
            "\n",
            "[epoch: 138/1000, batch:     9/   15, ite: 2064] train loss: 0.296907, tar: 0.033458 \n",
            "l0: 0.026326, l1: 0.026403, l2: 0.027942, l3: 0.029214, l4: 0.037665, l5: 0.038734, l6: 0.051248\n",
            "\n",
            "[epoch: 138/1000, batch:    10/   15, ite: 2065] train loss: 0.295994, tar: 0.033348 \n",
            "l0: 0.041298, l1: 0.043249, l2: 0.052572, l3: 0.050678, l4: 0.042498, l5: 0.044490, l6: 0.047746\n",
            "\n",
            "[epoch: 138/1000, batch:    11/   15, ite: 2066] train loss: 0.296396, tar: 0.033469 \n",
            "l0: 0.043480, l1: 0.048445, l2: 0.058069, l3: 0.053906, l4: 0.049400, l5: 0.049763, l6: 0.075877\n",
            "\n",
            "[epoch: 138/1000, batch:    12/   15, ite: 2067] train loss: 0.297628, tar: 0.033618 \n",
            "l0: 0.035128, l1: 0.038421, l2: 0.042719, l3: 0.040297, l4: 0.044177, l5: 0.040462, l6: 0.050163\n",
            "\n",
            "[epoch: 138/1000, batch:    13/   15, ite: 2068] train loss: 0.297536, tar: 0.033640 \n",
            "l0: 0.017774, l1: 0.017661, l2: 0.018627, l3: 0.020681, l4: 0.023808, l5: 0.030293, l6: 0.045048\n",
            "\n",
            "[epoch: 138/1000, batch:    14/   15, ite: 2069] train loss: 0.295744, tar: 0.033410 \n",
            "l0: 0.020871, l1: 0.018935, l2: 0.023378, l3: 0.027620, l4: 0.030901, l5: 0.037620, l6: 0.064879\n",
            "\n",
            "[epoch: 138/1000, batch:    15/   15, ite: 2070] train loss: 0.294722, tar: 0.033231 \n",
            "l0: 0.028944, l1: 0.026066, l2: 0.034968, l3: 0.035056, l4: 0.039619, l5: 0.050325, l6: 0.065817\n",
            "\n",
            "[epoch: 139/1000, batch:     1/   15, ite: 2071] train loss: 0.294526, tar: 0.033171 \n",
            "l0: 0.029328, l1: 0.029976, l2: 0.030352, l3: 0.035560, l4: 0.036519, l5: 0.048411, l6: 0.054089\n",
            "\n",
            "[epoch: 139/1000, batch:     2/   15, ite: 2072] train loss: 0.294105, tar: 0.033118 \n",
            "l0: 0.035694, l1: 0.034929, l2: 0.039814, l3: 0.044197, l4: 0.054176, l5: 0.051230, l6: 0.066335\n",
            "\n",
            "[epoch: 139/1000, batch:     3/   15, ite: 2073] train loss: 0.294547, tar: 0.033153 \n",
            "l0: 0.017278, l1: 0.017323, l2: 0.018593, l3: 0.019845, l4: 0.024503, l5: 0.028126, l6: 0.039979\n",
            "\n",
            "[epoch: 139/1000, batch:     4/   15, ite: 2074] train loss: 0.292805, tar: 0.032938 \n",
            "l0: 0.062752, l1: 0.059331, l2: 0.060855, l3: 0.071356, l4: 0.070325, l5: 0.071405, l6: 0.098997\n",
            "\n",
            "[epoch: 139/1000, batch:     5/   15, ite: 2075] train loss: 0.295501, tar: 0.033336 \n",
            "l0: 0.024029, l1: 0.028080, l2: 0.025423, l3: 0.027874, l4: 0.033411, l5: 0.040497, l6: 0.043230\n",
            "\n",
            "[epoch: 139/1000, batch:     6/   15, ite: 2076] train loss: 0.294541, tar: 0.033213 \n",
            "l0: 0.021277, l1: 0.019081, l2: 0.027354, l3: 0.029925, l4: 0.033081, l5: 0.034635, l6: 0.050883\n",
            "\n",
            "[epoch: 139/1000, batch:     7/   15, ite: 2077] train loss: 0.293524, tar: 0.033058 \n",
            "l0: 0.039377, l1: 0.056117, l2: 0.047601, l3: 0.034891, l4: 0.040088, l5: 0.045829, l6: 0.043608\n",
            "\n",
            "[epoch: 139/1000, batch:     8/   15, ite: 2078] train loss: 0.293704, tar: 0.033139 \n",
            "l0: 0.024880, l1: 0.024465, l2: 0.030654, l3: 0.034236, l4: 0.040859, l5: 0.040218, l6: 0.063409\n",
            "\n",
            "[epoch: 139/1000, batch:     9/   15, ite: 2079] train loss: 0.293261, tar: 0.033035 \n",
            "l0: 0.019483, l1: 0.015926, l2: 0.024138, l3: 0.028611, l4: 0.030360, l5: 0.041091, l6: 0.061961\n",
            "\n",
            "[epoch: 139/1000, batch:    10/   15, ite: 2080] train loss: 0.292365, tar: 0.032865 \n",
            "l0: 0.021082, l1: 0.019583, l2: 0.024802, l3: 0.028712, l4: 0.036942, l5: 0.043019, l6: 0.065117\n",
            "\n",
            "[epoch: 139/1000, batch:    11/   15, ite: 2081] train loss: 0.291709, tar: 0.032720 \n",
            "l0: 0.020502, l1: 0.020724, l2: 0.024071, l3: 0.029550, l4: 0.030711, l5: 0.033478, l6: 0.045871\n",
            "\n",
            "[epoch: 139/1000, batch:    12/   15, ite: 2082] train loss: 0.290650, tar: 0.032571 \n",
            "l0: 0.033920, l1: 0.033005, l2: 0.039578, l3: 0.039816, l4: 0.041084, l5: 0.052291, l6: 0.061910\n",
            "\n",
            "[epoch: 139/1000, batch:    13/   15, ite: 2083] train loss: 0.290782, tar: 0.032587 \n",
            "l0: 0.019325, l1: 0.018954, l2: 0.020876, l3: 0.022036, l4: 0.026406, l5: 0.034306, l6: 0.039783\n",
            "\n",
            "[epoch: 139/1000, batch:    14/   15, ite: 2084] train loss: 0.289484, tar: 0.032429 \n",
            "l0: 0.024113, l1: 0.026234, l2: 0.027926, l3: 0.031534, l4: 0.028512, l5: 0.030240, l6: 0.045011\n",
            "\n",
            "[epoch: 139/1000, batch:    15/   15, ite: 2085] train loss: 0.288591, tar: 0.032331 \n",
            "l0: 0.023329, l1: 0.021055, l2: 0.030179, l3: 0.032004, l4: 0.037017, l5: 0.038949, l6: 0.058710\n",
            "\n",
            "[epoch: 140/1000, batch:     1/   15, ite: 2086] train loss: 0.288040, tar: 0.032227 \n",
            "l0: 0.030291, l1: 0.033018, l2: 0.036002, l3: 0.029619, l4: 0.039349, l5: 0.045666, l6: 0.056424\n",
            "\n",
            "[epoch: 140/1000, batch:     2/   15, ite: 2087] train loss: 0.287837, tar: 0.032204 \n",
            "l0: 0.028070, l1: 0.031235, l2: 0.035756, l3: 0.034368, l4: 0.034176, l5: 0.040505, l6: 0.052868\n",
            "\n",
            "[epoch: 140/1000, batch:     3/   15, ite: 2088] train loss: 0.287486, tar: 0.032158 \n",
            "l0: 0.017940, l1: 0.017850, l2: 0.021029, l3: 0.026252, l4: 0.030248, l5: 0.031066, l6: 0.043370\n",
            "\n",
            "[epoch: 140/1000, batch:     4/   15, ite: 2089] train loss: 0.286366, tar: 0.031998 \n",
            "l0: 0.031414, l1: 0.033407, l2: 0.032165, l3: 0.034021, l4: 0.032188, l5: 0.046970, l6: 0.046902\n",
            "\n",
            "[epoch: 140/1000, batch:     5/   15, ite: 2090] train loss: 0.286040, tar: 0.031991 \n",
            "l0: 0.034071, l1: 0.030473, l2: 0.038947, l3: 0.040353, l4: 0.052491, l5: 0.056732, l6: 0.078044\n",
            "\n",
            "[epoch: 140/1000, batch:     6/   15, ite: 2091] train loss: 0.286535, tar: 0.032014 \n",
            "l0: 0.020873, l1: 0.022373, l2: 0.024195, l3: 0.027748, l4: 0.027010, l5: 0.031338, l6: 0.046991\n",
            "\n",
            "[epoch: 140/1000, batch:     7/   15, ite: 2092] train loss: 0.285601, tar: 0.031893 \n",
            "l0: 0.018701, l1: 0.018303, l2: 0.019294, l3: 0.022138, l4: 0.025340, l5: 0.030971, l6: 0.035671\n",
            "\n",
            "[epoch: 140/1000, batch:     8/   15, ite: 2093] train loss: 0.284362, tar: 0.031751 \n",
            "l0: 0.041897, l1: 0.042365, l2: 0.045699, l3: 0.046911, l4: 0.061416, l5: 0.071226, l6: 0.070563\n",
            "\n",
            "[epoch: 140/1000, batch:     9/   15, ite: 2094] train loss: 0.285380, tar: 0.031859 \n",
            "l0: 0.022719, l1: 0.022552, l2: 0.027106, l3: 0.030236, l4: 0.030580, l5: 0.037447, l6: 0.057802\n",
            "\n",
            "[epoch: 140/1000, batch:    10/   15, ite: 2095] train loss: 0.284781, tar: 0.031763 \n",
            "l0: 0.038559, l1: 0.035425, l2: 0.043005, l3: 0.043138, l4: 0.051604, l5: 0.048508, l6: 0.056476\n",
            "\n",
            "[epoch: 140/1000, batch:    11/   15, ite: 2096] train loss: 0.285114, tar: 0.031834 \n",
            "l0: 0.023038, l1: 0.020959, l2: 0.024078, l3: 0.027222, l4: 0.030106, l5: 0.034235, l6: 0.048616\n",
            "\n",
            "[epoch: 140/1000, batch:    12/   15, ite: 2097] train loss: 0.284321, tar: 0.031743 \n",
            "l0: 0.025574, l1: 0.022849, l2: 0.026151, l3: 0.031808, l4: 0.031050, l5: 0.033072, l6: 0.044854\n",
            "\n",
            "[epoch: 140/1000, batch:    13/   15, ite: 2098] train loss: 0.283618, tar: 0.031680 \n",
            "l0: 0.022121, l1: 0.020204, l2: 0.022411, l3: 0.024948, l4: 0.028192, l5: 0.034810, l6: 0.044802\n",
            "\n",
            "[epoch: 140/1000, batch:    14/   15, ite: 2099] train loss: 0.282748, tar: 0.031584 \n",
            "l0: 0.024049, l1: 0.021895, l2: 0.028396, l3: 0.030064, l4: 0.031106, l5: 0.040220, l6: 0.053292\n",
            "\n",
            "[epoch: 140/1000, batch:    15/   15, ite: 2100] train loss: 0.282210, tar: 0.031508 \n",
            "l0: 0.019394, l1: 0.017681, l2: 0.020124, l3: 0.022651, l4: 0.024038, l5: 0.030705, l6: 0.057063\n",
            "\n",
            "[epoch: 141/1000, batch:     1/   15, ite: 2101] train loss: 0.191656, tar: 0.019394 \n",
            "l0: 0.033563, l1: 0.036918, l2: 0.038941, l3: 0.042179, l4: 0.048798, l5: 0.045717, l6: 0.059735\n",
            "\n",
            "[epoch: 141/1000, batch:     2/   15, ite: 2102] train loss: 0.248754, tar: 0.026479 \n",
            "l0: 0.024768, l1: 0.024359, l2: 0.028379, l3: 0.030911, l4: 0.036095, l5: 0.038704, l6: 0.046905\n",
            "\n",
            "[epoch: 141/1000, batch:     3/   15, ite: 2103] train loss: 0.242542, tar: 0.025909 \n",
            "l0: 0.028470, l1: 0.027707, l2: 0.032578, l3: 0.034873, l4: 0.036383, l5: 0.045115, l6: 0.059705\n",
            "\n",
            "[epoch: 141/1000, batch:     4/   15, ite: 2104] train loss: 0.248115, tar: 0.026549 \n",
            "l0: 0.017628, l1: 0.017475, l2: 0.019422, l3: 0.023790, l4: 0.028022, l5: 0.032204, l6: 0.046907\n",
            "\n",
            "[epoch: 141/1000, batch:     5/   15, ite: 2105] train loss: 0.235581, tar: 0.024765 \n",
            "l0: 0.020834, l1: 0.020862, l2: 0.022920, l3: 0.025909, l4: 0.030396, l5: 0.033531, l6: 0.044270\n",
            "\n",
            "[epoch: 141/1000, batch:     6/   15, ite: 2106] train loss: 0.229438, tar: 0.024110 \n",
            "l0: 0.017888, l1: 0.017875, l2: 0.021011, l3: 0.024589, l4: 0.025994, l5: 0.032418, l6: 0.050629\n",
            "\n",
            "[epoch: 141/1000, batch:     7/   15, ite: 2107] train loss: 0.223862, tar: 0.023221 \n",
            "l0: 0.022665, l1: 0.024142, l2: 0.025867, l3: 0.026704, l4: 0.027061, l5: 0.027788, l6: 0.035264\n",
            "\n",
            "[epoch: 141/1000, batch:     8/   15, ite: 2108] train loss: 0.219565, tar: 0.023151 \n",
            "l0: 0.024124, l1: 0.024388, l2: 0.024988, l3: 0.027105, l4: 0.032423, l5: 0.037572, l6: 0.052956\n",
            "\n",
            "[epoch: 141/1000, batch:     9/   15, ite: 2109] train loss: 0.220009, tar: 0.023259 \n",
            "l0: 0.025572, l1: 0.028115, l2: 0.027904, l3: 0.033591, l4: 0.031354, l5: 0.029817, l6: 0.046657\n",
            "\n",
            "[epoch: 141/1000, batch:    10/   15, ite: 2110] train loss: 0.220309, tar: 0.023491 \n",
            "l0: 0.015861, l1: 0.014280, l2: 0.016285, l3: 0.017620, l4: 0.021559, l5: 0.028889, l6: 0.044719\n",
            "\n",
            "[epoch: 141/1000, batch:    11/   15, ite: 2111] train loss: 0.214755, tar: 0.022797 \n",
            "l0: 0.021749, l1: 0.023676, l2: 0.024612, l3: 0.026116, l4: 0.030280, l5: 0.031844, l6: 0.043833\n",
            "\n",
            "[epoch: 141/1000, batch:    12/   15, ite: 2112] train loss: 0.213701, tar: 0.022710 \n",
            "l0: 0.025694, l1: 0.024286, l2: 0.029464, l3: 0.032498, l4: 0.033228, l5: 0.039188, l6: 0.060319\n",
            "\n",
            "[epoch: 141/1000, batch:    13/   15, ite: 2113] train loss: 0.216084, tar: 0.022939 \n",
            "l0: 0.024322, l1: 0.024951, l2: 0.028295, l3: 0.028391, l4: 0.031904, l5: 0.033781, l6: 0.041115\n",
            "\n",
            "[epoch: 141/1000, batch:    14/   15, ite: 2114] train loss: 0.215846, tar: 0.023038 \n",
            "l0: 0.032371, l1: 0.034955, l2: 0.032068, l3: 0.032189, l4: 0.027354, l5: 0.030408, l6: 0.041144\n",
            "\n",
            "[epoch: 141/1000, batch:    15/   15, ite: 2115] train loss: 0.216822, tar: 0.023660 \n",
            "l0: 0.028182, l1: 0.028526, l2: 0.028281, l3: 0.033186, l4: 0.033511, l5: 0.047853, l6: 0.065940\n",
            "\n",
            "[epoch: 142/1000, batch:     1/   15, ite: 2116] train loss: 0.219863, tar: 0.023943 \n",
            "l0: 0.021638, l1: 0.022448, l2: 0.022611, l3: 0.023657, l4: 0.024571, l5: 0.029532, l6: 0.040546\n",
            "\n",
            "[epoch: 142/1000, batch:     2/   15, ite: 2117] train loss: 0.217813, tar: 0.023807 \n",
            "l0: 0.020161, l1: 0.019256, l2: 0.021734, l3: 0.026712, l4: 0.029689, l5: 0.033452, l6: 0.050893\n",
            "\n",
            "[epoch: 142/1000, batch:     3/   15, ite: 2118] train loss: 0.216929, tar: 0.023605 \n",
            "l0: 0.034639, l1: 0.033633, l2: 0.040471, l3: 0.045576, l4: 0.045257, l5: 0.044116, l6: 0.070418\n",
            "\n",
            "[epoch: 142/1000, batch:     4/   15, ite: 2119] train loss: 0.222043, tar: 0.024185 \n",
            "l0: 0.018853, l1: 0.018427, l2: 0.021335, l3: 0.021883, l4: 0.026823, l5: 0.030106, l6: 0.049115\n",
            "\n",
            "[epoch: 142/1000, batch:     5/   15, ite: 2120] train loss: 0.220268, tar: 0.023919 \n",
            "l0: 0.014495, l1: 0.014541, l2: 0.015933, l3: 0.016934, l4: 0.020722, l5: 0.025029, l6: 0.040865\n",
            "\n",
            "[epoch: 142/1000, batch:     6/   15, ite: 2121] train loss: 0.216852, tar: 0.023470 \n",
            "l0: 0.026682, l1: 0.027947, l2: 0.030174, l3: 0.032277, l4: 0.030292, l5: 0.033024, l6: 0.051644\n",
            "\n",
            "[epoch: 142/1000, batch:     7/   15, ite: 2122] train loss: 0.217542, tar: 0.023616 \n",
            "l0: 0.021721, l1: 0.022858, l2: 0.024571, l3: 0.025795, l4: 0.026089, l5: 0.032863, l6: 0.048417\n",
            "\n",
            "[epoch: 142/1000, batch:     8/   15, ite: 2123] train loss: 0.216880, tar: 0.023534 \n",
            "l0: 0.014050, l1: 0.012863, l2: 0.016235, l3: 0.020447, l4: 0.022592, l5: 0.025247, l6: 0.049796\n",
            "\n",
            "[epoch: 142/1000, batch:     9/   15, ite: 2124] train loss: 0.214561, tar: 0.023139 \n",
            "l0: 0.018191, l1: 0.019245, l2: 0.020331, l3: 0.023615, l4: 0.026269, l5: 0.030018, l6: 0.041432\n",
            "\n",
            "[epoch: 142/1000, batch:    10/   15, ite: 2125] train loss: 0.213143, tar: 0.022941 \n",
            "l0: 0.021716, l1: 0.021302, l2: 0.023274, l3: 0.025342, l4: 0.026869, l5: 0.031347, l6: 0.045797\n",
            "\n",
            "[epoch: 142/1000, batch:    11/   15, ite: 2126] train loss: 0.212470, tar: 0.022894 \n",
            "l0: 0.020677, l1: 0.018879, l2: 0.025065, l3: 0.028572, l4: 0.026706, l5: 0.029750, l6: 0.036162\n",
            "\n",
            "[epoch: 142/1000, batch:    12/   15, ite: 2127] train loss: 0.211483, tar: 0.022811 \n",
            "l0: 0.028066, l1: 0.032397, l2: 0.034427, l3: 0.035200, l4: 0.036166, l5: 0.032040, l6: 0.043150\n",
            "\n",
            "[epoch: 142/1000, batch:    13/   15, ite: 2128] train loss: 0.212553, tar: 0.022999 \n",
            "l0: 0.026592, l1: 0.023759, l2: 0.027797, l3: 0.035883, l4: 0.036485, l5: 0.041819, l6: 0.059258\n",
            "\n",
            "[epoch: 142/1000, batch:    14/   15, ite: 2129] train loss: 0.213899, tar: 0.023123 \n",
            "l0: 0.022730, l1: 0.023629, l2: 0.022473, l3: 0.023690, l4: 0.024663, l5: 0.030251, l6: 0.043370\n",
            "\n",
            "[epoch: 142/1000, batch:    15/   15, ite: 2130] train loss: 0.213129, tar: 0.023110 \n",
            "l0: 0.023981, l1: 0.021368, l2: 0.026238, l3: 0.029907, l4: 0.029419, l5: 0.033290, l6: 0.049472\n",
            "\n",
            "[epoch: 143/1000, batch:     1/   15, ite: 2131] train loss: 0.213147, tar: 0.023138 \n",
            "l0: 0.026800, l1: 0.030188, l2: 0.030010, l3: 0.029589, l4: 0.030309, l5: 0.025726, l6: 0.034840\n",
            "\n",
            "[epoch: 143/1000, batch:     2/   15, ite: 2132] train loss: 0.212969, tar: 0.023252 \n",
            "l0: 0.022348, l1: 0.020795, l2: 0.022130, l3: 0.024793, l4: 0.025863, l5: 0.035876, l6: 0.050829\n",
            "\n",
            "[epoch: 143/1000, batch:     3/   15, ite: 2133] train loss: 0.212656, tar: 0.023225 \n",
            "l0: 0.028412, l1: 0.028273, l2: 0.031034, l3: 0.034176, l4: 0.036395, l5: 0.037273, l6: 0.056580\n",
            "\n",
            "[epoch: 143/1000, batch:     4/   15, ite: 2134] train loss: 0.213817, tar: 0.023378 \n",
            "l0: 0.016155, l1: 0.016093, l2: 0.017687, l3: 0.019684, l4: 0.021882, l5: 0.024543, l6: 0.039281\n",
            "\n",
            "[epoch: 143/1000, batch:     5/   15, ite: 2135] train loss: 0.212146, tar: 0.023171 \n",
            "l0: 0.024605, l1: 0.023776, l2: 0.026774, l3: 0.029994, l4: 0.033881, l5: 0.037595, l6: 0.062770\n",
            "\n",
            "[epoch: 143/1000, batch:     6/   15, ite: 2136] train loss: 0.212903, tar: 0.023211 \n",
            "l0: 0.028427, l1: 0.033380, l2: 0.034477, l3: 0.032301, l4: 0.033221, l5: 0.042612, l6: 0.053558\n",
            "\n",
            "[epoch: 143/1000, batch:     7/   15, ite: 2137] train loss: 0.214121, tar: 0.023352 \n",
            "l0: 0.015500, l1: 0.014406, l2: 0.015522, l3: 0.019004, l4: 0.020298, l5: 0.026097, l6: 0.040023\n",
            "\n",
            "[epoch: 143/1000, batch:     8/   15, ite: 2138] train loss: 0.212456, tar: 0.023145 \n",
            "l0: 0.016103, l1: 0.016595, l2: 0.017366, l3: 0.019830, l4: 0.021779, l5: 0.028210, l6: 0.047193\n",
            "\n",
            "[epoch: 143/1000, batch:     9/   15, ite: 2139] train loss: 0.211293, tar: 0.022965 \n",
            "l0: 0.021975, l1: 0.021719, l2: 0.024558, l3: 0.027118, l4: 0.027301, l5: 0.030257, l6: 0.045553\n",
            "\n",
            "[epoch: 143/1000, batch:    10/   15, ite: 2140] train loss: 0.210973, tar: 0.022940 \n",
            "l0: 0.019658, l1: 0.019271, l2: 0.022338, l3: 0.023708, l4: 0.027058, l5: 0.032483, l6: 0.046266\n",
            "\n",
            "[epoch: 143/1000, batch:    11/   15, ite: 2141] train loss: 0.210480, tar: 0.022860 \n",
            "l0: 0.023600, l1: 0.022164, l2: 0.026601, l3: 0.029253, l4: 0.032845, l5: 0.040516, l6: 0.051258\n",
            "\n",
            "[epoch: 143/1000, batch:    12/   15, ite: 2142] train loss: 0.210855, tar: 0.022878 \n",
            "l0: 0.025659, l1: 0.024327, l2: 0.030809, l3: 0.032763, l4: 0.041399, l5: 0.047319, l6: 0.070016\n",
            "\n",
            "[epoch: 143/1000, batch:    13/   15, ite: 2143] train loss: 0.212284, tar: 0.022942 \n",
            "l0: 0.017261, l1: 0.017439, l2: 0.019023, l3: 0.021033, l4: 0.024983, l5: 0.030249, l6: 0.052083\n",
            "\n",
            "[epoch: 143/1000, batch:    14/   15, ite: 2144] train loss: 0.211597, tar: 0.022813 \n",
            "l0: 0.022174, l1: 0.021757, l2: 0.028037, l3: 0.027444, l4: 0.028393, l5: 0.032401, l6: 0.046144\n",
            "\n",
            "[epoch: 143/1000, batch:    15/   15, ite: 2145] train loss: 0.211481, tar: 0.022799 \n",
            "l0: 0.018003, l1: 0.018112, l2: 0.019376, l3: 0.022004, l4: 0.021802, l5: 0.027711, l6: 0.039565\n",
            "\n",
            "[epoch: 144/1000, batch:     1/   15, ite: 2146] train loss: 0.210504, tar: 0.022695 \n",
            "l0: 0.016753, l1: 0.016177, l2: 0.018866, l3: 0.021254, l4: 0.022520, l5: 0.026827, l6: 0.048759\n",
            "\n",
            "[epoch: 144/1000, batch:     2/   15, ite: 2147] train loss: 0.209667, tar: 0.022568 \n",
            "l0: 0.019054, l1: 0.018681, l2: 0.020746, l3: 0.024758, l4: 0.033655, l5: 0.034983, l6: 0.046728\n",
            "\n",
            "[epoch: 144/1000, batch:     3/   15, ite: 2148] train loss: 0.209437, tar: 0.022495 \n",
            "l0: 0.023397, l1: 0.026402, l2: 0.026565, l3: 0.027631, l4: 0.027047, l5: 0.023437, l6: 0.030941\n",
            "\n",
            "[epoch: 144/1000, batch:     4/   15, ite: 2149] train loss: 0.208947, tar: 0.022514 \n",
            "l0: 0.020790, l1: 0.019653, l2: 0.022766, l3: 0.027350, l4: 0.029519, l5: 0.033243, l6: 0.052387\n",
            "\n",
            "[epoch: 144/1000, batch:     5/   15, ite: 2150] train loss: 0.208882, tar: 0.022479 \n",
            "l0: 0.019081, l1: 0.018450, l2: 0.021707, l3: 0.023978, l4: 0.026369, l5: 0.029835, l6: 0.041167\n",
            "\n",
            "[epoch: 144/1000, batch:     6/   15, ite: 2151] train loss: 0.208327, tar: 0.022412 \n",
            "l0: 0.019027, l1: 0.019580, l2: 0.020562, l3: 0.026197, l4: 0.026417, l5: 0.028181, l6: 0.040170\n",
            "\n",
            "[epoch: 144/1000, batch:     7/   15, ite: 2152] train loss: 0.207785, tar: 0.022347 \n",
            "l0: 0.019169, l1: 0.018259, l2: 0.022882, l3: 0.024974, l4: 0.027536, l5: 0.029876, l6: 0.042100\n",
            "\n",
            "[epoch: 144/1000, batch:     8/   15, ite: 2153] train loss: 0.207351, tar: 0.022287 \n",
            "l0: 0.026942, l1: 0.032273, l2: 0.024975, l3: 0.030948, l4: 0.028587, l5: 0.031267, l6: 0.051600\n",
            "\n",
            "[epoch: 144/1000, batch:     9/   15, ite: 2154] train loss: 0.207707, tar: 0.022374 \n",
            "l0: 0.016172, l1: 0.016254, l2: 0.016938, l3: 0.018283, l4: 0.021600, l5: 0.025204, l6: 0.039190\n",
            "\n",
            "[epoch: 144/1000, batch:    10/   15, ite: 2155] train loss: 0.206724, tar: 0.022261 \n",
            "l0: 0.018813, l1: 0.017917, l2: 0.019932, l3: 0.020847, l4: 0.024483, l5: 0.032283, l6: 0.054133\n",
            "\n",
            "[epoch: 144/1000, batch:    11/   15, ite: 2156] train loss: 0.206397, tar: 0.022199 \n",
            "l0: 0.013188, l1: 0.013196, l2: 0.015212, l3: 0.016170, l4: 0.018461, l5: 0.021365, l6: 0.035299\n",
            "\n",
            "[epoch: 144/1000, batch:    12/   15, ite: 2157] train loss: 0.205108, tar: 0.022041 \n",
            "l0: 0.019004, l1: 0.019387, l2: 0.021371, l3: 0.023806, l4: 0.026135, l5: 0.029006, l6: 0.034632\n",
            "\n",
            "[epoch: 144/1000, batch:    13/   15, ite: 2158] train loss: 0.204560, tar: 0.021989 \n",
            "l0: 0.029204, l1: 0.029970, l2: 0.034605, l3: 0.038767, l4: 0.040944, l5: 0.039215, l6: 0.049341\n",
            "\n",
            "[epoch: 144/1000, batch:    14/   15, ite: 2159] train loss: 0.205534, tar: 0.022111 \n",
            "l0: 0.024012, l1: 0.023746, l2: 0.024749, l3: 0.029597, l4: 0.029896, l5: 0.036996, l6: 0.047951\n",
            "\n",
            "[epoch: 144/1000, batch:    15/   15, ite: 2160] train loss: 0.205725, tar: 0.022143 \n",
            "l0: 0.017045, l1: 0.017823, l2: 0.021460, l3: 0.023659, l4: 0.024556, l5: 0.029583, l6: 0.037973\n",
            "\n",
            "[epoch: 145/1000, batch:     1/   15, ite: 2161] train loss: 0.205173, tar: 0.022059 \n",
            "l0: 0.026227, l1: 0.025301, l2: 0.030624, l3: 0.031382, l4: 0.028448, l5: 0.030773, l6: 0.051595\n",
            "\n",
            "[epoch: 145/1000, batch:     2/   15, ite: 2162] train loss: 0.205483, tar: 0.022126 \n",
            "l0: 0.020516, l1: 0.017903, l2: 0.022597, l3: 0.025270, l4: 0.025340, l5: 0.030518, l6: 0.044038\n",
            "\n",
            "[epoch: 145/1000, batch:     3/   15, ite: 2163] train loss: 0.205176, tar: 0.022101 \n",
            "l0: 0.018847, l1: 0.018270, l2: 0.020508, l3: 0.023600, l4: 0.026191, l5: 0.031446, l6: 0.034491\n",
            "\n",
            "[epoch: 145/1000, batch:     4/   15, ite: 2164] train loss: 0.204679, tar: 0.022050 \n",
            "l0: 0.019473, l1: 0.020887, l2: 0.021220, l3: 0.025818, l4: 0.027276, l5: 0.030217, l6: 0.041893\n",
            "\n",
            "[epoch: 145/1000, batch:     5/   15, ite: 2165] train loss: 0.204404, tar: 0.022010 \n",
            "l0: 0.025452, l1: 0.024389, l2: 0.027012, l3: 0.027009, l4: 0.030616, l5: 0.033559, l6: 0.055967\n",
            "\n",
            "[epoch: 145/1000, batch:     6/   15, ite: 2166] train loss: 0.204701, tar: 0.022062 \n",
            "l0: 0.015004, l1: 0.014092, l2: 0.016221, l3: 0.017907, l4: 0.020214, l5: 0.026307, l6: 0.037993\n",
            "\n",
            "[epoch: 145/1000, batch:     7/   15, ite: 2167] train loss: 0.203851, tar: 0.021957 \n",
            "l0: 0.014461, l1: 0.014377, l2: 0.016198, l3: 0.017003, l4: 0.019139, l5: 0.021651, l6: 0.031830\n",
            "\n",
            "[epoch: 145/1000, batch:     8/   15, ite: 2168] train loss: 0.202833, tar: 0.021847 \n",
            "l0: 0.017247, l1: 0.017442, l2: 0.018924, l3: 0.019733, l4: 0.023087, l5: 0.027382, l6: 0.047291\n",
            "\n",
            "[epoch: 145/1000, batch:     9/   15, ite: 2169] train loss: 0.202373, tar: 0.021780 \n",
            "l0: 0.034388, l1: 0.033386, l2: 0.035392, l3: 0.037569, l4: 0.036936, l5: 0.048168, l6: 0.066275\n",
            "\n",
            "[epoch: 145/1000, batch:    10/   15, ite: 2170] train loss: 0.203655, tar: 0.021960 \n",
            "l0: 0.025267, l1: 0.028629, l2: 0.028637, l3: 0.029614, l4: 0.028184, l5: 0.034227, l6: 0.036050\n",
            "\n",
            "[epoch: 145/1000, batch:    11/   15, ite: 2171] train loss: 0.203753, tar: 0.022007 \n",
            "l0: 0.024175, l1: 0.022528, l2: 0.026347, l3: 0.034528, l4: 0.035741, l5: 0.039263, l6: 0.062485\n",
            "\n",
            "[epoch: 145/1000, batch:    12/   15, ite: 2172] train loss: 0.204327, tar: 0.022037 \n",
            "l0: 0.018652, l1: 0.015975, l2: 0.020751, l3: 0.028155, l4: 0.036274, l5: 0.040453, l6: 0.059747\n",
            "\n",
            "[epoch: 145/1000, batch:    13/   15, ite: 2173] train loss: 0.204542, tar: 0.021991 \n",
            "l0: 0.022984, l1: 0.023125, l2: 0.024027, l3: 0.028867, l4: 0.032374, l5: 0.035638, l6: 0.053468\n",
            "\n",
            "[epoch: 145/1000, batch:    14/   15, ite: 2174] train loss: 0.204757, tar: 0.022004 \n",
            "l0: 0.026214, l1: 0.026304, l2: 0.029616, l3: 0.031545, l4: 0.030443, l5: 0.031885, l6: 0.047754\n",
            "\n",
            "[epoch: 145/1000, batch:    15/   15, ite: 2175] train loss: 0.205011, tar: 0.022060 \n",
            "l0: 0.019752, l1: 0.019364, l2: 0.022105, l3: 0.023653, l4: 0.027432, l5: 0.034052, l6: 0.055748\n",
            "\n",
            "[epoch: 146/1000, batch:     1/   15, ite: 2176] train loss: 0.204972, tar: 0.022030 \n",
            "l0: 0.014627, l1: 0.014460, l2: 0.017522, l3: 0.020958, l4: 0.022601, l5: 0.027358, l6: 0.048582\n",
            "\n",
            "[epoch: 146/1000, batch:     2/   15, ite: 2177] train loss: 0.204468, tar: 0.021934 \n",
            "l0: 0.017160, l1: 0.017620, l2: 0.020449, l3: 0.021286, l4: 0.021534, l5: 0.025792, l6: 0.037729\n",
            "\n",
            "[epoch: 146/1000, batch:     3/   15, ite: 2178] train loss: 0.203918, tar: 0.021873 \n",
            "l0: 0.022885, l1: 0.022435, l2: 0.024640, l3: 0.027193, l4: 0.028170, l5: 0.033789, l6: 0.056303\n",
            "\n",
            "[epoch: 146/1000, batch:     4/   15, ite: 2179] train loss: 0.204063, tar: 0.021885 \n",
            "l0: 0.025794, l1: 0.022727, l2: 0.027828, l3: 0.030320, l4: 0.037849, l5: 0.045268, l6: 0.049066\n",
            "\n",
            "[epoch: 146/1000, batch:     5/   15, ite: 2180] train loss: 0.204498, tar: 0.021934 \n",
            "l0: 0.020924, l1: 0.020074, l2: 0.023464, l3: 0.026424, l4: 0.028332, l5: 0.032307, l6: 0.049161\n",
            "\n",
            "[epoch: 146/1000, batch:     6/   15, ite: 2181] train loss: 0.204451, tar: 0.021922 \n",
            "l0: 0.022361, l1: 0.019836, l2: 0.025872, l3: 0.023667, l4: 0.031439, l5: 0.042431, l6: 0.069796\n",
            "\n",
            "[epoch: 146/1000, batch:     7/   15, ite: 2182] train loss: 0.204829, tar: 0.021927 \n",
            "l0: 0.027343, l1: 0.028625, l2: 0.027929, l3: 0.028040, l4: 0.033542, l5: 0.037676, l6: 0.045040\n",
            "\n",
            "[epoch: 146/1000, batch:     8/   15, ite: 2183] train loss: 0.205110, tar: 0.021992 \n",
            "l0: 0.020528, l1: 0.020118, l2: 0.024042, l3: 0.026585, l4: 0.030045, l5: 0.032808, l6: 0.050230\n",
            "\n",
            "[epoch: 146/1000, batch:     9/   15, ite: 2184] train loss: 0.205101, tar: 0.021975 \n",
            "l0: 0.027737, l1: 0.024999, l2: 0.030632, l3: 0.037824, l4: 0.045975, l5: 0.052973, l6: 0.080472\n",
            "\n",
            "[epoch: 146/1000, batch:    10/   15, ite: 2185] train loss: 0.206225, tar: 0.022043 \n",
            "l0: 0.021706, l1: 0.022275, l2: 0.025443, l3: 0.023262, l4: 0.028297, l5: 0.036757, l6: 0.056233\n",
            "\n",
            "[epoch: 146/1000, batch:    11/   15, ite: 2186] train loss: 0.206315, tar: 0.022039 \n",
            "l0: 0.016087, l1: 0.015433, l2: 0.017930, l3: 0.020076, l4: 0.020608, l5: 0.027234, l6: 0.044026\n",
            "\n",
            "[epoch: 146/1000, batch:    12/   15, ite: 2187] train loss: 0.205798, tar: 0.021970 \n",
            "l0: 0.015589, l1: 0.016188, l2: 0.018870, l3: 0.020430, l4: 0.023048, l5: 0.025469, l6: 0.043416\n",
            "\n",
            "[epoch: 146/1000, batch:    13/   15, ite: 2188] train loss: 0.205312, tar: 0.021898 \n",
            "l0: 0.016246, l1: 0.015955, l2: 0.017255, l3: 0.019401, l4: 0.019670, l5: 0.024355, l6: 0.035960\n",
            "\n",
            "[epoch: 146/1000, batch:    14/   15, ite: 2189] train loss: 0.204678, tar: 0.021834 \n",
            "l0: 0.016883, l1: 0.015600, l2: 0.016858, l3: 0.020284, l4: 0.021338, l5: 0.025526, l6: 0.040988\n",
            "\n",
            "[epoch: 146/1000, batch:    15/   15, ite: 2190] train loss: 0.204153, tar: 0.021779 \n",
            "l0: 0.017435, l1: 0.016308, l2: 0.020089, l3: 0.022065, l4: 0.024050, l5: 0.025934, l6: 0.033970\n",
            "\n",
            "[epoch: 147/1000, batch:     1/   15, ite: 2191] train loss: 0.203666, tar: 0.021732 \n",
            "l0: 0.017876, l1: 0.016421, l2: 0.019637, l3: 0.023107, l4: 0.022339, l5: 0.027874, l6: 0.047231\n",
            "\n",
            "[epoch: 147/1000, batch:     2/   15, ite: 2192] train loss: 0.203349, tar: 0.021690 \n",
            "l0: 0.017611, l1: 0.016775, l2: 0.018413, l3: 0.020461, l4: 0.023881, l5: 0.027742, l6: 0.039957\n",
            "\n",
            "[epoch: 147/1000, batch:     3/   15, ite: 2193] train loss: 0.202935, tar: 0.021646 \n",
            "l0: 0.026800, l1: 0.028071, l2: 0.032586, l3: 0.033828, l4: 0.037311, l5: 0.037896, l6: 0.062846\n",
            "\n",
            "[epoch: 147/1000, batch:     4/   15, ite: 2194] train loss: 0.203535, tar: 0.021701 \n",
            "l0: 0.024175, l1: 0.023300, l2: 0.026624, l3: 0.028188, l4: 0.031262, l5: 0.036423, l6: 0.047189\n",
            "\n",
            "[epoch: 147/1000, batch:     5/   15, ite: 2195] train loss: 0.203679, tar: 0.021727 \n",
            "l0: 0.033017, l1: 0.032087, l2: 0.037816, l3: 0.040101, l4: 0.046948, l5: 0.047957, l6: 0.057308\n",
            "\n",
            "[epoch: 147/1000, batch:     6/   15, ite: 2196] train loss: 0.204632, tar: 0.021844 \n",
            "l0: 0.025710, l1: 0.034375, l2: 0.025428, l3: 0.023127, l4: 0.023847, l5: 0.029533, l6: 0.049606\n",
            "\n",
            "[epoch: 147/1000, batch:     7/   15, ite: 2197] train loss: 0.204704, tar: 0.021884 \n",
            "l0: 0.020525, l1: 0.021240, l2: 0.025602, l3: 0.027386, l4: 0.033161, l5: 0.029782, l6: 0.043863\n",
            "\n",
            "[epoch: 147/1000, batch:     8/   15, ite: 2198] train loss: 0.204672, tar: 0.021870 \n",
            "l0: 0.019352, l1: 0.021569, l2: 0.021896, l3: 0.022496, l4: 0.027359, l5: 0.033196, l6: 0.036907\n",
            "\n",
            "[epoch: 147/1000, batch:     9/   15, ite: 2199] train loss: 0.204451, tar: 0.021845 \n",
            "l0: 0.019517, l1: 0.019903, l2: 0.024168, l3: 0.025460, l4: 0.026447, l5: 0.027847, l6: 0.046030\n",
            "\n",
            "[epoch: 147/1000, batch:    10/   15, ite: 2200] train loss: 0.204300, tar: 0.021822 \n",
            "l0: 0.028678, l1: 0.025744, l2: 0.030385, l3: 0.033308, l4: 0.036880, l5: 0.045788, l6: 0.081370\n",
            "\n",
            "[epoch: 147/1000, batch:    11/   15, ite: 2201] train loss: 0.282153, tar: 0.028678 \n",
            "l0: 0.047301, l1: 0.047276, l2: 0.050570, l3: 0.048612, l4: 0.046366, l5: 0.047800, l6: 0.078163\n",
            "\n",
            "[epoch: 147/1000, batch:    12/   15, ite: 2202] train loss: 0.324121, tar: 0.037989 \n",
            "l0: 0.024073, l1: 0.022388, l2: 0.027738, l3: 0.030836, l4: 0.034986, l5: 0.041340, l6: 0.064308\n",
            "\n",
            "[epoch: 147/1000, batch:    13/   15, ite: 2203] train loss: 0.297971, tar: 0.033351 \n",
            "l0: 0.030317, l1: 0.032613, l2: 0.033500, l3: 0.035481, l4: 0.039255, l5: 0.036536, l6: 0.039690\n",
            "\n",
            "[epoch: 147/1000, batch:    14/   15, ite: 2204] train loss: 0.285326, tar: 0.032592 \n",
            "l0: 0.014563, l1: 0.014159, l2: 0.015020, l3: 0.016447, l4: 0.020211, l5: 0.028214, l6: 0.038252\n",
            "\n",
            "[epoch: 147/1000, batch:    15/   15, ite: 2205] train loss: 0.257634, tar: 0.028986 \n",
            "l0: 0.023904, l1: 0.024649, l2: 0.025584, l3: 0.024939, l4: 0.026377, l5: 0.032507, l6: 0.046925\n",
            "\n",
            "[epoch: 148/1000, batch:     1/   15, ite: 2206] train loss: 0.248843, tar: 0.028139 \n",
            "l0: 0.020405, l1: 0.022886, l2: 0.019985, l3: 0.023585, l4: 0.023670, l5: 0.030189, l6: 0.043560\n",
            "\n",
            "[epoch: 148/1000, batch:     2/   15, ite: 2207] train loss: 0.239619, tar: 0.027035 \n",
            "l0: 0.015509, l1: 0.016475, l2: 0.017264, l3: 0.018458, l4: 0.021578, l5: 0.027690, l6: 0.040528\n",
            "\n",
            "[epoch: 148/1000, batch:     3/   15, ite: 2208] train loss: 0.229355, tar: 0.025594 \n",
            "l0: 0.034867, l1: 0.033230, l2: 0.040331, l3: 0.045827, l4: 0.044489, l5: 0.043504, l6: 0.067301\n",
            "\n",
            "[epoch: 148/1000, batch:     4/   15, ite: 2209] train loss: 0.238265, tar: 0.026624 \n",
            "l0: 0.017475, l1: 0.017513, l2: 0.021080, l3: 0.026330, l4: 0.026886, l5: 0.034522, l6: 0.046384\n",
            "\n",
            "[epoch: 148/1000, batch:     5/   15, ite: 2210] train loss: 0.233458, tar: 0.025709 \n",
            "l0: 0.024481, l1: 0.023625, l2: 0.026385, l3: 0.027371, l4: 0.033281, l5: 0.043110, l6: 0.065194\n",
            "\n",
            "[epoch: 148/1000, batch:     6/   15, ite: 2211] train loss: 0.234366, tar: 0.025598 \n",
            "l0: 0.013257, l1: 0.012980, l2: 0.015677, l3: 0.019764, l4: 0.018974, l5: 0.023361, l6: 0.047353\n",
            "\n",
            "[epoch: 148/1000, batch:     7/   15, ite: 2212] train loss: 0.227449, tar: 0.024569 \n",
            "l0: 0.016502, l1: 0.015765, l2: 0.016660, l3: 0.019869, l4: 0.022950, l5: 0.030166, l6: 0.042652\n",
            "\n",
            "[epoch: 148/1000, batch:     8/   15, ite: 2213] train loss: 0.222612, tar: 0.023949 \n",
            "l0: 0.031333, l1: 0.029314, l2: 0.037490, l3: 0.036859, l4: 0.040938, l5: 0.046902, l6: 0.056701\n",
            "\n",
            "[epoch: 148/1000, batch:     9/   15, ite: 2214] train loss: 0.226678, tar: 0.024476 \n",
            "l0: 0.022332, l1: 0.025357, l2: 0.026035, l3: 0.029448, l4: 0.035096, l5: 0.040839, l6: 0.062052\n",
            "\n",
            "[epoch: 148/1000, batch:    10/   15, ite: 2215] train loss: 0.227643, tar: 0.024333 \n",
            "l0: 0.020660, l1: 0.019284, l2: 0.025243, l3: 0.026955, l4: 0.027380, l5: 0.031370, l6: 0.047565\n",
            "\n",
            "[epoch: 148/1000, batch:    11/   15, ite: 2216] train loss: 0.225819, tar: 0.024104 \n",
            "l0: 0.016726, l1: 0.020060, l2: 0.017344, l3: 0.021296, l4: 0.024063, l5: 0.026162, l6: 0.035707\n",
            "\n",
            "[epoch: 148/1000, batch:    12/   15, ite: 2217] train loss: 0.222027, tar: 0.023670 \n",
            "l0: 0.040246, l1: 0.042301, l2: 0.043786, l3: 0.040787, l4: 0.048129, l5: 0.042438, l6: 0.056927\n",
            "\n",
            "[epoch: 148/1000, batch:    13/   15, ite: 2218] train loss: 0.227171, tar: 0.024591 \n",
            "l0: 0.020874, l1: 0.019978, l2: 0.020564, l3: 0.023384, l4: 0.032671, l5: 0.034744, l6: 0.056341\n",
            "\n",
            "[epoch: 148/1000, batch:    14/   15, ite: 2219] train loss: 0.226191, tar: 0.024395 \n",
            "l0: 0.033358, l1: 0.033122, l2: 0.036652, l3: 0.038010, l4: 0.039155, l5: 0.040918, l6: 0.059822\n",
            "\n",
            "[epoch: 148/1000, batch:    15/   15, ite: 2220] train loss: 0.228934, tar: 0.024843 \n",
            "l0: 0.026462, l1: 0.027685, l2: 0.024356, l3: 0.027673, l4: 0.033980, l5: 0.046444, l6: 0.045378\n",
            "\n",
            "[epoch: 149/1000, batch:     1/   15, ite: 2221] train loss: 0.229079, tar: 0.024920 \n",
            "l0: 0.022116, l1: 0.021288, l2: 0.023496, l3: 0.024784, l4: 0.028297, l5: 0.039042, l6: 0.049857\n",
            "\n",
            "[epoch: 149/1000, batch:     2/   15, ite: 2222] train loss: 0.228160, tar: 0.024793 \n",
            "l0: 0.026466, l1: 0.024085, l2: 0.028809, l3: 0.029319, l4: 0.032858, l5: 0.040554, l6: 0.047083\n",
            "\n",
            "[epoch: 149/1000, batch:     3/   15, ite: 2223] train loss: 0.228204, tar: 0.024865 \n",
            "l0: 0.027856, l1: 0.028717, l2: 0.028191, l3: 0.030673, l4: 0.039406, l5: 0.040035, l6: 0.043132\n",
            "\n",
            "[epoch: 149/1000, batch:     4/   15, ite: 2224] train loss: 0.228613, tar: 0.024990 \n",
            "l0: 0.028893, l1: 0.029337, l2: 0.032176, l3: 0.034118, l4: 0.035700, l5: 0.037297, l6: 0.048149\n",
            "\n",
            "[epoch: 149/1000, batch:     5/   15, ite: 2225] train loss: 0.229295, tar: 0.025146 \n",
            "l0: 0.021716, l1: 0.019448, l2: 0.027156, l3: 0.030405, l4: 0.032575, l5: 0.031106, l6: 0.049397\n",
            "\n",
            "[epoch: 149/1000, batch:     6/   15, ite: 2226] train loss: 0.228623, tar: 0.025014 \n",
            "l0: 0.022080, l1: 0.023195, l2: 0.023539, l3: 0.024974, l4: 0.027930, l5: 0.027761, l6: 0.037946\n",
            "\n",
            "[epoch: 149/1000, batch:     7/   15, ite: 2227] train loss: 0.227097, tar: 0.024906 \n",
            "l0: 0.022479, l1: 0.025925, l2: 0.024626, l3: 0.028232, l4: 0.028024, l5: 0.031607, l6: 0.049808\n",
            "\n",
            "[epoch: 149/1000, batch:     8/   15, ite: 2228] train loss: 0.226511, tar: 0.024819 \n",
            "l0: 0.018524, l1: 0.018576, l2: 0.023598, l3: 0.024050, l4: 0.024039, l5: 0.029888, l6: 0.044960\n",
            "\n",
            "[epoch: 149/1000, batch:     9/   15, ite: 2229] train loss: 0.225033, tar: 0.024602 \n",
            "l0: 0.052068, l1: 0.049648, l2: 0.055537, l3: 0.065420, l4: 0.067024, l5: 0.075898, l6: 0.075957\n",
            "\n",
            "[epoch: 149/1000, batch:    10/   15, ite: 2230] train loss: 0.232250, tar: 0.025517 \n",
            "l0: 0.034564, l1: 0.036184, l2: 0.035654, l3: 0.040585, l4: 0.044020, l5: 0.051069, l6: 0.075335\n",
            "\n",
            "[epoch: 149/1000, batch:    11/   15, ite: 2231] train loss: 0.234997, tar: 0.025809 \n",
            "l0: 0.019763, l1: 0.019625, l2: 0.021849, l3: 0.025681, l4: 0.033875, l5: 0.042384, l6: 0.054927\n",
            "\n",
            "[epoch: 149/1000, batch:    12/   15, ite: 2232] train loss: 0.234469, tar: 0.025620 \n",
            "l0: 0.018151, l1: 0.018918, l2: 0.020738, l3: 0.019118, l4: 0.025805, l5: 0.038315, l6: 0.053861\n",
            "\n",
            "[epoch: 149/1000, batch:    13/   15, ite: 2233] train loss: 0.233270, tar: 0.025394 \n",
            "l0: 0.019349, l1: 0.018637, l2: 0.021734, l3: 0.022610, l4: 0.024770, l5: 0.033740, l6: 0.046407\n",
            "\n",
            "[epoch: 149/1000, batch:    14/   15, ite: 2234] train loss: 0.231917, tar: 0.025216 \n",
            "l0: 0.027792, l1: 0.030467, l2: 0.031497, l3: 0.029324, l4: 0.033985, l5: 0.042982, l6: 0.070673\n",
            "\n",
            "[epoch: 149/1000, batch:    15/   15, ite: 2235] train loss: 0.232911, tar: 0.025290 \n",
            "l0: 0.032227, l1: 0.034056, l2: 0.035417, l3: 0.038739, l4: 0.033252, l5: 0.036725, l6: 0.073771\n",
            "\n",
            "[epoch: 150/1000, batch:     1/   15, ite: 2236] train loss: 0.234335, tar: 0.025482 \n",
            "l0: 0.027155, l1: 0.027665, l2: 0.032304, l3: 0.031740, l4: 0.033408, l5: 0.034079, l6: 0.043657\n",
            "\n",
            "[epoch: 150/1000, batch:     2/   15, ite: 2237] train loss: 0.234219, tar: 0.025528 \n",
            "l0: 0.016046, l1: 0.014874, l2: 0.016823, l3: 0.019401, l4: 0.024493, l5: 0.030373, l6: 0.040221\n",
            "\n",
            "[epoch: 150/1000, batch:     3/   15, ite: 2238] train loss: 0.232324, tar: 0.025278 \n",
            "l0: 0.029098, l1: 0.035440, l2: 0.031763, l3: 0.029476, l4: 0.031515, l5: 0.025514, l6: 0.034205\n",
            "\n",
            "[epoch: 150/1000, batch:     4/   15, ite: 2239] train loss: 0.231931, tar: 0.025376 \n",
            "l0: 0.041135, l1: 0.041285, l2: 0.047971, l3: 0.045750, l4: 0.047047, l5: 0.049434, l6: 0.055619\n",
            "\n",
            "[epoch: 150/1000, batch:     5/   15, ite: 2240] train loss: 0.234339, tar: 0.025770 \n",
            "l0: 0.027234, l1: 0.029270, l2: 0.031606, l3: 0.029887, l4: 0.029636, l5: 0.038327, l6: 0.058142\n",
            "\n",
            "[epoch: 150/1000, batch:     6/   15, ite: 2241] train loss: 0.234577, tar: 0.025806 \n",
            "l0: 0.023943, l1: 0.022648, l2: 0.026198, l3: 0.027118, l4: 0.031312, l5: 0.042749, l6: 0.068757\n",
            "\n",
            "[epoch: 150/1000, batch:     7/   15, ite: 2242] train loss: 0.234771, tar: 0.025761 \n",
            "l0: 0.015561, l1: 0.016630, l2: 0.017082, l3: 0.017053, l4: 0.019346, l5: 0.033409, l6: 0.051598\n",
            "\n",
            "[epoch: 150/1000, batch:     8/   15, ite: 2243] train loss: 0.233281, tar: 0.025524 \n",
            "l0: 0.022079, l1: 0.028233, l2: 0.024857, l3: 0.020537, l4: 0.019567, l5: 0.025176, l6: 0.042320\n",
            "\n",
            "[epoch: 150/1000, batch:     9/   15, ite: 2244] train loss: 0.232133, tar: 0.025446 \n",
            "l0: 0.023767, l1: 0.023607, l2: 0.026834, l3: 0.028667, l4: 0.031383, l5: 0.041865, l6: 0.053251\n",
            "\n",
            "[epoch: 150/1000, batch:    10/   15, ite: 2245] train loss: 0.232072, tar: 0.025409 \n",
            "l0: 0.022194, l1: 0.023558, l2: 0.022536, l3: 0.027716, l4: 0.029652, l5: 0.036959, l6: 0.061868\n",
            "\n",
            "[epoch: 150/1000, batch:    11/   15, ite: 2246] train loss: 0.231907, tar: 0.025339 \n",
            "l0: 0.017823, l1: 0.017138, l2: 0.019569, l3: 0.021440, l4: 0.025037, l5: 0.035448, l6: 0.051658\n",
            "\n",
            "[epoch: 150/1000, batch:    12/   15, ite: 2247] train loss: 0.230975, tar: 0.025179 \n",
            "l0: 0.019777, l1: 0.020969, l2: 0.022650, l3: 0.023636, l4: 0.025966, l5: 0.034402, l6: 0.051099\n",
            "\n",
            "[epoch: 150/1000, batch:    13/   15, ite: 2248] train loss: 0.230298, tar: 0.025066 \n",
            "l0: 0.022255, l1: 0.020885, l2: 0.024826, l3: 0.029191, l4: 0.031835, l5: 0.040136, l6: 0.056819\n",
            "\n",
            "[epoch: 150/1000, batch:    14/   15, ite: 2249] train loss: 0.230209, tar: 0.025009 \n",
            "l0: 0.017123, l1: 0.018725, l2: 0.020455, l3: 0.022249, l4: 0.025638, l5: 0.033654, l6: 0.056564\n",
            "\n",
            "[epoch: 150/1000, batch:    15/   15, ite: 2250] train loss: 0.229493, tar: 0.024851 \n",
            "l0: 0.021443, l1: 0.024792, l2: 0.025903, l3: 0.029095, l4: 0.024026, l5: 0.025708, l6: 0.047981\n",
            "\n",
            "[epoch: 151/1000, batch:     1/   15, ite: 2251] train loss: 0.228894, tar: 0.024784 \n",
            "l0: 0.019463, l1: 0.020025, l2: 0.019534, l3: 0.024468, l4: 0.026068, l5: 0.030745, l6: 0.039101\n",
            "\n",
            "[epoch: 151/1000, batch:     2/   15, ite: 2252] train loss: 0.227943, tar: 0.024682 \n",
            "l0: 0.021325, l1: 0.026673, l2: 0.025414, l3: 0.025820, l4: 0.028215, l5: 0.027112, l6: 0.039623\n",
            "\n",
            "[epoch: 151/1000, batch:     3/   15, ite: 2253] train loss: 0.227306, tar: 0.024619 \n",
            "l0: 0.020923, l1: 0.022949, l2: 0.023252, l3: 0.025416, l4: 0.027868, l5: 0.040701, l6: 0.078681\n",
            "\n",
            "[epoch: 151/1000, batch:     4/   15, ite: 2254] train loss: 0.227537, tar: 0.024550 \n",
            "l0: 0.020702, l1: 0.019320, l2: 0.021207, l3: 0.025944, l4: 0.027663, l5: 0.044146, l6: 0.058083\n",
            "\n",
            "[epoch: 151/1000, batch:     5/   15, ite: 2255] train loss: 0.227347, tar: 0.024480 \n",
            "l0: 0.018014, l1: 0.017740, l2: 0.019444, l3: 0.022129, l4: 0.025083, l5: 0.034818, l6: 0.052636\n",
            "\n",
            "[epoch: 151/1000, batch:     6/   15, ite: 2256] train loss: 0.226677, tar: 0.024365 \n",
            "l0: 0.021277, l1: 0.021047, l2: 0.019095, l3: 0.020993, l4: 0.024689, l5: 0.035368, l6: 0.037704\n",
            "\n",
            "[epoch: 151/1000, batch:     7/   15, ite: 2257] train loss: 0.225861, tar: 0.024311 \n",
            "l0: 0.017705, l1: 0.017733, l2: 0.019349, l3: 0.024730, l4: 0.029990, l5: 0.028203, l6: 0.033460\n",
            "\n",
            "[epoch: 151/1000, batch:     8/   15, ite: 2258] train loss: 0.224918, tar: 0.024197 \n",
            "l0: 0.018684, l1: 0.018903, l2: 0.018614, l3: 0.021125, l4: 0.024619, l5: 0.036008, l6: 0.047782\n",
            "\n",
            "[epoch: 151/1000, batch:     9/   15, ite: 2259] train loss: 0.224254, tar: 0.024103 \n",
            "l0: 0.018396, l1: 0.017420, l2: 0.019089, l3: 0.020510, l4: 0.024967, l5: 0.035579, l6: 0.052574\n",
            "\n",
            "[epoch: 151/1000, batch:    10/   15, ite: 2260] train loss: 0.223659, tar: 0.024008 \n",
            "l0: 0.028863, l1: 0.030307, l2: 0.031093, l3: 0.035603, l4: 0.038304, l5: 0.037310, l6: 0.049585\n",
            "\n",
            "[epoch: 151/1000, batch:    11/   15, ite: 2261] train loss: 0.224108, tar: 0.024088 \n",
            "l0: 0.025564, l1: 0.025976, l2: 0.031521, l3: 0.030375, l4: 0.033170, l5: 0.036168, l6: 0.078191\n",
            "\n",
            "[epoch: 151/1000, batch:    12/   15, ite: 2262] train loss: 0.224703, tar: 0.024112 \n",
            "l0: 0.019626, l1: 0.022460, l2: 0.022199, l3: 0.022180, l4: 0.024870, l5: 0.026366, l6: 0.048833\n",
            "\n",
            "[epoch: 151/1000, batch:    13/   15, ite: 2263] train loss: 0.224097, tar: 0.024040 \n",
            "l0: 0.031383, l1: 0.035549, l2: 0.031961, l3: 0.032307, l4: 0.041769, l5: 0.043679, l6: 0.043168\n",
            "\n",
            "[epoch: 151/1000, batch:    14/   15, ite: 2264] train loss: 0.224655, tar: 0.024155 \n",
            "l0: 0.032300, l1: 0.033000, l2: 0.034674, l3: 0.038562, l4: 0.034729, l5: 0.036861, l6: 0.061816\n",
            "\n",
            "[epoch: 151/1000, batch:    15/   15, ite: 2265] train loss: 0.225382, tar: 0.024280 \n",
            "l0: 0.020997, l1: 0.021567, l2: 0.023093, l3: 0.024777, l4: 0.027297, l5: 0.032803, l6: 0.048331\n",
            "\n",
            "[epoch: 152/1000, batch:     1/   15, ite: 2266] train loss: 0.224981, tar: 0.024231 \n",
            "l0: 0.019830, l1: 0.017324, l2: 0.019946, l3: 0.022786, l4: 0.026229, l5: 0.033550, l6: 0.058652\n",
            "\n",
            "[epoch: 152/1000, batch:     2/   15, ite: 2267] train loss: 0.224583, tar: 0.024165 \n",
            "l0: 0.018426, l1: 0.019172, l2: 0.020548, l3: 0.022006, l4: 0.023976, l5: 0.030579, l6: 0.042585\n",
            "\n",
            "[epoch: 152/1000, batch:     3/   15, ite: 2268] train loss: 0.223887, tar: 0.024081 \n",
            "l0: 0.017993, l1: 0.018556, l2: 0.020342, l3: 0.023607, l4: 0.029311, l5: 0.035299, l6: 0.065257\n",
            "\n",
            "[epoch: 152/1000, batch:     4/   15, ite: 2269] train loss: 0.223691, tar: 0.023992 \n",
            "l0: 0.018588, l1: 0.018809, l2: 0.021560, l3: 0.025335, l4: 0.030247, l5: 0.031518, l6: 0.051162\n",
            "\n",
            "[epoch: 152/1000, batch:     5/   15, ite: 2270] train loss: 0.223313, tar: 0.023915 \n",
            "l0: 0.026339, l1: 0.027206, l2: 0.031924, l3: 0.033542, l4: 0.030514, l5: 0.033166, l6: 0.051072\n",
            "\n",
            "[epoch: 152/1000, batch:     6/   15, ite: 2271] train loss: 0.223460, tar: 0.023949 \n",
            "l0: 0.015139, l1: 0.015671, l2: 0.016741, l3: 0.017749, l4: 0.019292, l5: 0.021988, l6: 0.031406\n",
            "\n",
            "[epoch: 152/1000, batch:     7/   15, ite: 2272] train loss: 0.222273, tar: 0.023827 \n",
            "l0: 0.028273, l1: 0.031128, l2: 0.039578, l3: 0.034596, l4: 0.032123, l5: 0.027305, l6: 0.033770\n",
            "\n",
            "[epoch: 152/1000, batch:     8/   15, ite: 2273] train loss: 0.222335, tar: 0.023888 \n",
            "l0: 0.025963, l1: 0.026777, l2: 0.032420, l3: 0.033142, l4: 0.033248, l5: 0.035142, l6: 0.053737\n",
            "\n",
            "[epoch: 152/1000, batch:     9/   15, ite: 2274] train loss: 0.222579, tar: 0.023916 \n",
            "l0: 0.016407, l1: 0.016523, l2: 0.017091, l3: 0.018646, l4: 0.022200, l5: 0.026680, l6: 0.042859\n",
            "\n",
            "[epoch: 152/1000, batch:    10/   15, ite: 2275] train loss: 0.221750, tar: 0.023816 \n",
            "l0: 0.019644, l1: 0.020258, l2: 0.021758, l3: 0.024701, l4: 0.025976, l5: 0.029131, l6: 0.062446\n",
            "\n",
            "[epoch: 152/1000, batch:    11/   15, ite: 2276] train loss: 0.221516, tar: 0.023761 \n",
            "l0: 0.022391, l1: 0.020632, l2: 0.025622, l3: 0.025702, l4: 0.032511, l5: 0.035042, l6: 0.040834\n",
            "\n",
            "[epoch: 152/1000, batch:    12/   15, ite: 2277] train loss: 0.221272, tar: 0.023743 \n",
            "l0: 0.017911, l1: 0.018243, l2: 0.019635, l3: 0.018330, l4: 0.020780, l5: 0.024323, l6: 0.045067\n",
            "\n",
            "[epoch: 152/1000, batch:    13/   15, ite: 2278] train loss: 0.220541, tar: 0.023668 \n",
            "l0: 0.020537, l1: 0.020420, l2: 0.024992, l3: 0.028806, l4: 0.027946, l5: 0.034443, l6: 0.042597\n",
            "\n",
            "[epoch: 152/1000, batch:    14/   15, ite: 2279] train loss: 0.220278, tar: 0.023629 \n",
            "l0: 0.020468, l1: 0.018700, l2: 0.023137, l3: 0.025847, l4: 0.027202, l5: 0.032407, l6: 0.052307\n",
            "\n",
            "[epoch: 152/1000, batch:    15/   15, ite: 2280] train loss: 0.220025, tar: 0.023589 \n",
            "l0: 0.017646, l1: 0.018357, l2: 0.020192, l3: 0.023011, l4: 0.026804, l5: 0.031185, l6: 0.041145\n",
            "\n",
            "[epoch: 153/1000, batch:     1/   15, ite: 2281] train loss: 0.219511, tar: 0.023516 \n",
            "l0: 0.023329, l1: 0.022803, l2: 0.028110, l3: 0.026652, l4: 0.028599, l5: 0.033483, l6: 0.043118\n",
            "\n",
            "[epoch: 153/1000, batch:     2/   15, ite: 2282] train loss: 0.219347, tar: 0.023513 \n",
            "l0: 0.022129, l1: 0.022142, l2: 0.027338, l3: 0.027846, l4: 0.031440, l5: 0.032571, l6: 0.046859\n",
            "\n",
            "[epoch: 153/1000, batch:     3/   15, ite: 2283] train loss: 0.219238, tar: 0.023497 \n",
            "l0: 0.027003, l1: 0.024678, l2: 0.026423, l3: 0.033512, l4: 0.034440, l5: 0.040163, l6: 0.056607\n",
            "\n",
            "[epoch: 153/1000, batch:     4/   15, ite: 2284] train loss: 0.219519, tar: 0.023539 \n",
            "l0: 0.017914, l1: 0.015956, l2: 0.018229, l3: 0.022582, l4: 0.027754, l5: 0.031528, l6: 0.044824\n",
            "\n",
            "[epoch: 153/1000, batch:     5/   15, ite: 2285] train loss: 0.219040, tar: 0.023472 \n",
            "l0: 0.025948, l1: 0.025202, l2: 0.027399, l3: 0.029736, l4: 0.028543, l5: 0.037288, l6: 0.041939\n",
            "\n",
            "[epoch: 153/1000, batch:     6/   15, ite: 2286] train loss: 0.219005, tar: 0.023501 \n",
            "l0: 0.014043, l1: 0.013293, l2: 0.014022, l3: 0.015837, l4: 0.019638, l5: 0.023099, l6: 0.034255\n",
            "\n",
            "[epoch: 153/1000, batch:     7/   15, ite: 2287] train loss: 0.218030, tar: 0.023392 \n",
            "l0: 0.023323, l1: 0.023063, l2: 0.025142, l3: 0.027800, l4: 0.030209, l5: 0.033228, l6: 0.049994\n",
            "\n",
            "[epoch: 153/1000, batch:     8/   15, ite: 2288] train loss: 0.217970, tar: 0.023392 \n",
            "l0: 0.016705, l1: 0.019509, l2: 0.017588, l3: 0.018275, l4: 0.021678, l5: 0.025649, l6: 0.037338\n",
            "\n",
            "[epoch: 153/1000, batch:     9/   15, ite: 2289] train loss: 0.217282, tar: 0.023317 \n",
            "l0: 0.027994, l1: 0.026432, l2: 0.033699, l3: 0.037399, l4: 0.041622, l5: 0.044657, l6: 0.059559\n",
            "\n",
            "[epoch: 153/1000, batch:    10/   15, ite: 2290] train loss: 0.217883, tar: 0.023368 \n",
            "l0: 0.023417, l1: 0.026182, l2: 0.031024, l3: 0.028373, l4: 0.028943, l5: 0.035351, l6: 0.062187\n",
            "\n",
            "[epoch: 153/1000, batch:    11/   15, ite: 2291] train loss: 0.218077, tar: 0.023369 \n",
            "l0: 0.019931, l1: 0.019649, l2: 0.019898, l3: 0.022276, l4: 0.027009, l5: 0.029201, l6: 0.040685\n",
            "\n",
            "[epoch: 153/1000, batch:    12/   15, ite: 2292] train loss: 0.217648, tar: 0.023332 \n",
            "l0: 0.017933, l1: 0.018151, l2: 0.023057, l3: 0.023705, l4: 0.024440, l5: 0.023871, l6: 0.046750\n",
            "\n",
            "[epoch: 153/1000, batch:    13/   15, ite: 2293] train loss: 0.217221, tar: 0.023274 \n",
            "l0: 0.015671, l1: 0.015469, l2: 0.017194, l3: 0.018752, l4: 0.020797, l5: 0.025200, l6: 0.034065\n",
            "\n",
            "[epoch: 153/1000, batch:    14/   15, ite: 2294] train loss: 0.216475, tar: 0.023193 \n",
            "l0: 0.018089, l1: 0.019800, l2: 0.021311, l3: 0.020453, l4: 0.023474, l5: 0.029553, l6: 0.036045\n",
            "\n",
            "[epoch: 153/1000, batch:    15/   15, ite: 2295] train loss: 0.215973, tar: 0.023139 \n",
            "l0: 0.018054, l1: 0.018514, l2: 0.021589, l3: 0.023641, l4: 0.026198, l5: 0.031005, l6: 0.060251\n",
            "\n",
            "[epoch: 154/1000, batch:     1/   15, ite: 2296] train loss: 0.215798, tar: 0.023086 \n",
            "l0: 0.013871, l1: 0.012941, l2: 0.014887, l3: 0.017994, l4: 0.019040, l5: 0.024401, l6: 0.043610\n",
            "\n",
            "[epoch: 154/1000, batch:     2/   15, ite: 2297] train loss: 0.215087, tar: 0.022991 \n",
            "l0: 0.022951, l1: 0.023079, l2: 0.025327, l3: 0.026131, l4: 0.026328, l5: 0.032908, l6: 0.046569\n",
            "\n",
            "[epoch: 154/1000, batch:     3/   15, ite: 2298] train loss: 0.214966, tar: 0.022991 \n",
            "l0: 0.015865, l1: 0.016974, l2: 0.017829, l3: 0.018393, l4: 0.020797, l5: 0.027093, l6: 0.034215\n",
            "\n",
            "[epoch: 154/1000, batch:     4/   15, ite: 2299] train loss: 0.214322, tar: 0.022919 \n",
            "l0: 0.030697, l1: 0.031773, l2: 0.032371, l3: 0.031824, l4: 0.032901, l5: 0.039910, l6: 0.058022\n",
            "\n",
            "[epoch: 154/1000, batch:     5/   15, ite: 2300] train loss: 0.214754, tar: 0.022996 \n",
            "l0: 0.020294, l1: 0.019977, l2: 0.023103, l3: 0.025088, l4: 0.030635, l5: 0.032649, l6: 0.041934\n",
            "\n",
            "[epoch: 154/1000, batch:     6/   15, ite: 2301] train loss: 0.193681, tar: 0.020294 \n",
            "l0: 0.017373, l1: 0.015758, l2: 0.020383, l3: 0.024228, l4: 0.029914, l5: 0.028675, l6: 0.038074\n",
            "\n",
            "[epoch: 154/1000, batch:     7/   15, ite: 2302] train loss: 0.184043, tar: 0.018833 \n",
            "l0: 0.026641, l1: 0.029615, l2: 0.027479, l3: 0.028818, l4: 0.028782, l5: 0.029616, l6: 0.031620\n",
            "\n",
            "[epoch: 154/1000, batch:     8/   15, ite: 2303] train loss: 0.190219, tar: 0.021436 \n",
            "l0: 0.017103, l1: 0.017149, l2: 0.019234, l3: 0.020413, l4: 0.022039, l5: 0.025066, l6: 0.044649\n",
            "\n",
            "[epoch: 154/1000, batch:     9/   15, ite: 2304] train loss: 0.184078, tar: 0.020353 \n",
            "l0: 0.017504, l1: 0.017123, l2: 0.018824, l3: 0.023121, l4: 0.029421, l5: 0.032266, l6: 0.055224\n",
            "\n",
            "[epoch: 154/1000, batch:    10/   15, ite: 2305] train loss: 0.185959, tar: 0.019783 \n",
            "l0: 0.020425, l1: 0.020455, l2: 0.023475, l3: 0.027430, l4: 0.030345, l5: 0.030495, l6: 0.055736\n",
            "\n",
            "[epoch: 154/1000, batch:    11/   15, ite: 2306] train loss: 0.189693, tar: 0.019890 \n",
            "l0: 0.017065, l1: 0.018408, l2: 0.020065, l3: 0.023142, l4: 0.025466, l5: 0.026695, l6: 0.046639\n",
            "\n",
            "[epoch: 154/1000, batch:    12/   15, ite: 2307] train loss: 0.187948, tar: 0.019486 \n",
            "l0: 0.016294, l1: 0.016177, l2: 0.018330, l3: 0.022525, l4: 0.025564, l5: 0.025583, l6: 0.052021\n",
            "\n",
            "[epoch: 154/1000, batch:    13/   15, ite: 2308] train loss: 0.186516, tar: 0.019087 \n",
            "l0: 0.013131, l1: 0.013656, l2: 0.013808, l3: 0.014778, l4: 0.018281, l5: 0.024009, l6: 0.043258\n",
            "\n",
            "[epoch: 154/1000, batch:    14/   15, ite: 2309] train loss: 0.181450, tar: 0.018426 \n",
            "l0: 0.017654, l1: 0.017210, l2: 0.023369, l3: 0.023488, l4: 0.022910, l5: 0.025704, l6: 0.046125\n",
            "\n",
            "[epoch: 154/1000, batch:    15/   15, ite: 2310] train loss: 0.180951, tar: 0.018348 \n",
            "l0: 0.015870, l1: 0.016362, l2: 0.018810, l3: 0.020876, l4: 0.020935, l5: 0.024193, l6: 0.042344\n",
            "\n",
            "[epoch: 155/1000, batch:     1/   15, ite: 2311] train loss: 0.178991, tar: 0.018123 \n",
            "l0: 0.019660, l1: 0.019675, l2: 0.021513, l3: 0.022269, l4: 0.026336, l5: 0.031543, l6: 0.043382\n",
            "\n",
            "[epoch: 155/1000, batch:     2/   15, ite: 2312] train loss: 0.179440, tar: 0.018251 \n",
            "l0: 0.025585, l1: 0.025854, l2: 0.027647, l3: 0.029830, l4: 0.031880, l5: 0.032808, l6: 0.053779\n",
            "\n",
            "[epoch: 155/1000, batch:     3/   15, ite: 2313] train loss: 0.183128, tar: 0.018815 \n",
            "l0: 0.026517, l1: 0.027380, l2: 0.030179, l3: 0.030564, l4: 0.031114, l5: 0.037402, l6: 0.045228\n",
            "\n",
            "[epoch: 155/1000, batch:     4/   15, ite: 2314] train loss: 0.186360, tar: 0.019365 \n",
            "l0: 0.014869, l1: 0.014691, l2: 0.017347, l3: 0.019966, l4: 0.021754, l5: 0.027608, l6: 0.040696\n",
            "\n",
            "[epoch: 155/1000, batch:     5/   15, ite: 2315] train loss: 0.184398, tar: 0.019066 \n",
            "l0: 0.017345, l1: 0.016633, l2: 0.020882, l3: 0.021309, l4: 0.024720, l5: 0.025641, l6: 0.042846\n",
            "\n",
            "[epoch: 155/1000, batch:     6/   15, ite: 2316] train loss: 0.183459, tar: 0.018958 \n",
            "l0: 0.020255, l1: 0.019261, l2: 0.025812, l3: 0.025610, l4: 0.027072, l5: 0.030547, l6: 0.040990\n",
            "\n",
            "[epoch: 155/1000, batch:     7/   15, ite: 2317] train loss: 0.183817, tar: 0.019034 \n",
            "l0: 0.025466, l1: 0.027729, l2: 0.027319, l3: 0.028029, l4: 0.034580, l5: 0.037111, l6: 0.050296\n",
            "\n",
            "[epoch: 155/1000, batch:     8/   15, ite: 2318] train loss: 0.186412, tar: 0.019392 \n",
            "l0: 0.019259, l1: 0.019551, l2: 0.019802, l3: 0.022035, l4: 0.026389, l5: 0.026799, l6: 0.039430\n",
            "\n",
            "[epoch: 155/1000, batch:     9/   15, ite: 2319] train loss: 0.185720, tar: 0.019385 \n",
            "l0: 0.014805, l1: 0.015031, l2: 0.016917, l3: 0.018891, l4: 0.021738, l5: 0.025174, l6: 0.036683\n",
            "\n",
            "[epoch: 155/1000, batch:    10/   15, ite: 2320] train loss: 0.183896, tar: 0.019156 \n",
            "l0: 0.020755, l1: 0.025593, l2: 0.023527, l3: 0.022623, l4: 0.024169, l5: 0.024772, l6: 0.036923\n",
            "\n",
            "[epoch: 155/1000, batch:    11/   15, ite: 2321] train loss: 0.183633, tar: 0.019232 \n",
            "l0: 0.016327, l1: 0.015888, l2: 0.019992, l3: 0.018649, l4: 0.021290, l5: 0.024569, l6: 0.041616\n",
            "\n",
            "[epoch: 155/1000, batch:    12/   15, ite: 2322] train loss: 0.182483, tar: 0.019100 \n",
            "l0: 0.021681, l1: 0.022114, l2: 0.025513, l3: 0.024668, l4: 0.026201, l5: 0.030627, l6: 0.044235\n",
            "\n",
            "[epoch: 155/1000, batch:    13/   15, ite: 2323] train loss: 0.183029, tar: 0.019212 \n",
            "l0: 0.039758, l1: 0.038487, l2: 0.045852, l3: 0.041866, l4: 0.044716, l5: 0.043907, l6: 0.055565\n",
            "\n",
            "[epoch: 155/1000, batch:    14/   15, ite: 2324] train loss: 0.188325, tar: 0.020068 \n",
            "l0: 0.019307, l1: 0.018894, l2: 0.022979, l3: 0.028487, l4: 0.031865, l5: 0.029526, l6: 0.037483\n",
            "\n",
            "[epoch: 155/1000, batch:    15/   15, ite: 2325] train loss: 0.188334, tar: 0.020038 \n",
            "l0: 0.019865, l1: 0.018276, l2: 0.023696, l3: 0.025073, l4: 0.026194, l5: 0.031156, l6: 0.046169\n",
            "\n",
            "[epoch: 156/1000, batch:     1/   15, ite: 2326] train loss: 0.188415, tar: 0.020031 \n",
            "l0: 0.015981, l1: 0.015681, l2: 0.018226, l3: 0.019548, l4: 0.023510, l5: 0.029922, l6: 0.044430\n",
            "\n",
            "[epoch: 156/1000, batch:     2/   15, ite: 2327] train loss: 0.187632, tar: 0.019881 \n",
            "l0: 0.015494, l1: 0.015790, l2: 0.016761, l3: 0.018705, l4: 0.021451, l5: 0.027082, l6: 0.032774\n",
            "\n",
            "[epoch: 156/1000, batch:     3/   15, ite: 2328] train loss: 0.186219, tar: 0.019724 \n",
            "l0: 0.017294, l1: 0.018096, l2: 0.017671, l3: 0.021558, l4: 0.023622, l5: 0.026819, l6: 0.036743\n",
            "\n",
            "[epoch: 156/1000, batch:     4/   15, ite: 2329] train loss: 0.185377, tar: 0.019641 \n",
            "l0: 0.018722, l1: 0.018391, l2: 0.024200, l3: 0.027240, l4: 0.028857, l5: 0.036051, l6: 0.055889\n",
            "\n",
            "[epoch: 156/1000, batch:     5/   15, ite: 2330] train loss: 0.186176, tar: 0.019610 \n",
            "l0: 0.017770, l1: 0.020014, l2: 0.019168, l3: 0.020825, l4: 0.020000, l5: 0.024375, l6: 0.043328\n",
            "\n",
            "[epoch: 156/1000, batch:     6/   15, ite: 2331] train loss: 0.185509, tar: 0.019551 \n",
            "l0: 0.014507, l1: 0.014745, l2: 0.016434, l3: 0.020296, l4: 0.020497, l5: 0.022991, l6: 0.039464\n",
            "\n",
            "[epoch: 156/1000, batch:     7/   15, ite: 2332] train loss: 0.184366, tar: 0.019393 \n",
            "l0: 0.021275, l1: 0.022561, l2: 0.025427, l3: 0.026423, l4: 0.028009, l5: 0.031641, l6: 0.046091\n",
            "\n",
            "[epoch: 156/1000, batch:     8/   15, ite: 2333] train loss: 0.184883, tar: 0.019450 \n",
            "l0: 0.016513, l1: 0.017970, l2: 0.018242, l3: 0.018913, l4: 0.021634, l5: 0.025618, l6: 0.041450\n",
            "\n",
            "[epoch: 156/1000, batch:     9/   15, ite: 2334] train loss: 0.184161, tar: 0.019364 \n",
            "l0: 0.021153, l1: 0.023146, l2: 0.024514, l3: 0.025391, l4: 0.026108, l5: 0.032066, l6: 0.045061\n",
            "\n",
            "[epoch: 156/1000, batch:    10/   15, ite: 2335] train loss: 0.184540, tar: 0.019415 \n",
            "l0: 0.028714, l1: 0.027348, l2: 0.031532, l3: 0.036581, l4: 0.039736, l5: 0.039850, l6: 0.059021\n",
            "\n",
            "[epoch: 156/1000, batch:    11/   15, ite: 2336] train loss: 0.186714, tar: 0.019673 \n",
            "l0: 0.024326, l1: 0.026886, l2: 0.026613, l3: 0.029212, l4: 0.023235, l5: 0.027260, l6: 0.048916\n",
            "\n",
            "[epoch: 156/1000, batch:    12/   15, ite: 2337] train loss: 0.187247, tar: 0.019799 \n",
            "l0: 0.018022, l1: 0.017620, l2: 0.020598, l3: 0.022780, l4: 0.023420, l5: 0.027735, l6: 0.052642\n",
            "\n",
            "[epoch: 156/1000, batch:    13/   15, ite: 2338] train loss: 0.187130, tar: 0.019752 \n",
            "l0: 0.013363, l1: 0.012594, l2: 0.014176, l3: 0.016333, l4: 0.019406, l5: 0.022617, l6: 0.036748\n",
            "\n",
            "[epoch: 156/1000, batch:    14/   15, ite: 2339] train loss: 0.185800, tar: 0.019588 \n",
            "l0: 0.020550, l1: 0.022509, l2: 0.022168, l3: 0.024938, l4: 0.023323, l5: 0.023206, l6: 0.035220\n",
            "\n",
            "[epoch: 156/1000, batch:    15/   15, ite: 2340] train loss: 0.185453, tar: 0.019612 \n",
            "l0: 0.020831, l1: 0.020174, l2: 0.024939, l3: 0.024355, l4: 0.029751, l5: 0.034017, l6: 0.054592\n",
            "\n",
            "[epoch: 157/1000, batch:     1/   15, ite: 2341] train loss: 0.186019, tar: 0.019642 \n",
            "l0: 0.018603, l1: 0.018362, l2: 0.020663, l3: 0.022093, l4: 0.027863, l5: 0.034352, l6: 0.042037\n",
            "\n",
            "[epoch: 157/1000, batch:     2/   15, ite: 2342] train loss: 0.185970, tar: 0.019617 \n",
            "l0: 0.013277, l1: 0.012565, l2: 0.014028, l3: 0.015822, l4: 0.018179, l5: 0.025255, l6: 0.039365\n",
            "\n",
            "[epoch: 157/1000, batch:     3/   15, ite: 2343] train loss: 0.184866, tar: 0.019470 \n",
            "l0: 0.017725, l1: 0.018701, l2: 0.018138, l3: 0.020383, l4: 0.020840, l5: 0.024479, l6: 0.037081\n",
            "\n",
            "[epoch: 157/1000, batch:     4/   15, ite: 2344] train loss: 0.184240, tar: 0.019430 \n",
            "l0: 0.019684, l1: 0.016893, l2: 0.022549, l3: 0.021916, l4: 0.021001, l5: 0.032841, l6: 0.065798\n",
            "\n",
            "[epoch: 157/1000, batch:     5/   15, ite: 2345] train loss: 0.184606, tar: 0.019436 \n",
            "l0: 0.025004, l1: 0.026126, l2: 0.027167, l3: 0.027775, l4: 0.030198, l5: 0.033680, l6: 0.062154\n",
            "\n",
            "[epoch: 157/1000, batch:     6/   15, ite: 2346] train loss: 0.185638, tar: 0.019557 \n",
            "l0: 0.018144, l1: 0.019599, l2: 0.019027, l3: 0.019248, l4: 0.018609, l5: 0.024773, l6: 0.037609\n",
            "\n",
            "[epoch: 157/1000, batch:     7/   15, ite: 2347] train loss: 0.185029, tar: 0.019527 \n",
            "l0: 0.013555, l1: 0.012596, l2: 0.015255, l3: 0.019350, l4: 0.020145, l5: 0.023673, l6: 0.046541\n",
            "\n",
            "[epoch: 157/1000, batch:     8/   15, ite: 2348] train loss: 0.184323, tar: 0.019402 \n",
            "l0: 0.020336, l1: 0.019856, l2: 0.026748, l3: 0.026283, l4: 0.024297, l5: 0.027665, l6: 0.050185\n",
            "\n",
            "[epoch: 157/1000, batch:     9/   15, ite: 2349] train loss: 0.184548, tar: 0.019422 \n",
            "l0: 0.026120, l1: 0.031431, l2: 0.030892, l3: 0.028815, l4: 0.024097, l5: 0.028313, l6: 0.033324\n",
            "\n",
            "[epoch: 157/1000, batch:    10/   15, ite: 2350] train loss: 0.184917, tar: 0.019555 \n",
            "l0: 0.023937, l1: 0.025455, l2: 0.027560, l3: 0.029476, l4: 0.028755, l5: 0.029945, l6: 0.046343\n",
            "\n",
            "[epoch: 157/1000, batch:    11/   15, ite: 2351] train loss: 0.185438, tar: 0.019641 \n",
            "l0: 0.015083, l1: 0.016249, l2: 0.017152, l3: 0.018887, l4: 0.018755, l5: 0.024760, l6: 0.040711\n",
            "\n",
            "[epoch: 157/1000, batch:    12/   15, ite: 2352] train loss: 0.184787, tar: 0.019554 \n",
            "l0: 0.016622, l1: 0.017082, l2: 0.020076, l3: 0.020347, l4: 0.022334, l5: 0.027670, l6: 0.051430\n",
            "\n",
            "[epoch: 157/1000, batch:    13/   15, ite: 2353] train loss: 0.184613, tar: 0.019498 \n",
            "l0: 0.016717, l1: 0.016032, l2: 0.023308, l3: 0.025799, l4: 0.024406, l5: 0.028192, l6: 0.043451\n",
            "\n",
            "[epoch: 157/1000, batch:    14/   15, ite: 2354] train loss: 0.184489, tar: 0.019447 \n",
            "l0: 0.021874, l1: 0.021211, l2: 0.024366, l3: 0.025852, l4: 0.029338, l5: 0.033826, l6: 0.045452\n",
            "\n",
            "[epoch: 157/1000, batch:    15/   15, ite: 2355] train loss: 0.184805, tar: 0.019491 \n",
            "l0: 0.024532, l1: 0.023480, l2: 0.030092, l3: 0.030873, l4: 0.033816, l5: 0.033583, l6: 0.062636\n",
            "\n",
            "[epoch: 158/1000, batch:     1/   15, ite: 2356] train loss: 0.185773, tar: 0.019581 \n",
            "l0: 0.015160, l1: 0.017412, l2: 0.017768, l3: 0.019615, l4: 0.019614, l5: 0.028860, l6: 0.040343\n",
            "\n",
            "[epoch: 158/1000, batch:     2/   15, ite: 2357] train loss: 0.185300, tar: 0.019504 \n",
            "l0: 0.017012, l1: 0.014318, l2: 0.018032, l3: 0.022652, l4: 0.025957, l5: 0.029734, l6: 0.044962\n",
            "\n",
            "[epoch: 158/1000, batch:     3/   15, ite: 2358] train loss: 0.185082, tar: 0.019461 \n",
            "l0: 0.020558, l1: 0.024014, l2: 0.023933, l3: 0.024757, l4: 0.027367, l5: 0.025460, l6: 0.035315\n",
            "\n",
            "[epoch: 158/1000, batch:     4/   15, ite: 2359] train loss: 0.185020, tar: 0.019479 \n",
            "l0: 0.020392, l1: 0.018386, l2: 0.021888, l3: 0.027173, l4: 0.029029, l5: 0.032071, l6: 0.046988\n",
            "\n",
            "[epoch: 158/1000, batch:     5/   15, ite: 2360] train loss: 0.185201, tar: 0.019494 \n",
            "l0: 0.026187, l1: 0.027894, l2: 0.030952, l3: 0.031603, l4: 0.027471, l5: 0.029628, l6: 0.043794\n",
            "\n",
            "[epoch: 158/1000, batch:     6/   15, ite: 2361] train loss: 0.185731, tar: 0.019604 \n",
            "l0: 0.014394, l1: 0.014442, l2: 0.014594, l3: 0.016562, l4: 0.018271, l5: 0.023059, l6: 0.039624\n",
            "\n",
            "[epoch: 158/1000, batch:     7/   15, ite: 2362] train loss: 0.185009, tar: 0.019520 \n",
            "l0: 0.019895, l1: 0.020513, l2: 0.023543, l3: 0.024126, l4: 0.021991, l5: 0.024127, l6: 0.039727\n",
            "\n",
            "[epoch: 158/1000, batch:     8/   15, ite: 2363] train loss: 0.184833, tar: 0.019526 \n",
            "l0: 0.026885, l1: 0.026385, l2: 0.031169, l3: 0.029824, l4: 0.030588, l5: 0.035713, l6: 0.058966\n",
            "\n",
            "[epoch: 158/1000, batch:     9/   15, ite: 2364] train loss: 0.185688, tar: 0.019641 \n",
            "l0: 0.014599, l1: 0.016509, l2: 0.016938, l3: 0.016357, l4: 0.017908, l5: 0.022385, l6: 0.043263\n",
            "\n",
            "[epoch: 158/1000, batch:    10/   15, ite: 2365] train loss: 0.185107, tar: 0.019563 \n",
            "l0: 0.017292, l1: 0.016216, l2: 0.020578, l3: 0.022296, l4: 0.025126, l5: 0.030268, l6: 0.035540\n",
            "\n",
            "[epoch: 158/1000, batch:    11/   15, ite: 2366] train loss: 0.184838, tar: 0.019529 \n",
            "l0: 0.022129, l1: 0.022244, l2: 0.023867, l3: 0.026526, l4: 0.027386, l5: 0.029561, l6: 0.043556\n",
            "\n",
            "[epoch: 158/1000, batch:    12/   15, ite: 2367] train loss: 0.184993, tar: 0.019568 \n",
            "l0: 0.014887, l1: 0.014627, l2: 0.016300, l3: 0.018865, l4: 0.019318, l5: 0.025206, l6: 0.040711\n",
            "\n",
            "[epoch: 158/1000, batch:    13/   15, ite: 2368] train loss: 0.184477, tar: 0.019499 \n",
            "l0: 0.020710, l1: 0.023493, l2: 0.022892, l3: 0.026287, l4: 0.027272, l5: 0.029351, l6: 0.042567\n",
            "\n",
            "[epoch: 158/1000, batch:    14/   15, ite: 2369] train loss: 0.184595, tar: 0.019517 \n",
            "l0: 0.021894, l1: 0.018948, l2: 0.024063, l3: 0.029328, l4: 0.035221, l5: 0.042115, l6: 0.055376\n",
            "\n",
            "[epoch: 158/1000, batch:    15/   15, ite: 2370] train loss: 0.185200, tar: 0.019550 \n",
            "l0: 0.022964, l1: 0.026226, l2: 0.025177, l3: 0.028012, l4: 0.026409, l5: 0.028408, l6: 0.050388\n",
            "\n",
            "[epoch: 159/1000, batch:     1/   15, ite: 2371] train loss: 0.185515, tar: 0.019599 \n",
            "l0: 0.027111, l1: 0.029569, l2: 0.030156, l3: 0.034651, l4: 0.039114, l5: 0.035928, l6: 0.047785\n",
            "\n",
            "[epoch: 159/1000, batch:     2/   15, ite: 2372] train loss: 0.186332, tar: 0.019703 \n",
            "l0: 0.021200, l1: 0.026645, l2: 0.025167, l3: 0.024114, l4: 0.023120, l5: 0.021099, l6: 0.026815\n",
            "\n",
            "[epoch: 159/1000, batch:     3/   15, ite: 2373] train loss: 0.186083, tar: 0.019723 \n",
            "l0: 0.017381, l1: 0.020638, l2: 0.020928, l3: 0.022375, l4: 0.021573, l5: 0.027700, l6: 0.046124\n",
            "\n",
            "[epoch: 159/1000, batch:     4/   15, ite: 2374] train loss: 0.185956, tar: 0.019692 \n",
            "l0: 0.015982, l1: 0.015058, l2: 0.018619, l3: 0.020668, l4: 0.022985, l5: 0.025655, l6: 0.039386\n",
            "\n",
            "[epoch: 159/1000, batch:     5/   15, ite: 2375] train loss: 0.185588, tar: 0.019642 \n",
            "l0: 0.018889, l1: 0.017944, l2: 0.019683, l3: 0.021104, l4: 0.026396, l5: 0.033111, l6: 0.044748\n",
            "\n",
            "[epoch: 159/1000, batch:     6/   15, ite: 2376] train loss: 0.185539, tar: 0.019632 \n",
            "l0: 0.016555, l1: 0.016264, l2: 0.016252, l3: 0.018296, l4: 0.023047, l5: 0.026872, l6: 0.041581\n",
            "\n",
            "[epoch: 159/1000, batch:     7/   15, ite: 2377] train loss: 0.185193, tar: 0.019592 \n",
            "l0: 0.037423, l1: 0.035363, l2: 0.040182, l3: 0.044471, l4: 0.046721, l5: 0.051851, l6: 0.062681\n",
            "\n",
            "[epoch: 159/1000, batch:     8/   15, ite: 2378] train loss: 0.186904, tar: 0.019821 \n",
            "l0: 0.014984, l1: 0.013785, l2: 0.016732, l3: 0.020638, l4: 0.025470, l5: 0.028813, l6: 0.032699\n",
            "\n",
            "[epoch: 159/1000, batch:     9/   15, ite: 2379] train loss: 0.186477, tar: 0.019760 \n",
            "l0: 0.026811, l1: 0.025286, l2: 0.028108, l3: 0.032283, l4: 0.037728, l5: 0.043261, l6: 0.055621\n",
            "\n",
            "[epoch: 159/1000, batch:    10/   15, ite: 2380] train loss: 0.187259, tar: 0.019848 \n",
            "l0: 0.023174, l1: 0.022139, l2: 0.025032, l3: 0.027657, l4: 0.028415, l5: 0.034249, l6: 0.045104\n",
            "\n",
            "[epoch: 159/1000, batch:    11/   15, ite: 2381] train loss: 0.187488, tar: 0.019889 \n",
            "l0: 0.018891, l1: 0.018103, l2: 0.021269, l3: 0.025245, l4: 0.025267, l5: 0.026652, l6: 0.037596\n",
            "\n",
            "[epoch: 159/1000, batch:    12/   15, ite: 2382] train loss: 0.187312, tar: 0.019877 \n",
            "l0: 0.016334, l1: 0.016016, l2: 0.017609, l3: 0.018751, l4: 0.020196, l5: 0.024738, l6: 0.045058\n",
            "\n",
            "[epoch: 159/1000, batch:    13/   15, ite: 2383] train loss: 0.186967, tar: 0.019834 \n",
            "l0: 0.016603, l1: 0.014782, l2: 0.016621, l3: 0.020685, l4: 0.022084, l5: 0.033171, l6: 0.048585\n",
            "\n",
            "[epoch: 159/1000, batch:    14/   15, ite: 2384] train loss: 0.186795, tar: 0.019796 \n",
            "l0: 0.013566, l1: 0.012476, l2: 0.014231, l3: 0.016759, l4: 0.018900, l5: 0.028852, l6: 0.039093\n",
            "\n",
            "[epoch: 159/1000, batch:    15/   15, ite: 2385] train loss: 0.186290, tar: 0.019722 \n",
            "l0: 0.020792, l1: 0.019838, l2: 0.022356, l3: 0.025940, l4: 0.029122, l5: 0.035325, l6: 0.047169\n",
            "\n",
            "[epoch: 160/1000, batch:     1/   15, ite: 2386] train loss: 0.186456, tar: 0.019735 \n",
            "l0: 0.019406, l1: 0.017148, l2: 0.024561, l3: 0.024852, l4: 0.023466, l5: 0.029008, l6: 0.042551\n",
            "\n",
            "[epoch: 160/1000, batch:     2/   15, ite: 2387] train loss: 0.186393, tar: 0.019731 \n",
            "l0: 0.015653, l1: 0.015957, l2: 0.017360, l3: 0.017959, l4: 0.020073, l5: 0.023065, l6: 0.039751\n",
            "\n",
            "[epoch: 160/1000, batch:     3/   15, ite: 2388] train loss: 0.185977, tar: 0.019685 \n",
            "l0: 0.017381, l1: 0.015882, l2: 0.020914, l3: 0.022686, l4: 0.023622, l5: 0.027023, l6: 0.042453\n",
            "\n",
            "[epoch: 160/1000, batch:     4/   15, ite: 2389] train loss: 0.185797, tar: 0.019659 \n",
            "l0: 0.015414, l1: 0.015627, l2: 0.017621, l3: 0.019006, l4: 0.020439, l5: 0.025869, l6: 0.040237\n",
            "\n",
            "[epoch: 160/1000, batch:     5/   15, ite: 2390] train loss: 0.185446, tar: 0.019612 \n",
            "l0: 0.016998, l1: 0.018107, l2: 0.018358, l3: 0.018074, l4: 0.019086, l5: 0.023038, l6: 0.029568\n",
            "\n",
            "[epoch: 160/1000, batch:     6/   15, ite: 2391] train loss: 0.184983, tar: 0.019583 \n",
            "l0: 0.019334, l1: 0.020375, l2: 0.022799, l3: 0.024534, l4: 0.026985, l5: 0.028710, l6: 0.041844\n",
            "\n",
            "[epoch: 160/1000, batch:     7/   15, ite: 2392] train loss: 0.184978, tar: 0.019580 \n",
            "l0: 0.020693, l1: 0.018439, l2: 0.024814, l3: 0.026692, l4: 0.028402, l5: 0.031025, l6: 0.038078\n",
            "\n",
            "[epoch: 160/1000, batch:     8/   15, ite: 2393] train loss: 0.185012, tar: 0.019592 \n",
            "l0: 0.017908, l1: 0.018333, l2: 0.019121, l3: 0.021490, l4: 0.022709, l5: 0.027657, l6: 0.044700\n",
            "\n",
            "[epoch: 160/1000, batch:     9/   15, ite: 2394] train loss: 0.184873, tar: 0.019574 \n",
            "l0: 0.025270, l1: 0.023046, l2: 0.028160, l3: 0.032892, l4: 0.035871, l5: 0.044498, l6: 0.060223\n",
            "\n",
            "[epoch: 160/1000, batch:    10/   15, ite: 2395] train loss: 0.185558, tar: 0.019634 \n",
            "l0: 0.018953, l1: 0.020015, l2: 0.020042, l3: 0.022609, l4: 0.025393, l5: 0.027099, l6: 0.028949\n",
            "\n",
            "[epoch: 160/1000, batch:    11/   15, ite: 2396] train loss: 0.185324, tar: 0.019627 \n",
            "l0: 0.014361, l1: 0.013438, l2: 0.016123, l3: 0.018354, l4: 0.020610, l5: 0.027834, l6: 0.044270\n",
            "\n",
            "[epoch: 160/1000, batch:    12/   15, ite: 2397] train loss: 0.185011, tar: 0.019573 \n",
            "l0: 0.014242, l1: 0.013298, l2: 0.014847, l3: 0.016941, l4: 0.023676, l5: 0.029241, l6: 0.039814\n",
            "\n",
            "[epoch: 160/1000, batch:    13/   15, ite: 2398] train loss: 0.184675, tar: 0.019518 \n",
            "l0: 0.032347, l1: 0.032633, l2: 0.034066, l3: 0.037250, l4: 0.040145, l5: 0.043989, l6: 0.068334\n",
            "\n",
            "[epoch: 160/1000, batch:    14/   15, ite: 2399] train loss: 0.185726, tar: 0.019648 \n",
            "l0: 0.023366, l1: 0.026527, l2: 0.025189, l3: 0.024445, l4: 0.025665, l5: 0.028823, l6: 0.043377\n",
            "\n",
            "[epoch: 160/1000, batch:    15/   15, ite: 2400] train loss: 0.185843, tar: 0.019685 \n",
            "l0: 0.017870, l1: 0.019288, l2: 0.018424, l3: 0.021647, l4: 0.021079, l5: 0.023782, l6: 0.035114\n",
            "\n",
            "[epoch: 161/1000, batch:     1/   15, ite: 2401] train loss: 0.157204, tar: 0.017870 \n",
            "l0: 0.028549, l1: 0.026470, l2: 0.031420, l3: 0.032069, l4: 0.039949, l5: 0.042715, l6: 0.039202\n",
            "\n",
            "[epoch: 161/1000, batch:     2/   15, ite: 2402] train loss: 0.198789, tar: 0.023210 \n",
            "l0: 0.020570, l1: 0.019862, l2: 0.024259, l3: 0.025579, l4: 0.028474, l5: 0.035473, l6: 0.050142\n",
            "\n",
            "[epoch: 161/1000, batch:     3/   15, ite: 2403] train loss: 0.200646, tar: 0.022330 \n",
            "l0: 0.018440, l1: 0.018524, l2: 0.020468, l3: 0.022418, l4: 0.025606, l5: 0.029540, l6: 0.055847\n",
            "\n",
            "[epoch: 161/1000, batch:     4/   15, ite: 2404] train loss: 0.198196, tar: 0.021357 \n",
            "l0: 0.026333, l1: 0.028147, l2: 0.029830, l3: 0.030532, l4: 0.028970, l5: 0.032074, l6: 0.042808\n",
            "\n",
            "[epoch: 161/1000, batch:     5/   15, ite: 2405] train loss: 0.202295, tar: 0.022352 \n",
            "l0: 0.015227, l1: 0.015821, l2: 0.016539, l3: 0.017801, l4: 0.019681, l5: 0.026135, l6: 0.044563\n",
            "\n",
            "[epoch: 161/1000, batch:     6/   15, ite: 2406] train loss: 0.194540, tar: 0.021165 \n",
            "l0: 0.030694, l1: 0.029915, l2: 0.034734, l3: 0.037054, l4: 0.043770, l5: 0.046031, l6: 0.059258\n",
            "\n",
            "[epoch: 161/1000, batch:     7/   15, ite: 2407] train loss: 0.206957, tar: 0.022526 \n",
            "l0: 0.030021, l1: 0.031133, l2: 0.033609, l3: 0.032219, l4: 0.037102, l5: 0.044718, l6: 0.059200\n",
            "\n",
            "[epoch: 161/1000, batch:     8/   15, ite: 2408] train loss: 0.214587, tar: 0.023463 \n",
            "l0: 0.028114, l1: 0.030378, l2: 0.030547, l3: 0.034826, l4: 0.037398, l5: 0.032774, l6: 0.037521\n",
            "\n",
            "[epoch: 161/1000, batch:     9/   15, ite: 2409] train loss: 0.216473, tar: 0.023980 \n",
            "l0: 0.014730, l1: 0.014314, l2: 0.017891, l3: 0.021891, l4: 0.027031, l5: 0.032967, l6: 0.046775\n",
            "\n",
            "[epoch: 161/1000, batch:    10/   15, ite: 2410] train loss: 0.212385, tar: 0.023055 \n",
            "l0: 0.014328, l1: 0.013794, l2: 0.016743, l3: 0.019983, l4: 0.023356, l5: 0.029690, l6: 0.057123\n",
            "\n",
            "[epoch: 161/1000, batch:    11/   15, ite: 2411] train loss: 0.208988, tar: 0.022261 \n",
            "l0: 0.014834, l1: 0.016371, l2: 0.016589, l3: 0.018887, l4: 0.017137, l5: 0.020231, l6: 0.033639\n",
            "\n",
            "[epoch: 161/1000, batch:    12/   15, ite: 2412] train loss: 0.203047, tar: 0.021643 \n",
            "l0: 0.013196, l1: 0.014817, l2: 0.016777, l3: 0.015216, l4: 0.019659, l5: 0.027071, l6: 0.044336\n",
            "\n",
            "[epoch: 161/1000, batch:    13/   15, ite: 2413] train loss: 0.199048, tar: 0.020993 \n",
            "l0: 0.019407, l1: 0.019575, l2: 0.022375, l3: 0.023662, l4: 0.025617, l5: 0.027626, l6: 0.047358\n",
            "\n",
            "[epoch: 161/1000, batch:    14/   15, ite: 2414] train loss: 0.198089, tar: 0.020880 \n",
            "l0: 0.023661, l1: 0.023721, l2: 0.025662, l3: 0.027300, l4: 0.037699, l5: 0.043453, l6: 0.053710\n",
            "\n",
            "[epoch: 161/1000, batch:    15/   15, ite: 2415] train loss: 0.200564, tar: 0.021065 \n",
            "l0: 0.018784, l1: 0.020749, l2: 0.021163, l3: 0.022803, l4: 0.023678, l5: 0.026781, l6: 0.040218\n",
            "\n",
            "[epoch: 162/1000, batch:     1/   15, ite: 2416] train loss: 0.198914, tar: 0.020922 \n",
            "l0: 0.043682, l1: 0.048558, l2: 0.040992, l3: 0.038427, l4: 0.037494, l5: 0.040896, l6: 0.053906\n",
            "\n",
            "[epoch: 162/1000, batch:     2/   15, ite: 2417] train loss: 0.205093, tar: 0.022261 \n",
            "l0: 0.016513, l1: 0.015620, l2: 0.016931, l3: 0.019847, l4: 0.023593, l5: 0.028435, l6: 0.045581\n",
            "\n",
            "[epoch: 162/1000, batch:     3/   15, ite: 2418] train loss: 0.202950, tar: 0.021942 \n",
            "l0: 0.012632, l1: 0.012080, l2: 0.013715, l3: 0.015469, l4: 0.020037, l5: 0.025769, l6: 0.040093\n",
            "\n",
            "[epoch: 162/1000, batch:     4/   15, ite: 2419] train loss: 0.199626, tar: 0.021452 \n",
            "l0: 0.026648, l1: 0.025866, l2: 0.028158, l3: 0.030413, l4: 0.028273, l5: 0.034231, l6: 0.050378\n",
            "\n",
            "[epoch: 162/1000, batch:     5/   15, ite: 2420] train loss: 0.200844, tar: 0.021712 \n",
            "l0: 0.015073, l1: 0.016535, l2: 0.016726, l3: 0.017068, l4: 0.018987, l5: 0.022148, l6: 0.045390\n",
            "\n",
            "[epoch: 162/1000, batch:     6/   15, ite: 2421] train loss: 0.198514, tar: 0.021395 \n",
            "l0: 0.015659, l1: 0.015664, l2: 0.018880, l3: 0.021282, l4: 0.019721, l5: 0.022627, l6: 0.048603\n",
            "\n",
            "[epoch: 162/1000, batch:     7/   15, ite: 2422] train loss: 0.196874, tar: 0.021135 \n",
            "l0: 0.020702, l1: 0.021083, l2: 0.025248, l3: 0.025876, l4: 0.028619, l5: 0.028355, l6: 0.043709\n",
            "\n",
            "[epoch: 162/1000, batch:     8/   15, ite: 2423] train loss: 0.196732, tar: 0.021116 \n",
            "l0: 0.029102, l1: 0.028301, l2: 0.031006, l3: 0.038974, l4: 0.038567, l5: 0.041188, l6: 0.058895\n",
            "\n",
            "[epoch: 162/1000, batch:     9/   15, ite: 2424] train loss: 0.199619, tar: 0.021449 \n",
            "l0: 0.018129, l1: 0.020048, l2: 0.020393, l3: 0.020304, l4: 0.023412, l5: 0.027016, l6: 0.040055\n",
            "\n",
            "[epoch: 162/1000, batch:    10/   15, ite: 2425] train loss: 0.198409, tar: 0.021316 \n",
            "l0: 0.018880, l1: 0.019952, l2: 0.020944, l3: 0.021592, l4: 0.023854, l5: 0.028654, l6: 0.041247\n",
            "\n",
            "[epoch: 162/1000, batch:    11/   15, ite: 2426] train loss: 0.197513, tar: 0.021222 \n",
            "l0: 0.016135, l1: 0.015134, l2: 0.019358, l3: 0.020963, l4: 0.025636, l5: 0.030986, l6: 0.060569\n",
            "\n",
            "[epoch: 162/1000, batch:    12/   15, ite: 2427] train loss: 0.197190, tar: 0.021034 \n",
            "l0: 0.023827, l1: 0.027590, l2: 0.025711, l3: 0.026217, l4: 0.025953, l5: 0.025849, l6: 0.030869\n",
            "\n",
            "[epoch: 162/1000, batch:    13/   15, ite: 2428] train loss: 0.196791, tar: 0.021134 \n",
            "l0: 0.017946, l1: 0.017163, l2: 0.023863, l3: 0.025236, l4: 0.028968, l5: 0.033871, l6: 0.042705\n",
            "\n",
            "[epoch: 162/1000, batch:    14/   15, ite: 2429] train loss: 0.196548, tar: 0.021024 \n",
            "l0: 0.020641, l1: 0.020959, l2: 0.022556, l3: 0.026386, l4: 0.026053, l5: 0.027534, l6: 0.048255\n",
            "\n",
            "[epoch: 162/1000, batch:    15/   15, ite: 2430] train loss: 0.196409, tar: 0.021011 \n",
            "l0: 0.017749, l1: 0.018040, l2: 0.020357, l3: 0.019624, l4: 0.023281, l5: 0.028439, l6: 0.045601\n",
            "\n",
            "[epoch: 163/1000, batch:     1/   15, ite: 2431] train loss: 0.195657, tar: 0.020906 \n",
            "l0: 0.016384, l1: 0.015828, l2: 0.019119, l3: 0.023762, l4: 0.028462, l5: 0.034328, l6: 0.065336\n",
            "\n",
            "[epoch: 163/1000, batch:     2/   15, ite: 2432] train loss: 0.195893, tar: 0.020764 \n",
            "l0: 0.017643, l1: 0.020688, l2: 0.019669, l3: 0.022608, l4: 0.022938, l5: 0.024832, l6: 0.037917\n",
            "\n",
            "[epoch: 163/1000, batch:     3/   15, ite: 2433] train loss: 0.194996, tar: 0.020670 \n",
            "l0: 0.020844, l1: 0.019110, l2: 0.022871, l3: 0.026150, l4: 0.025829, l5: 0.029154, l6: 0.041381\n",
            "\n",
            "[epoch: 163/1000, batch:     4/   15, ite: 2434] train loss: 0.194712, tar: 0.020675 \n",
            "l0: 0.027533, l1: 0.029582, l2: 0.030462, l3: 0.033378, l4: 0.034062, l5: 0.034352, l6: 0.050120\n",
            "\n",
            "[epoch: 163/1000, batch:     5/   15, ite: 2435] train loss: 0.195991, tar: 0.020871 \n",
            "l0: 0.021746, l1: 0.021305, l2: 0.023889, l3: 0.025109, l4: 0.025858, l5: 0.032723, l6: 0.050979\n",
            "\n",
            "[epoch: 163/1000, batch:     6/   15, ite: 2436] train loss: 0.196147, tar: 0.020895 \n",
            "l0: 0.023451, l1: 0.028379, l2: 0.029973, l3: 0.029797, l4: 0.026272, l5: 0.033324, l6: 0.048383\n",
            "\n",
            "[epoch: 163/1000, batch:     7/   15, ite: 2437] train loss: 0.196781, tar: 0.020964 \n",
            "l0: 0.016754, l1: 0.017629, l2: 0.019359, l3: 0.020513, l4: 0.020586, l5: 0.026644, l6: 0.033567\n",
            "\n",
            "[epoch: 163/1000, batch:     8/   15, ite: 2438] train loss: 0.195683, tar: 0.020853 \n",
            "l0: 0.013559, l1: 0.013579, l2: 0.016485, l3: 0.017690, l4: 0.020848, l5: 0.028034, l6: 0.041656\n",
            "\n",
            "[epoch: 163/1000, batch:     9/   15, ite: 2439] train loss: 0.194559, tar: 0.020666 \n",
            "l0: 0.018515, l1: 0.016682, l2: 0.022632, l3: 0.025135, l4: 0.027791, l5: 0.033568, l6: 0.044995\n",
            "\n",
            "[epoch: 163/1000, batch:    10/   15, ite: 2440] train loss: 0.194428, tar: 0.020613 \n",
            "l0: 0.014701, l1: 0.014410, l2: 0.015362, l3: 0.018484, l4: 0.018336, l5: 0.024597, l6: 0.041711\n",
            "\n",
            "[epoch: 163/1000, batch:    11/   15, ite: 2441] train loss: 0.193286, tar: 0.020468 \n",
            "l0: 0.022948, l1: 0.023502, l2: 0.024789, l3: 0.025891, l4: 0.027716, l5: 0.034860, l6: 0.049459\n",
            "\n",
            "[epoch: 163/1000, batch:    12/   15, ite: 2442] train loss: 0.193664, tar: 0.020527 \n",
            "l0: 0.015361, l1: 0.015186, l2: 0.018254, l3: 0.019837, l4: 0.019137, l5: 0.020484, l6: 0.042754\n",
            "\n",
            "[epoch: 163/1000, batch:    13/   15, ite: 2443] train loss: 0.192672, tar: 0.020407 \n",
            "l0: 0.017908, l1: 0.016125, l2: 0.017800, l3: 0.020442, l4: 0.020245, l5: 0.027955, l6: 0.042445\n",
            "\n",
            "[epoch: 163/1000, batch:    14/   15, ite: 2444] train loss: 0.191996, tar: 0.020351 \n",
            "l0: 0.016129, l1: 0.015899, l2: 0.016265, l3: 0.018162, l4: 0.018184, l5: 0.025045, l6: 0.035747\n",
            "\n",
            "[epoch: 163/1000, batch:    15/   15, ite: 2445] train loss: 0.190961, tar: 0.020257 \n",
            "l0: 0.017628, l1: 0.017296, l2: 0.020559, l3: 0.021349, l4: 0.024660, l5: 0.026110, l6: 0.052489\n",
            "\n",
            "[epoch: 164/1000, batch:     1/   15, ite: 2446] train loss: 0.190725, tar: 0.020200 \n",
            "l0: 0.021674, l1: 0.022463, l2: 0.025365, l3: 0.026332, l4: 0.027386, l5: 0.029149, l6: 0.043825\n",
            "\n",
            "[epoch: 164/1000, batch:     2/   15, ite: 2447] train loss: 0.190841, tar: 0.020231 \n",
            "l0: 0.014510, l1: 0.013738, l2: 0.015608, l3: 0.018342, l4: 0.021469, l5: 0.024194, l6: 0.037112\n",
            "\n",
            "[epoch: 164/1000, batch:     3/   15, ite: 2448] train loss: 0.189885, tar: 0.020112 \n",
            "l0: 0.016001, l1: 0.019225, l2: 0.017992, l3: 0.020263, l4: 0.019881, l5: 0.019286, l6: 0.033202\n",
            "\n",
            "[epoch: 164/1000, batch:     4/   15, ite: 2449] train loss: 0.188987, tar: 0.020028 \n",
            "l0: 0.022607, l1: 0.022234, l2: 0.024365, l3: 0.026744, l4: 0.028674, l5: 0.033496, l6: 0.059231\n",
            "\n",
            "[epoch: 164/1000, batch:     5/   15, ite: 2450] train loss: 0.189554, tar: 0.020079 \n",
            "l0: 0.012744, l1: 0.013258, l2: 0.014013, l3: 0.014388, l4: 0.019106, l5: 0.022514, l6: 0.036402\n",
            "\n",
            "[epoch: 164/1000, batch:     6/   15, ite: 2451] train loss: 0.188434, tar: 0.019936 \n",
            "l0: 0.022410, l1: 0.022008, l2: 0.023676, l3: 0.025832, l4: 0.032342, l5: 0.032433, l6: 0.048326\n",
            "\n",
            "[epoch: 164/1000, batch:     7/   15, ite: 2452] train loss: 0.188791, tar: 0.019983 \n",
            "l0: 0.018704, l1: 0.017131, l2: 0.023318, l3: 0.024517, l4: 0.028059, l5: 0.031049, l6: 0.047368\n",
            "\n",
            "[epoch: 164/1000, batch:     8/   15, ite: 2453] train loss: 0.188817, tar: 0.019959 \n",
            "l0: 0.013208, l1: 0.013177, l2: 0.015569, l3: 0.018441, l4: 0.018385, l5: 0.022589, l6: 0.038558\n",
            "\n",
            "[epoch: 164/1000, batch:     9/   15, ite: 2454] train loss: 0.187912, tar: 0.019834 \n",
            "l0: 0.015234, l1: 0.016123, l2: 0.018506, l3: 0.017723, l4: 0.021157, l5: 0.024675, l6: 0.039340\n",
            "\n",
            "[epoch: 164/1000, batch:    10/   15, ite: 2455] train loss: 0.187273, tar: 0.019750 \n",
            "l0: 0.020867, l1: 0.021255, l2: 0.020832, l3: 0.020336, l4: 0.021710, l5: 0.035678, l6: 0.048778\n",
            "\n",
            "[epoch: 164/1000, batch:    11/   15, ite: 2456] train loss: 0.187312, tar: 0.019770 \n",
            "l0: 0.020938, l1: 0.021205, l2: 0.021460, l3: 0.024933, l4: 0.031654, l5: 0.036211, l6: 0.040429\n",
            "\n",
            "[epoch: 164/1000, batch:    12/   15, ite: 2457] train loss: 0.187479, tar: 0.019791 \n",
            "l0: 0.018609, l1: 0.020545, l2: 0.021354, l3: 0.023657, l4: 0.021481, l5: 0.026224, l6: 0.044382\n",
            "\n",
            "[epoch: 164/1000, batch:    13/   15, ite: 2458] train loss: 0.187285, tar: 0.019770 \n",
            "l0: 0.017159, l1: 0.018357, l2: 0.019492, l3: 0.022206, l4: 0.024703, l5: 0.027277, l6: 0.043836\n",
            "\n",
            "[epoch: 164/1000, batch:    14/   15, ite: 2459] train loss: 0.187043, tar: 0.019726 \n",
            "l0: 0.018762, l1: 0.017606, l2: 0.022228, l3: 0.026992, l4: 0.028212, l5: 0.033023, l6: 0.051988\n",
            "\n",
            "[epoch: 164/1000, batch:    15/   15, ite: 2460] train loss: 0.187239, tar: 0.019710 \n",
            "l0: 0.013678, l1: 0.012849, l2: 0.014881, l3: 0.019815, l4: 0.020791, l5: 0.027029, l6: 0.043991\n",
            "\n",
            "[epoch: 165/1000, batch:     1/   15, ite: 2461] train loss: 0.186679, tar: 0.019611 \n",
            "l0: 0.021219, l1: 0.022772, l2: 0.026068, l3: 0.026218, l4: 0.025402, l5: 0.030971, l6: 0.053967\n",
            "\n",
            "[epoch: 165/1000, batch:     2/   15, ite: 2462] train loss: 0.187000, tar: 0.019637 \n",
            "l0: 0.018069, l1: 0.018036, l2: 0.021055, l3: 0.022872, l4: 0.024569, l5: 0.027793, l6: 0.043301\n",
            "\n",
            "[epoch: 165/1000, batch:     3/   15, ite: 2463] train loss: 0.186821, tar: 0.019612 \n",
            "l0: 0.016010, l1: 0.015445, l2: 0.017649, l3: 0.020320, l4: 0.019963, l5: 0.026026, l6: 0.039591\n",
            "\n",
            "[epoch: 165/1000, batch:     4/   15, ite: 2464] train loss: 0.186324, tar: 0.019556 \n",
            "l0: 0.019906, l1: 0.018342, l2: 0.021561, l3: 0.023309, l4: 0.028582, l5: 0.031674, l6: 0.043320\n",
            "\n",
            "[epoch: 165/1000, batch:     5/   15, ite: 2465] train loss: 0.186329, tar: 0.019561 \n",
            "l0: 0.019232, l1: 0.019388, l2: 0.021516, l3: 0.023180, l4: 0.026614, l5: 0.032686, l6: 0.041734\n",
            "\n",
            "[epoch: 165/1000, batch:     6/   15, ite: 2466] train loss: 0.186299, tar: 0.019556 \n",
            "l0: 0.015436, l1: 0.013425, l2: 0.016322, l3: 0.017823, l4: 0.019774, l5: 0.029305, l6: 0.042772\n",
            "\n",
            "[epoch: 165/1000, batch:     7/   15, ite: 2467] train loss: 0.185830, tar: 0.019495 \n",
            "l0: 0.016056, l1: 0.016988, l2: 0.018327, l3: 0.019195, l4: 0.020211, l5: 0.024303, l6: 0.038367\n",
            "\n",
            "[epoch: 165/1000, batch:     8/   15, ite: 2468] train loss: 0.185354, tar: 0.019444 \n",
            "l0: 0.025652, l1: 0.027736, l2: 0.029411, l3: 0.029951, l4: 0.032368, l5: 0.037169, l6: 0.060773\n",
            "\n",
            "[epoch: 165/1000, batch:     9/   15, ite: 2469] train loss: 0.186190, tar: 0.019534 \n",
            "l0: 0.015696, l1: 0.014098, l2: 0.016707, l3: 0.020276, l4: 0.021595, l5: 0.025036, l6: 0.041503\n",
            "\n",
            "[epoch: 165/1000, batch:    10/   15, ite: 2470] train loss: 0.185743, tar: 0.019479 \n",
            "l0: 0.016193, l1: 0.014450, l2: 0.018992, l3: 0.022245, l4: 0.021537, l5: 0.021269, l6: 0.031464\n",
            "\n",
            "[epoch: 165/1000, batch:    11/   15, ite: 2471] train loss: 0.185186, tar: 0.019433 \n",
            "l0: 0.019970, l1: 0.020372, l2: 0.021288, l3: 0.025171, l4: 0.022187, l5: 0.024322, l6: 0.037281\n",
            "\n",
            "[epoch: 165/1000, batch:    12/   15, ite: 2472] train loss: 0.184983, tar: 0.019441 \n",
            "l0: 0.016165, l1: 0.016502, l2: 0.019994, l3: 0.020829, l4: 0.021526, l5: 0.025165, l6: 0.040179\n",
            "\n",
            "[epoch: 165/1000, batch:    13/   15, ite: 2473] train loss: 0.184646, tar: 0.019396 \n",
            "l0: 0.014190, l1: 0.015124, l2: 0.015330, l3: 0.017273, l4: 0.017457, l5: 0.022189, l6: 0.039556\n",
            "\n",
            "[epoch: 165/1000, batch:    14/   15, ite: 2474] train loss: 0.184058, tar: 0.019325 \n",
            "l0: 0.015380, l1: 0.014593, l2: 0.017999, l3: 0.022154, l4: 0.029211, l5: 0.036866, l6: 0.043339\n",
            "\n",
            "[epoch: 165/1000, batch:    15/   15, ite: 2475] train loss: 0.183997, tar: 0.019273 \n",
            "l0: 0.016450, l1: 0.016406, l2: 0.019550, l3: 0.020392, l4: 0.023057, l5: 0.026369, l6: 0.034442\n",
            "\n",
            "[epoch: 166/1000, batch:     1/   15, ite: 2476] train loss: 0.183638, tar: 0.019236 \n",
            "l0: 0.011256, l1: 0.010710, l2: 0.013629, l3: 0.016785, l4: 0.019221, l5: 0.024182, l6: 0.037747\n",
            "\n",
            "[epoch: 166/1000, batch:     2/   15, ite: 2477] train loss: 0.182987, tar: 0.019132 \n",
            "l0: 0.013874, l1: 0.013847, l2: 0.015402, l3: 0.016787, l4: 0.019404, l5: 0.025838, l6: 0.035567\n",
            "\n",
            "[epoch: 166/1000, batch:     3/   15, ite: 2478] train loss: 0.182445, tar: 0.019065 \n",
            "l0: 0.011699, l1: 0.011508, l2: 0.012376, l3: 0.014670, l4: 0.017484, l5: 0.022527, l6: 0.040579\n",
            "\n",
            "[epoch: 166/1000, batch:     4/   15, ite: 2479] train loss: 0.181792, tar: 0.018971 \n",
            "l0: 0.012245, l1: 0.011920, l2: 0.013852, l3: 0.016083, l4: 0.018324, l5: 0.024033, l6: 0.037870\n",
            "\n",
            "[epoch: 166/1000, batch:     5/   15, ite: 2480] train loss: 0.181199, tar: 0.018887 \n",
            "l0: 0.023186, l1: 0.021430, l2: 0.024687, l3: 0.026845, l4: 0.027673, l5: 0.032824, l6: 0.057971\n",
            "\n",
            "[epoch: 166/1000, batch:     6/   15, ite: 2481] train loss: 0.181611, tar: 0.018940 \n",
            "l0: 0.013701, l1: 0.013064, l2: 0.015663, l3: 0.015855, l4: 0.018133, l5: 0.022845, l6: 0.044493\n",
            "\n",
            "[epoch: 166/1000, batch:     7/   15, ite: 2482] train loss: 0.181149, tar: 0.018876 \n",
            "l0: 0.017262, l1: 0.018133, l2: 0.019531, l3: 0.021679, l4: 0.022889, l5: 0.026992, l6: 0.036300\n",
            "\n",
            "[epoch: 166/1000, batch:     8/   15, ite: 2483] train loss: 0.180928, tar: 0.018857 \n",
            "l0: 0.019650, l1: 0.018705, l2: 0.020083, l3: 0.022304, l4: 0.026460, l5: 0.037087, l6: 0.064445\n",
            "\n",
            "[epoch: 166/1000, batch:     9/   15, ite: 2484] train loss: 0.181259, tar: 0.018866 \n",
            "l0: 0.015301, l1: 0.015043, l2: 0.017636, l3: 0.018355, l4: 0.019991, l5: 0.026888, l6: 0.044589\n",
            "\n",
            "[epoch: 166/1000, batch:    10/   15, ite: 2485] train loss: 0.180983, tar: 0.018825 \n",
            "l0: 0.019127, l1: 0.020246, l2: 0.023508, l3: 0.022822, l4: 0.022848, l5: 0.024362, l6: 0.035986\n",
            "\n",
            "[epoch: 166/1000, batch:    11/   15, ite: 2486] train loss: 0.180843, tar: 0.018828 \n",
            "l0: 0.015167, l1: 0.013630, l2: 0.017177, l3: 0.020178, l4: 0.022028, l5: 0.026700, l6: 0.038087\n",
            "\n",
            "[epoch: 166/1000, batch:    12/   15, ite: 2487] train loss: 0.180522, tar: 0.018786 \n",
            "l0: 0.016293, l1: 0.015174, l2: 0.019227, l3: 0.023711, l4: 0.021285, l5: 0.025328, l6: 0.037900\n",
            "\n",
            "[epoch: 166/1000, batch:    13/   15, ite: 2488] train loss: 0.180277, tar: 0.018758 \n",
            "l0: 0.015670, l1: 0.015472, l2: 0.017542, l3: 0.018752, l4: 0.020703, l5: 0.029885, l6: 0.038640\n",
            "\n",
            "[epoch: 166/1000, batch:    14/   15, ite: 2489] train loss: 0.180012, tar: 0.018723 \n",
            "l0: 0.013128, l1: 0.013365, l2: 0.013973, l3: 0.016098, l4: 0.018643, l5: 0.026399, l6: 0.040361\n",
            "\n",
            "[epoch: 166/1000, batch:    15/   15, ite: 2490] train loss: 0.179589, tar: 0.018661 \n",
            "l0: 0.015656, l1: 0.015034, l2: 0.016439, l3: 0.017873, l4: 0.019817, l5: 0.027984, l6: 0.037684\n",
            "\n",
            "[epoch: 167/1000, batch:     1/   15, ite: 2491] train loss: 0.179269, tar: 0.018628 \n",
            "l0: 0.016765, l1: 0.014663, l2: 0.019452, l3: 0.020928, l4: 0.024414, l5: 0.031488, l6: 0.063945\n",
            "\n",
            "[epoch: 167/1000, batch:     2/   15, ite: 2492] train loss: 0.179404, tar: 0.018608 \n",
            "l0: 0.011239, l1: 0.011472, l2: 0.012809, l3: 0.014825, l4: 0.015756, l5: 0.020150, l6: 0.031571\n",
            "\n",
            "[epoch: 167/1000, batch:     3/   15, ite: 2493] train loss: 0.178742, tar: 0.018528 \n",
            "l0: 0.014820, l1: 0.015129, l2: 0.015318, l3: 0.016707, l4: 0.020739, l5: 0.023710, l6: 0.039490\n",
            "\n",
            "[epoch: 167/1000, batch:     4/   15, ite: 2494] train loss: 0.178392, tar: 0.018489 \n",
            "l0: 0.021047, l1: 0.024384, l2: 0.021721, l3: 0.022005, l4: 0.024677, l5: 0.025347, l6: 0.035948\n",
            "\n",
            "[epoch: 167/1000, batch:     5/   15, ite: 2495] train loss: 0.178358, tar: 0.018516 \n",
            "l0: 0.012181, l1: 0.011083, l2: 0.012736, l3: 0.013615, l4: 0.019752, l5: 0.026972, l6: 0.039839\n",
            "\n",
            "[epoch: 167/1000, batch:     6/   15, ite: 2496] train loss: 0.177919, tar: 0.018450 \n",
            "l0: 0.014777, l1: 0.013982, l2: 0.017166, l3: 0.018869, l4: 0.020993, l5: 0.025239, l6: 0.037821\n",
            "\n",
            "[epoch: 167/1000, batch:     7/   15, ite: 2497] train loss: 0.177619, tar: 0.018412 \n",
            "l0: 0.023900, l1: 0.022068, l2: 0.023716, l3: 0.030175, l4: 0.031571, l5: 0.038112, l6: 0.053024\n",
            "\n",
            "[epoch: 167/1000, batch:     8/   15, ite: 2498] train loss: 0.178078, tar: 0.018468 \n",
            "l0: 0.026713, l1: 0.023491, l2: 0.030245, l3: 0.031892, l4: 0.032402, l5: 0.033453, l6: 0.047761\n",
            "\n",
            "[epoch: 167/1000, batch:     9/   15, ite: 2499] train loss: 0.178561, tar: 0.018551 \n",
            "l0: 0.022332, l1: 0.023487, l2: 0.021918, l3: 0.023793, l4: 0.025191, l5: 0.036183, l6: 0.059369\n",
            "\n",
            "[epoch: 167/1000, batch:    10/   15, ite: 2500] train loss: 0.178898, tar: 0.018589 \n",
            "l0: 0.012316, l1: 0.013174, l2: 0.012843, l3: 0.014603, l4: 0.015735, l5: 0.017975, l6: 0.024034\n",
            "\n",
            "[epoch: 167/1000, batch:    11/   15, ite: 2501] train loss: 0.110680, tar: 0.012316 \n",
            "l0: 0.011710, l1: 0.012068, l2: 0.013101, l3: 0.016139, l4: 0.019487, l5: 0.025202, l6: 0.034710\n",
            "\n",
            "[epoch: 167/1000, batch:    12/   15, ite: 2502] train loss: 0.121549, tar: 0.012013 \n",
            "l0: 0.011788, l1: 0.011745, l2: 0.013115, l3: 0.015939, l4: 0.017969, l5: 0.022169, l6: 0.045050\n",
            "\n",
            "[epoch: 167/1000, batch:    13/   15, ite: 2503] train loss: 0.126957, tar: 0.011938 \n",
            "l0: 0.013732, l1: 0.014023, l2: 0.015615, l3: 0.020046, l4: 0.023586, l5: 0.024219, l6: 0.038849\n",
            "\n",
            "[epoch: 167/1000, batch:    14/   15, ite: 2504] train loss: 0.132735, tar: 0.012386 \n",
            "l0: 0.021093, l1: 0.022394, l2: 0.022535, l3: 0.024463, l4: 0.025832, l5: 0.029155, l6: 0.043491\n",
            "\n",
            "[epoch: 167/1000, batch:    15/   15, ite: 2505] train loss: 0.143981, tar: 0.014128 \n",
            "l0: 0.023643, l1: 0.027423, l2: 0.024759, l3: 0.025764, l4: 0.030061, l5: 0.033132, l6: 0.054074\n",
            "\n",
            "[epoch: 168/1000, batch:     1/   15, ite: 2506] train loss: 0.156460, tar: 0.015714 \n",
            "l0: 0.016690, l1: 0.015916, l2: 0.017343, l3: 0.021160, l4: 0.021034, l5: 0.027908, l6: 0.055231\n",
            "\n",
            "[epoch: 168/1000, batch:     2/   15, ite: 2507] train loss: 0.159149, tar: 0.015853 \n",
            "l0: 0.013882, l1: 0.012679, l2: 0.015909, l3: 0.019510, l4: 0.021347, l5: 0.023533, l6: 0.034004\n",
            "\n",
            "[epoch: 168/1000, batch:     3/   15, ite: 2508] train loss: 0.156863, tar: 0.015607 \n",
            "l0: 0.019299, l1: 0.018399, l2: 0.021210, l3: 0.022394, l4: 0.025045, l5: 0.031271, l6: 0.044755\n",
            "\n",
            "[epoch: 168/1000, batch:     4/   15, ite: 2509] train loss: 0.159698, tar: 0.016017 \n",
            "l0: 0.022202, l1: 0.021825, l2: 0.025421, l3: 0.027314, l4: 0.026511, l5: 0.029728, l6: 0.055034\n",
            "\n",
            "[epoch: 168/1000, batch:     5/   15, ite: 2510] train loss: 0.164531, tar: 0.016635 \n",
            "l0: 0.012650, l1: 0.012818, l2: 0.013671, l3: 0.014125, l4: 0.016032, l5: 0.019389, l6: 0.030872\n",
            "\n",
            "[epoch: 168/1000, batch:     6/   15, ite: 2511] train loss: 0.160443, tar: 0.016273 \n",
            "l0: 0.016610, l1: 0.016805, l2: 0.017127, l3: 0.020092, l4: 0.024116, l5: 0.029784, l6: 0.049914\n",
            "\n",
            "[epoch: 168/1000, batch:     7/   15, ite: 2512] train loss: 0.161610, tar: 0.016301 \n",
            "l0: 0.025214, l1: 0.024502, l2: 0.024842, l3: 0.024369, l4: 0.031799, l5: 0.038456, l6: 0.052155\n",
            "\n",
            "[epoch: 168/1000, batch:     8/   15, ite: 2513] train loss: 0.166204, tar: 0.016987 \n",
            "l0: 0.017484, l1: 0.017711, l2: 0.017313, l3: 0.020106, l4: 0.022147, l5: 0.025707, l6: 0.043505\n",
            "\n",
            "[epoch: 168/1000, batch:     9/   15, ite: 2514] train loss: 0.166045, tar: 0.017022 \n",
            "l0: 0.030962, l1: 0.034956, l2: 0.033729, l3: 0.033232, l4: 0.037099, l5: 0.033912, l6: 0.037175\n",
            "\n",
            "[epoch: 168/1000, batch:    10/   15, ite: 2515] train loss: 0.171046, tar: 0.017952 \n",
            "l0: 0.016957, l1: 0.017254, l2: 0.018537, l3: 0.020023, l4: 0.021992, l5: 0.024732, l6: 0.049623\n",
            "\n",
            "[epoch: 168/1000, batch:    11/   15, ite: 2516] train loss: 0.170926, tar: 0.017889 \n",
            "l0: 0.014230, l1: 0.014010, l2: 0.015811, l3: 0.017613, l4: 0.021501, l5: 0.025372, l6: 0.036284\n",
            "\n",
            "[epoch: 168/1000, batch:    12/   15, ite: 2517] train loss: 0.169390, tar: 0.017674 \n",
            "l0: 0.014562, l1: 0.015151, l2: 0.017241, l3: 0.017371, l4: 0.018487, l5: 0.022490, l6: 0.041894\n",
            "\n",
            "[epoch: 168/1000, batch:    13/   15, ite: 2518] train loss: 0.168157, tar: 0.017501 \n",
            "l0: 0.017953, l1: 0.018650, l2: 0.019769, l3: 0.023022, l4: 0.023394, l5: 0.028053, l6: 0.043804\n",
            "\n",
            "[epoch: 168/1000, batch:    14/   15, ite: 2519] train loss: 0.168499, tar: 0.017525 \n",
            "l0: 0.013841, l1: 0.013324, l2: 0.016495, l3: 0.018033, l4: 0.020685, l5: 0.025768, l6: 0.046424\n",
            "\n",
            "[epoch: 168/1000, batch:    15/   15, ite: 2520] train loss: 0.167802, tar: 0.017341 \n",
            "l0: 0.016296, l1: 0.017237, l2: 0.019498, l3: 0.023417, l4: 0.024352, l5: 0.026421, l6: 0.057292\n",
            "\n",
            "[epoch: 169/1000, batch:     1/   15, ite: 2521] train loss: 0.168598, tar: 0.017291 \n",
            "l0: 0.026657, l1: 0.026479, l2: 0.031961, l3: 0.032051, l4: 0.032365, l5: 0.036981, l6: 0.053802\n",
            "\n",
            "[epoch: 169/1000, batch:     2/   15, ite: 2522] train loss: 0.171857, tar: 0.017717 \n",
            "l0: 0.011796, l1: 0.011916, l2: 0.013336, l3: 0.014919, l4: 0.017629, l5: 0.019798, l6: 0.028798\n",
            "\n",
            "[epoch: 169/1000, batch:     3/   15, ite: 2523] train loss: 0.169524, tar: 0.017459 \n",
            "l0: 0.030894, l1: 0.027052, l2: 0.036957, l3: 0.036775, l4: 0.035992, l5: 0.042155, l6: 0.061933\n",
            "\n",
            "[epoch: 169/1000, batch:     4/   15, ite: 2524] train loss: 0.173784, tar: 0.018019 \n",
            "l0: 0.018822, l1: 0.016468, l2: 0.019609, l3: 0.024301, l4: 0.026952, l5: 0.032375, l6: 0.054834\n",
            "\n",
            "[epoch: 169/1000, batch:     5/   15, ite: 2525] train loss: 0.174567, tar: 0.018051 \n",
            "l0: 0.015633, l1: 0.014234, l2: 0.018238, l3: 0.020102, l4: 0.021743, l5: 0.025319, l6: 0.039855\n",
            "\n",
            "[epoch: 169/1000, batch:     6/   15, ite: 2526] train loss: 0.173819, tar: 0.017958 \n",
            "l0: 0.017218, l1: 0.016951, l2: 0.019137, l3: 0.020770, l4: 0.022519, l5: 0.025139, l6: 0.033294\n",
            "\n",
            "[epoch: 169/1000, batch:     7/   15, ite: 2527] train loss: 0.173123, tar: 0.017931 \n",
            "l0: 0.018391, l1: 0.016640, l2: 0.021212, l3: 0.023981, l4: 0.025209, l5: 0.030657, l6: 0.052310\n",
            "\n",
            "[epoch: 169/1000, batch:     8/   15, ite: 2528] train loss: 0.173669, tar: 0.017947 \n",
            "l0: 0.013908, l1: 0.012125, l2: 0.014413, l3: 0.016123, l4: 0.020239, l5: 0.026904, l6: 0.036875\n",
            "\n",
            "[epoch: 169/1000, batch:     9/   15, ite: 2529] train loss: 0.172528, tar: 0.017808 \n",
            "l0: 0.015288, l1: 0.016687, l2: 0.015800, l3: 0.019504, l4: 0.021097, l5: 0.024566, l6: 0.040880\n",
            "\n",
            "[epoch: 169/1000, batch:    10/   15, ite: 2530] train loss: 0.171904, tar: 0.017724 \n",
            "l0: 0.021307, l1: 0.019435, l2: 0.021413, l3: 0.025149, l4: 0.028418, l5: 0.032658, l6: 0.042026\n",
            "\n",
            "[epoch: 169/1000, batch:    11/   15, ite: 2531] train loss: 0.172501, tar: 0.017840 \n",
            "l0: 0.016898, l1: 0.016715, l2: 0.020677, l3: 0.021674, l4: 0.025514, l5: 0.031297, l6: 0.046606\n",
            "\n",
            "[epoch: 169/1000, batch:    12/   15, ite: 2532] train loss: 0.172716, tar: 0.017810 \n",
            "l0: 0.024908, l1: 0.025762, l2: 0.028496, l3: 0.027911, l4: 0.028126, l5: 0.035447, l6: 0.050659\n",
            "\n",
            "[epoch: 169/1000, batch:    13/   15, ite: 2533] train loss: 0.174189, tar: 0.018025 \n",
            "l0: 0.028865, l1: 0.030257, l2: 0.032197, l3: 0.032330, l4: 0.038633, l5: 0.037995, l6: 0.060396\n",
            "\n",
            "[epoch: 169/1000, batch:    14/   15, ite: 2534] train loss: 0.176732, tar: 0.018344 \n",
            "l0: 0.014473, l1: 0.014732, l2: 0.015087, l3: 0.017305, l4: 0.021693, l5: 0.027176, l6: 0.035993\n",
            "\n",
            "[epoch: 169/1000, batch:    15/   15, ite: 2535] train loss: 0.175867, tar: 0.018234 \n",
            "l0: 0.024091, l1: 0.027613, l2: 0.024906, l3: 0.025016, l4: 0.026608, l5: 0.026111, l6: 0.035692\n",
            "\n",
            "[epoch: 170/1000, batch:     1/   15, ite: 2536] train loss: 0.176261, tar: 0.018396 \n",
            "l0: 0.015779, l1: 0.015527, l2: 0.015910, l3: 0.018203, l4: 0.024033, l5: 0.030340, l6: 0.038327\n",
            "\n",
            "[epoch: 170/1000, batch:     2/   15, ite: 2537] train loss: 0.175771, tar: 0.018326 \n",
            "l0: 0.013596, l1: 0.012698, l2: 0.015712, l3: 0.017897, l4: 0.019527, l5: 0.023484, l6: 0.040313\n",
            "\n",
            "[epoch: 170/1000, batch:     3/   15, ite: 2538] train loss: 0.174914, tar: 0.018201 \n",
            "l0: 0.019354, l1: 0.018334, l2: 0.022158, l3: 0.023126, l4: 0.029555, l5: 0.034987, l6: 0.047561\n",
            "\n",
            "[epoch: 170/1000, batch:     4/   15, ite: 2539] train loss: 0.175431, tar: 0.018231 \n",
            "l0: 0.019259, l1: 0.015597, l2: 0.021812, l3: 0.025443, l4: 0.030629, l5: 0.038162, l6: 0.072179\n",
            "\n",
            "[epoch: 170/1000, batch:     5/   15, ite: 2540] train loss: 0.176622, tar: 0.018256 \n",
            "l0: 0.016519, l1: 0.016362, l2: 0.018837, l3: 0.021395, l4: 0.022166, l5: 0.025542, l6: 0.036088\n",
            "\n",
            "[epoch: 170/1000, batch:     6/   15, ite: 2541] train loss: 0.176142, tar: 0.018214 \n",
            "l0: 0.020415, l1: 0.020124, l2: 0.021300, l3: 0.024755, l4: 0.030683, l5: 0.037099, l6: 0.047890\n",
            "\n",
            "[epoch: 170/1000, batch:     7/   15, ite: 2542] train loss: 0.176764, tar: 0.018266 \n",
            "l0: 0.014942, l1: 0.014094, l2: 0.017650, l3: 0.020016, l4: 0.022476, l5: 0.028249, l6: 0.045650\n",
            "\n",
            "[epoch: 170/1000, batch:     8/   15, ite: 2543] train loss: 0.176445, tar: 0.018189 \n",
            "l0: 0.022323, l1: 0.022163, l2: 0.027303, l3: 0.027646, l4: 0.030763, l5: 0.029445, l6: 0.044638\n",
            "\n",
            "[epoch: 170/1000, batch:     9/   15, ite: 2544] train loss: 0.177078, tar: 0.018283 \n",
            "l0: 0.011447, l1: 0.010933, l2: 0.011364, l3: 0.013049, l4: 0.016456, l5: 0.023886, l6: 0.041519\n",
            "\n",
            "[epoch: 170/1000, batch:    10/   15, ite: 2545] train loss: 0.176002, tar: 0.018131 \n",
            "l0: 0.019495, l1: 0.018722, l2: 0.020627, l3: 0.022239, l4: 0.025847, l5: 0.033963, l6: 0.056071\n",
            "\n",
            "[epoch: 170/1000, batch:    11/   15, ite: 2546] train loss: 0.176458, tar: 0.018161 \n",
            "l0: 0.014943, l1: 0.013970, l2: 0.017195, l3: 0.020561, l4: 0.024972, l5: 0.032115, l6: 0.048424\n",
            "\n",
            "[epoch: 170/1000, batch:    12/   15, ite: 2547] train loss: 0.176367, tar: 0.018092 \n",
            "l0: 0.011088, l1: 0.011278, l2: 0.012272, l3: 0.013762, l4: 0.016429, l5: 0.020738, l6: 0.035550\n",
            "\n",
            "[epoch: 170/1000, batch:    13/   15, ite: 2548] train loss: 0.175215, tar: 0.017946 \n",
            "l0: 0.011255, l1: 0.011088, l2: 0.013149, l3: 0.016836, l4: 0.016414, l5: 0.021623, l6: 0.043423\n",
            "\n",
            "[epoch: 170/1000, batch:    14/   15, ite: 2549] train loss: 0.174370, tar: 0.017810 \n",
            "l0: 0.037805, l1: 0.037306, l2: 0.037564, l3: 0.044057, l4: 0.051486, l5: 0.050066, l6: 0.055168\n",
            "\n",
            "[epoch: 170/1000, batch:    15/   15, ite: 2550] train loss: 0.177152, tar: 0.018210 \n",
            "l0: 0.013956, l1: 0.014456, l2: 0.016587, l3: 0.018251, l4: 0.018720, l5: 0.022820, l6: 0.039791\n",
            "\n",
            "[epoch: 171/1000, batch:     1/   15, ite: 2551] train loss: 0.176513, tar: 0.018126 \n",
            "l0: 0.022188, l1: 0.022779, l2: 0.023482, l3: 0.025659, l4: 0.030446, l5: 0.030533, l6: 0.048367\n",
            "\n",
            "[epoch: 171/1000, batch:     2/   15, ite: 2552] train loss: 0.177031, tar: 0.018204 \n",
            "l0: 0.019432, l1: 0.022354, l2: 0.021632, l3: 0.023716, l4: 0.022391, l5: 0.027250, l6: 0.041620\n",
            "\n",
            "[epoch: 171/1000, batch:     3/   15, ite: 2553] train loss: 0.177057, tar: 0.018228 \n",
            "l0: 0.012979, l1: 0.013978, l2: 0.014927, l3: 0.016293, l4: 0.014819, l5: 0.017588, l6: 0.027063\n",
            "\n",
            "[epoch: 171/1000, batch:     4/   15, ite: 2554] train loss: 0.175957, tar: 0.018130 \n",
            "l0: 0.016944, l1: 0.016403, l2: 0.018929, l3: 0.021301, l4: 0.025307, l5: 0.032160, l6: 0.040407\n",
            "\n",
            "[epoch: 171/1000, batch:     5/   15, ite: 2555] train loss: 0.175875, tar: 0.018109 \n",
            "l0: 0.016800, l1: 0.016687, l2: 0.021042, l3: 0.022877, l4: 0.024517, l5: 0.024239, l6: 0.042267\n",
            "\n",
            "[epoch: 171/1000, batch:     6/   15, ite: 2556] train loss: 0.175742, tar: 0.018085 \n",
            "l0: 0.023724, l1: 0.024969, l2: 0.025815, l3: 0.027161, l4: 0.032670, l5: 0.036419, l6: 0.061408\n",
            "\n",
            "[epoch: 171/1000, batch:     7/   15, ite: 2557] train loss: 0.176732, tar: 0.018184 \n",
            "l0: 0.021139, l1: 0.023910, l2: 0.021676, l3: 0.022067, l4: 0.025944, l5: 0.024200, l6: 0.031051\n",
            "\n",
            "[epoch: 171/1000, batch:     8/   15, ite: 2558] train loss: 0.176615, tar: 0.018235 \n",
            "l0: 0.015817, l1: 0.015510, l2: 0.016307, l3: 0.019524, l4: 0.022905, l5: 0.025152, l6: 0.038604\n",
            "\n",
            "[epoch: 171/1000, batch:     9/   15, ite: 2559] train loss: 0.176229, tar: 0.018194 \n",
            "l0: 0.013063, l1: 0.011761, l2: 0.014190, l3: 0.015926, l4: 0.019918, l5: 0.025114, l6: 0.044106\n",
            "\n",
            "[epoch: 171/1000, batch:    10/   15, ite: 2560] train loss: 0.175693, tar: 0.018109 \n",
            "l0: 0.019352, l1: 0.016431, l2: 0.022066, l3: 0.024309, l4: 0.027382, l5: 0.029965, l6: 0.046659\n",
            "\n",
            "[epoch: 171/1000, batch:    11/   15, ite: 2561] train loss: 0.175865, tar: 0.018129 \n",
            "l0: 0.027524, l1: 0.027344, l2: 0.029782, l3: 0.029713, l4: 0.032391, l5: 0.033609, l6: 0.044897\n",
            "\n",
            "[epoch: 171/1000, batch:    12/   15, ite: 2562] train loss: 0.176662, tar: 0.018281 \n",
            "l0: 0.022042, l1: 0.022106, l2: 0.023754, l3: 0.024096, l4: 0.028880, l5: 0.034501, l6: 0.062168\n",
            "\n",
            "[epoch: 171/1000, batch:    13/   15, ite: 2563] train loss: 0.177311, tar: 0.018340 \n",
            "l0: 0.017078, l1: 0.015663, l2: 0.019360, l3: 0.022629, l4: 0.026363, l5: 0.030510, l6: 0.043721\n",
            "\n",
            "[epoch: 171/1000, batch:    14/   15, ite: 2564] train loss: 0.177280, tar: 0.018321 \n",
            "l0: 0.016413, l1: 0.017030, l2: 0.019769, l3: 0.021466, l4: 0.022949, l5: 0.024529, l6: 0.055476\n",
            "\n",
            "[epoch: 171/1000, batch:    15/   15, ite: 2565] train loss: 0.177285, tar: 0.018291 \n",
            "l0: 0.020033, l1: 0.024768, l2: 0.021801, l3: 0.020494, l4: 0.024746, l5: 0.026610, l6: 0.050940\n",
            "\n",
            "[epoch: 172/1000, batch:     1/   15, ite: 2566] train loss: 0.177468, tar: 0.018318 \n",
            "l0: 0.015237, l1: 0.014581, l2: 0.018694, l3: 0.019976, l4: 0.021960, l5: 0.025049, l6: 0.031199\n",
            "\n",
            "[epoch: 172/1000, batch:     2/   15, ite: 2567] train loss: 0.177009, tar: 0.018272 \n",
            "l0: 0.026803, l1: 0.031159, l2: 0.029701, l3: 0.027907, l4: 0.031277, l5: 0.030087, l6: 0.034240\n",
            "\n",
            "[epoch: 172/1000, batch:     3/   15, ite: 2568] train loss: 0.177512, tar: 0.018397 \n",
            "l0: 0.019770, l1: 0.019121, l2: 0.023878, l3: 0.028076, l4: 0.029172, l5: 0.031103, l6: 0.047672\n",
            "\n",
            "[epoch: 172/1000, batch:     4/   15, ite: 2569] train loss: 0.177820, tar: 0.018417 \n",
            "l0: 0.016344, l1: 0.015794, l2: 0.020377, l3: 0.022054, l4: 0.024161, l5: 0.027144, l6: 0.044521\n",
            "\n",
            "[epoch: 172/1000, batch:     5/   15, ite: 2570] train loss: 0.177714, tar: 0.018388 \n",
            "l0: 0.023996, l1: 0.023668, l2: 0.028221, l3: 0.030794, l4: 0.030281, l5: 0.032225, l6: 0.054882\n",
            "\n",
            "[epoch: 172/1000, batch:     6/   15, ite: 2571] train loss: 0.178367, tar: 0.018467 \n",
            "l0: 0.020269, l1: 0.021313, l2: 0.025804, l3: 0.025241, l4: 0.024631, l5: 0.027474, l6: 0.037442\n",
            "\n",
            "[epoch: 172/1000, batch:     7/   15, ite: 2572] train loss: 0.178420, tar: 0.018492 \n",
            "l0: 0.016054, l1: 0.013899, l2: 0.016008, l3: 0.020918, l4: 0.026640, l5: 0.033448, l6: 0.051545\n",
            "\n",
            "[epoch: 172/1000, batch:     8/   15, ite: 2573] train loss: 0.178421, tar: 0.018458 \n",
            "l0: 0.023620, l1: 0.022960, l2: 0.024357, l3: 0.028464, l4: 0.031909, l5: 0.033247, l6: 0.059513\n",
            "\n",
            "[epoch: 172/1000, batch:     9/   15, ite: 2574] train loss: 0.179038, tar: 0.018528 \n",
            "l0: 0.019643, l1: 0.018869, l2: 0.019772, l3: 0.023079, l4: 0.024757, l5: 0.032178, l6: 0.054536\n",
            "\n",
            "[epoch: 172/1000, batch:    10/   15, ite: 2575] train loss: 0.179222, tar: 0.018543 \n",
            "l0: 0.013033, l1: 0.012861, l2: 0.013837, l3: 0.015028, l4: 0.018283, l5: 0.020221, l6: 0.033713\n",
            "\n",
            "[epoch: 172/1000, batch:    11/   15, ite: 2576] train loss: 0.178534, tar: 0.018470 \n",
            "l0: 0.016914, l1: 0.016055, l2: 0.016852, l3: 0.019284, l4: 0.027483, l5: 0.033238, l6: 0.048681\n",
            "\n",
            "[epoch: 172/1000, batch:    12/   15, ite: 2577] train loss: 0.178534, tar: 0.018450 \n",
            "l0: 0.014956, l1: 0.014053, l2: 0.017789, l3: 0.022260, l4: 0.020927, l5: 0.025080, l6: 0.040231\n",
            "\n",
            "[epoch: 172/1000, batch:    13/   15, ite: 2578] train loss: 0.178236, tar: 0.018405 \n",
            "l0: 0.017149, l1: 0.016163, l2: 0.018996, l3: 0.020262, l4: 0.022951, l5: 0.026147, l6: 0.037398\n",
            "\n",
            "[epoch: 172/1000, batch:    14/   15, ite: 2579] train loss: 0.177993, tar: 0.018389 \n",
            "l0: 0.012885, l1: 0.011904, l2: 0.014146, l3: 0.016399, l4: 0.019350, l5: 0.024581, l6: 0.040015\n",
            "\n",
            "[epoch: 172/1000, batch:    15/   15, ite: 2580] train loss: 0.177509, tar: 0.018321 \n",
            "l0: 0.021018, l1: 0.021516, l2: 0.021397, l3: 0.024368, l4: 0.022298, l5: 0.025600, l6: 0.040371\n",
            "\n",
            "[epoch: 173/1000, batch:     1/   15, ite: 2581] train loss: 0.177498, tar: 0.018354 \n",
            "l0: 0.015978, l1: 0.015800, l2: 0.018297, l3: 0.020121, l4: 0.022138, l5: 0.029103, l6: 0.049528\n",
            "\n",
            "[epoch: 173/1000, batch:     2/   15, ite: 2582] train loss: 0.177418, tar: 0.018325 \n",
            "l0: 0.037426, l1: 0.041852, l2: 0.035860, l3: 0.037442, l4: 0.030295, l5: 0.031761, l6: 0.044582\n",
            "\n",
            "[epoch: 173/1000, batch:     3/   15, ite: 2583] train loss: 0.178404, tar: 0.018555 \n",
            "l0: 0.019130, l1: 0.018581, l2: 0.021222, l3: 0.024056, l4: 0.025048, l5: 0.029404, l6: 0.044184\n",
            "\n",
            "[epoch: 173/1000, batch:     4/   15, ite: 2584] train loss: 0.178442, tar: 0.018562 \n",
            "l0: 0.026662, l1: 0.026743, l2: 0.028294, l3: 0.031258, l4: 0.036451, l5: 0.044106, l6: 0.055655\n",
            "\n",
            "[epoch: 173/1000, batch:     5/   15, ite: 2585] train loss: 0.179274, tar: 0.018657 \n",
            "l0: 0.014662, l1: 0.014542, l2: 0.015861, l3: 0.019859, l4: 0.020429, l5: 0.028866, l6: 0.042981\n",
            "\n",
            "[epoch: 173/1000, batch:     6/   15, ite: 2586] train loss: 0.179017, tar: 0.018611 \n",
            "l0: 0.015144, l1: 0.014799, l2: 0.017226, l3: 0.021773, l4: 0.022420, l5: 0.027207, l6: 0.048736\n",
            "\n",
            "[epoch: 173/1000, batch:     7/   15, ite: 2587] train loss: 0.178883, tar: 0.018571 \n",
            "l0: 0.014544, l1: 0.014433, l2: 0.015851, l3: 0.018539, l4: 0.019487, l5: 0.025587, l6: 0.038934\n",
            "\n",
            "[epoch: 173/1000, batch:     8/   15, ite: 2588] train loss: 0.178525, tar: 0.018525 \n",
            "l0: 0.011972, l1: 0.011286, l2: 0.012452, l3: 0.014810, l4: 0.017405, l5: 0.024147, l6: 0.038930\n",
            "\n",
            "[epoch: 173/1000, batch:     9/   15, ite: 2589] train loss: 0.177991, tar: 0.018451 \n",
            "l0: 0.014613, l1: 0.016031, l2: 0.016680, l3: 0.018456, l4: 0.020279, l5: 0.024695, l6: 0.055220\n",
            "\n",
            "[epoch: 173/1000, batch:    10/   15, ite: 2590] train loss: 0.177857, tar: 0.018409 \n",
            "l0: 0.023791, l1: 0.023548, l2: 0.025585, l3: 0.026889, l4: 0.028730, l5: 0.037141, l6: 0.068050\n",
            "\n",
            "[epoch: 173/1000, batch:    11/   15, ite: 2591] train loss: 0.178471, tar: 0.018468 \n",
            "l0: 0.020047, l1: 0.018464, l2: 0.022124, l3: 0.024633, l4: 0.026085, l5: 0.030938, l6: 0.046861\n",
            "\n",
            "[epoch: 173/1000, batch:    12/   15, ite: 2592] train loss: 0.178587, tar: 0.018485 \n",
            "l0: 0.016954, l1: 0.016290, l2: 0.018779, l3: 0.023557, l4: 0.027220, l5: 0.031497, l6: 0.042647\n",
            "\n",
            "[epoch: 173/1000, batch:    13/   15, ite: 2593] train loss: 0.178570, tar: 0.018469 \n",
            "l0: 0.015159, l1: 0.015373, l2: 0.017370, l3: 0.018872, l4: 0.020282, l5: 0.023831, l6: 0.034079\n",
            "\n",
            "[epoch: 173/1000, batch:    14/   15, ite: 2594] train loss: 0.178212, tar: 0.018433 \n",
            "l0: 0.011949, l1: 0.011272, l2: 0.013592, l3: 0.016206, l4: 0.016379, l5: 0.018156, l6: 0.028082\n",
            "\n",
            "[epoch: 173/1000, batch:    15/   15, ite: 2595] train loss: 0.177554, tar: 0.018365 \n",
            "l0: 0.011429, l1: 0.011694, l2: 0.013508, l3: 0.014641, l4: 0.017782, l5: 0.022705, l6: 0.035806\n",
            "\n",
            "[epoch: 174/1000, batch:     1/   15, ite: 2596] train loss: 0.177033, tar: 0.018293 \n",
            "l0: 0.024701, l1: 0.021674, l2: 0.024406, l3: 0.029471, l4: 0.032545, l5: 0.043786, l6: 0.070462\n",
            "\n",
            "[epoch: 174/1000, batch:     2/   15, ite: 2597] train loss: 0.177755, tar: 0.018359 \n",
            "l0: 0.046795, l1: 0.044015, l2: 0.048014, l3: 0.059911, l4: 0.059591, l5: 0.052908, l6: 0.066083\n",
            "\n",
            "[epoch: 174/1000, batch:     3/   15, ite: 2598] train loss: 0.179791, tar: 0.018649 \n",
            "l0: 0.019168, l1: 0.019174, l2: 0.022394, l3: 0.026218, l4: 0.027577, l5: 0.033228, l6: 0.047031\n",
            "\n",
            "[epoch: 174/1000, batch:     4/   15, ite: 2599] train loss: 0.179942, tar: 0.018654 \n",
            "l0: 0.016825, l1: 0.018062, l2: 0.020519, l3: 0.025005, l4: 0.022099, l5: 0.020255, l6: 0.039524\n",
            "\n",
            "[epoch: 174/1000, batch:     5/   15, ite: 2600] train loss: 0.179766, tar: 0.018636 \n",
            "l0: 0.019921, l1: 0.022586, l2: 0.024131, l3: 0.024424, l4: 0.024208, l5: 0.024252, l6: 0.037297\n",
            "\n",
            "[epoch: 174/1000, batch:     6/   15, ite: 2601] train loss: 0.176820, tar: 0.019921 \n",
            "l0: 0.012918, l1: 0.013566, l2: 0.013784, l3: 0.015904, l4: 0.017940, l5: 0.023996, l6: 0.033949\n",
            "\n",
            "[epoch: 174/1000, batch:     7/   15, ite: 2602] train loss: 0.154438, tar: 0.016420 \n",
            "l0: 0.012843, l1: 0.012742, l2: 0.014201, l3: 0.016080, l4: 0.016123, l5: 0.024007, l6: 0.029998\n",
            "\n",
            "[epoch: 174/1000, batch:     8/   15, ite: 2603] train loss: 0.144956, tar: 0.015227 \n",
            "l0: 0.016275, l1: 0.015154, l2: 0.018104, l3: 0.021135, l4: 0.022724, l5: 0.025903, l6: 0.031680\n",
            "\n",
            "[epoch: 174/1000, batch:     9/   15, ite: 2604] train loss: 0.146461, tar: 0.015489 \n",
            "l0: 0.019552, l1: 0.017691, l2: 0.020555, l3: 0.026361, l4: 0.029546, l5: 0.034899, l6: 0.032998\n",
            "\n",
            "[epoch: 174/1000, batch:    10/   15, ite: 2605] train loss: 0.153489, tar: 0.016302 \n",
            "l0: 0.018501, l1: 0.016261, l2: 0.021102, l3: 0.024684, l4: 0.025808, l5: 0.029717, l6: 0.039966\n",
            "\n",
            "[epoch: 174/1000, batch:    11/   15, ite: 2606] train loss: 0.157248, tar: 0.016668 \n",
            "l0: 0.018013, l1: 0.017384, l2: 0.020365, l3: 0.019887, l4: 0.023687, l5: 0.027451, l6: 0.053224\n",
            "\n",
            "[epoch: 174/1000, batch:    12/   15, ite: 2607] train loss: 0.160499, tar: 0.016861 \n",
            "l0: 0.025026, l1: 0.020650, l2: 0.026499, l3: 0.031487, l4: 0.038900, l5: 0.045674, l6: 0.059530\n",
            "\n",
            "[epoch: 174/1000, batch:    13/   15, ite: 2608] train loss: 0.171408, tar: 0.017881 \n",
            "l0: 0.018600, l1: 0.019084, l2: 0.020566, l3: 0.023290, l4: 0.023900, l5: 0.027901, l6: 0.047459\n",
            "\n",
            "[epoch: 174/1000, batch:    14/   15, ite: 2609] train loss: 0.172451, tar: 0.017961 \n",
            "l0: 0.020310, l1: 0.019578, l2: 0.022110, l3: 0.024970, l4: 0.028424, l5: 0.035698, l6: 0.066415\n",
            "\n",
            "[epoch: 174/1000, batch:    15/   15, ite: 2610] train loss: 0.176957, tar: 0.018196 \n",
            "l0: 0.014110, l1: 0.014778, l2: 0.015813, l3: 0.020657, l4: 0.022162, l5: 0.029875, l6: 0.042689\n",
            "\n",
            "[epoch: 175/1000, batch:     1/   15, ite: 2611] train loss: 0.175423, tar: 0.017825 \n",
            "l0: 0.016736, l1: 0.018138, l2: 0.018101, l3: 0.019946, l4: 0.020330, l5: 0.025646, l6: 0.038425\n",
            "\n",
            "[epoch: 175/1000, batch:     2/   15, ite: 2612] train loss: 0.173915, tar: 0.017734 \n",
            "l0: 0.016335, l1: 0.015811, l2: 0.020340, l3: 0.021567, l4: 0.021444, l5: 0.026104, l6: 0.037671\n",
            "\n",
            "[epoch: 175/1000, batch:     3/   15, ite: 2613] train loss: 0.172788, tar: 0.017626 \n",
            "l0: 0.026023, l1: 0.025104, l2: 0.028972, l3: 0.036203, l4: 0.040201, l5: 0.047791, l6: 0.054468\n",
            "\n",
            "[epoch: 175/1000, batch:     4/   15, ite: 2614] train loss: 0.178929, tar: 0.018226 \n",
            "l0: 0.025708, l1: 0.028273, l2: 0.028595, l3: 0.027780, l4: 0.025163, l5: 0.019985, l6: 0.032939\n",
            "\n",
            "[epoch: 175/1000, batch:     5/   15, ite: 2615] train loss: 0.179563, tar: 0.018725 \n",
            "l0: 0.013348, l1: 0.013048, l2: 0.019166, l3: 0.019755, l4: 0.018753, l5: 0.020775, l6: 0.040582\n",
            "\n",
            "[epoch: 175/1000, batch:     6/   15, ite: 2616] train loss: 0.177430, tar: 0.018389 \n",
            "l0: 0.012857, l1: 0.013359, l2: 0.014654, l3: 0.015732, l4: 0.020116, l5: 0.029319, l6: 0.042831\n",
            "\n",
            "[epoch: 175/1000, batch:     7/   15, ite: 2617] train loss: 0.175750, tar: 0.018063 \n",
            "l0: 0.013536, l1: 0.013132, l2: 0.016124, l3: 0.018356, l4: 0.019845, l5: 0.023849, l6: 0.035186\n",
            "\n",
            "[epoch: 175/1000, batch:     8/   15, ite: 2618] train loss: 0.173765, tar: 0.017812 \n",
            "l0: 0.013817, l1: 0.013944, l2: 0.017099, l3: 0.018207, l4: 0.018678, l5: 0.023017, l6: 0.041941\n",
            "\n",
            "[epoch: 175/1000, batch:     9/   15, ite: 2619] train loss: 0.172341, tar: 0.017602 \n",
            "l0: 0.014768, l1: 0.015007, l2: 0.015158, l3: 0.017455, l4: 0.020537, l5: 0.026664, l6: 0.039757\n",
            "\n",
            "[epoch: 175/1000, batch:    10/   15, ite: 2620] train loss: 0.171191, tar: 0.017460 \n",
            "l0: 0.023685, l1: 0.025857, l2: 0.025753, l3: 0.028471, l4: 0.028967, l5: 0.029201, l6: 0.046039\n",
            "\n",
            "[epoch: 175/1000, batch:    11/   15, ite: 2621] train loss: 0.172943, tar: 0.017756 \n",
            "l0: 0.022699, l1: 0.024509, l2: 0.025708, l3: 0.024911, l4: 0.026880, l5: 0.025735, l6: 0.034014\n",
            "\n",
            "[epoch: 175/1000, batch:    12/   15, ite: 2622] train loss: 0.173466, tar: 0.017981 \n",
            "l0: 0.021235, l1: 0.021346, l2: 0.026022, l3: 0.026434, l4: 0.029445, l5: 0.034231, l6: 0.058181\n",
            "\n",
            "[epoch: 175/1000, batch:    13/   15, ite: 2623] train loss: 0.175354, tar: 0.018122 \n",
            "l0: 0.016846, l1: 0.015969, l2: 0.018116, l3: 0.021008, l4: 0.025762, l5: 0.035830, l6: 0.067390\n",
            "\n",
            "[epoch: 175/1000, batch:    14/   15, ite: 2624] train loss: 0.176419, tar: 0.018069 \n",
            "l0: 0.016486, l1: 0.016399, l2: 0.018114, l3: 0.020894, l4: 0.025480, l5: 0.027134, l6: 0.040095\n",
            "\n",
            "[epoch: 175/1000, batch:    15/   15, ite: 2625] train loss: 0.175947, tar: 0.018006 \n",
            "l0: 0.020742, l1: 0.021310, l2: 0.023411, l3: 0.023842, l4: 0.025117, l5: 0.033515, l6: 0.054361\n",
            "\n",
            "[epoch: 176/1000, batch:     1/   15, ite: 2626] train loss: 0.176960, tar: 0.018111 \n",
            "l0: 0.013617, l1: 0.013033, l2: 0.014832, l3: 0.018698, l4: 0.024138, l5: 0.027787, l6: 0.035450\n",
            "\n",
            "[epoch: 176/1000, batch:     2/   15, ite: 2627] train loss: 0.175871, tar: 0.017945 \n",
            "l0: 0.016994, l1: 0.016486, l2: 0.017739, l3: 0.020372, l4: 0.023988, l5: 0.029225, l6: 0.038064\n",
            "\n",
            "[epoch: 176/1000, batch:     3/   15, ite: 2628] train loss: 0.175407, tar: 0.017911 \n",
            "l0: 0.017825, l1: 0.019134, l2: 0.021242, l3: 0.023168, l4: 0.034465, l5: 0.036674, l6: 0.050935\n",
            "\n",
            "[epoch: 176/1000, batch:     4/   15, ite: 2629] train loss: 0.176373, tar: 0.017908 \n",
            "l0: 0.015371, l1: 0.017069, l2: 0.018756, l3: 0.020062, l4: 0.021391, l5: 0.027537, l6: 0.039399\n",
            "\n",
            "[epoch: 176/1000, batch:     5/   15, ite: 2630] train loss: 0.175814, tar: 0.017823 \n",
            "l0: 0.018789, l1: 0.022093, l2: 0.022773, l3: 0.022247, l4: 0.018034, l5: 0.023735, l6: 0.047365\n",
            "\n",
            "[epoch: 176/1000, batch:     6/   15, ite: 2631] train loss: 0.175789, tar: 0.017854 \n",
            "l0: 0.015352, l1: 0.017386, l2: 0.019272, l3: 0.021485, l4: 0.023013, l5: 0.031283, l6: 0.043758\n",
            "\n",
            "[epoch: 176/1000, batch:     7/   15, ite: 2632] train loss: 0.175656, tar: 0.017776 \n",
            "l0: 0.020818, l1: 0.023145, l2: 0.021535, l3: 0.023220, l4: 0.020868, l5: 0.021139, l6: 0.028732\n",
            "\n",
            "[epoch: 176/1000, batch:     8/   15, ite: 2633] train loss: 0.175165, tar: 0.017868 \n",
            "l0: 0.014854, l1: 0.014020, l2: 0.017261, l3: 0.020881, l4: 0.021104, l5: 0.025661, l6: 0.037478\n",
            "\n",
            "[epoch: 176/1000, batch:     9/   15, ite: 2634] train loss: 0.174462, tar: 0.017780 \n",
            "l0: 0.014691, l1: 0.015222, l2: 0.017996, l3: 0.019651, l4: 0.021804, l5: 0.020970, l6: 0.030613\n",
            "\n",
            "[epoch: 176/1000, batch:    10/   15, ite: 2635] train loss: 0.173505, tar: 0.017692 \n",
            "l0: 0.019907, l1: 0.018941, l2: 0.025539, l3: 0.027777, l4: 0.032720, l5: 0.039578, l6: 0.058424\n",
            "\n",
            "[epoch: 176/1000, batch:    11/   15, ite: 2636] train loss: 0.174876, tar: 0.017753 \n",
            "l0: 0.014535, l1: 0.013831, l2: 0.016094, l3: 0.019877, l4: 0.025014, l5: 0.031574, l6: 0.057224\n",
            "\n",
            "[epoch: 176/1000, batch:    12/   15, ite: 2637] train loss: 0.174965, tar: 0.017666 \n",
            "l0: 0.020473, l1: 0.019745, l2: 0.020515, l3: 0.021610, l4: 0.024421, l5: 0.029027, l6: 0.049163\n",
            "\n",
            "[epoch: 176/1000, batch:    13/   15, ite: 2638] train loss: 0.175228, tar: 0.017740 \n",
            "l0: 0.023628, l1: 0.022642, l2: 0.027474, l3: 0.033258, l4: 0.034212, l5: 0.030896, l6: 0.058915\n",
            "\n",
            "[epoch: 176/1000, batch:    14/   15, ite: 2639] train loss: 0.176658, tar: 0.017891 \n",
            "l0: 0.028738, l1: 0.032541, l2: 0.032116, l3: 0.024926, l4: 0.027030, l5: 0.033372, l6: 0.036622\n",
            "\n",
            "[epoch: 176/1000, batch:    15/   15, ite: 2640] train loss: 0.177626, tar: 0.018162 \n",
            "l0: 0.019458, l1: 0.019039, l2: 0.019761, l3: 0.023060, l4: 0.026460, l5: 0.028618, l6: 0.046172\n",
            "\n",
            "[epoch: 177/1000, batch:     1/   15, ite: 2641] train loss: 0.177746, tar: 0.018194 \n",
            "l0: 0.018961, l1: 0.021485, l2: 0.019570, l3: 0.019876, l4: 0.022586, l5: 0.026725, l6: 0.041880\n",
            "\n",
            "[epoch: 177/1000, batch:     2/   15, ite: 2642] train loss: 0.177588, tar: 0.018212 \n",
            "l0: 0.020735, l1: 0.020407, l2: 0.022363, l3: 0.024674, l4: 0.026491, l5: 0.029789, l6: 0.047199\n",
            "\n",
            "[epoch: 177/1000, batch:     3/   15, ite: 2643] train loss: 0.177915, tar: 0.018271 \n",
            "l0: 0.014654, l1: 0.015115, l2: 0.016956, l3: 0.018007, l4: 0.019412, l5: 0.024009, l6: 0.032348\n",
            "\n",
            "[epoch: 177/1000, batch:     4/   15, ite: 2644] train loss: 0.177064, tar: 0.018188 \n",
            "l0: 0.014279, l1: 0.014517, l2: 0.017819, l3: 0.018149, l4: 0.018421, l5: 0.022825, l6: 0.039672\n",
            "\n",
            "[epoch: 177/1000, batch:     5/   15, ite: 2645] train loss: 0.176367, tar: 0.018102 \n",
            "l0: 0.019767, l1: 0.022584, l2: 0.024352, l3: 0.023936, l4: 0.024616, l5: 0.025059, l6: 0.035576\n",
            "\n",
            "[epoch: 177/1000, batch:     6/   15, ite: 2646] train loss: 0.176357, tar: 0.018138 \n",
            "l0: 0.018623, l1: 0.020887, l2: 0.020437, l3: 0.022232, l4: 0.023893, l5: 0.032017, l6: 0.057405\n",
            "\n",
            "[epoch: 177/1000, batch:     7/   15, ite: 2647] train loss: 0.176764, tar: 0.018148 \n",
            "l0: 0.015614, l1: 0.019131, l2: 0.017750, l3: 0.017590, l4: 0.020760, l5: 0.028942, l6: 0.046319\n",
            "\n",
            "[epoch: 177/1000, batch:     8/   15, ite: 2648] train loss: 0.176542, tar: 0.018095 \n",
            "l0: 0.023630, l1: 0.024039, l2: 0.031839, l3: 0.027939, l4: 0.027687, l5: 0.029898, l6: 0.045234\n",
            "\n",
            "[epoch: 177/1000, batch:     9/   15, ite: 2649] train loss: 0.177230, tar: 0.018208 \n",
            "l0: 0.016028, l1: 0.017490, l2: 0.020707, l3: 0.021872, l4: 0.022797, l5: 0.026897, l6: 0.040123\n",
            "\n",
            "[epoch: 177/1000, batch:    10/   15, ite: 2650] train loss: 0.177004, tar: 0.018165 \n",
            "l0: 0.020604, l1: 0.023252, l2: 0.022374, l3: 0.024437, l4: 0.025393, l5: 0.030232, l6: 0.046790\n",
            "\n",
            "[epoch: 177/1000, batch:    11/   15, ite: 2651] train loss: 0.177319, tar: 0.018212 \n",
            "l0: 0.016588, l1: 0.017690, l2: 0.019052, l3: 0.021502, l4: 0.022880, l5: 0.025066, l6: 0.046708\n",
            "\n",
            "[epoch: 177/1000, batch:    12/   15, ite: 2652] train loss: 0.177168, tar: 0.018181 \n",
            "l0: 0.012449, l1: 0.014947, l2: 0.013663, l3: 0.015837, l4: 0.018668, l5: 0.027032, l6: 0.044960\n",
            "\n",
            "[epoch: 177/1000, batch:    13/   15, ite: 2653] train loss: 0.176610, tar: 0.018073 \n",
            "l0: 0.015933, l1: 0.013824, l2: 0.018494, l3: 0.021767, l4: 0.024197, l5: 0.025369, l6: 0.045142\n",
            "\n",
            "[epoch: 177/1000, batch:    14/   15, ite: 2654] train loss: 0.176390, tar: 0.018033 \n",
            "l0: 0.019400, l1: 0.019036, l2: 0.024002, l3: 0.029607, l4: 0.032765, l5: 0.033361, l6: 0.046707\n",
            "\n",
            "[epoch: 177/1000, batch:    15/   15, ite: 2655] train loss: 0.176907, tar: 0.018058 \n",
            "l0: 0.022206, l1: 0.021893, l2: 0.023310, l3: 0.024252, l4: 0.028979, l5: 0.032983, l6: 0.044062\n",
            "\n",
            "[epoch: 178/1000, batch:     1/   15, ite: 2656] train loss: 0.177279, tar: 0.018132 \n",
            "l0: 0.014728, l1: 0.014202, l2: 0.016087, l3: 0.019021, l4: 0.019718, l5: 0.023762, l6: 0.040601\n",
            "\n",
            "[epoch: 178/1000, batch:     2/   15, ite: 2657] train loss: 0.176767, tar: 0.018073 \n",
            "l0: 0.018121, l1: 0.020891, l2: 0.019312, l3: 0.020714, l4: 0.021141, l5: 0.026339, l6: 0.045619\n",
            "\n",
            "[epoch: 178/1000, batch:     3/   15, ite: 2658] train loss: 0.176687, tar: 0.018073 \n",
            "l0: 0.019016, l1: 0.018759, l2: 0.020470, l3: 0.021487, l4: 0.023405, l5: 0.031687, l6: 0.061767\n",
            "\n",
            "[epoch: 178/1000, batch:     4/   15, ite: 2659] train loss: 0.177025, tar: 0.018089 \n",
            "l0: 0.017303, l1: 0.016156, l2: 0.018724, l3: 0.020129, l4: 0.022358, l5: 0.030350, l6: 0.049348\n",
            "\n",
            "[epoch: 178/1000, batch:     5/   15, ite: 2660] train loss: 0.176980, tar: 0.018076 \n",
            "l0: 0.015750, l1: 0.014793, l2: 0.017707, l3: 0.020975, l4: 0.024028, l5: 0.025751, l6: 0.042765\n",
            "\n",
            "[epoch: 178/1000, batch:     6/   15, ite: 2661] train loss: 0.176731, tar: 0.018038 \n",
            "l0: 0.013933, l1: 0.011915, l2: 0.014932, l3: 0.020860, l4: 0.021749, l5: 0.030517, l6: 0.049604\n",
            "\n",
            "[epoch: 178/1000, batch:     7/   15, ite: 2662] train loss: 0.176518, tar: 0.017972 \n",
            "l0: 0.020107, l1: 0.020574, l2: 0.024898, l3: 0.024306, l4: 0.026110, l5: 0.031323, l6: 0.053345\n",
            "\n",
            "[epoch: 178/1000, batch:     8/   15, ite: 2663] train loss: 0.176901, tar: 0.018006 \n",
            "l0: 0.020013, l1: 0.019368, l2: 0.024873, l3: 0.026888, l4: 0.030697, l5: 0.034362, l6: 0.046698\n",
            "\n",
            "[epoch: 178/1000, batch:     9/   15, ite: 2664] train loss: 0.177307, tar: 0.018037 \n",
            "l0: 0.012405, l1: 0.012413, l2: 0.015607, l3: 0.016851, l4: 0.017493, l5: 0.020491, l6: 0.035638\n",
            "\n",
            "[epoch: 178/1000, batch:    10/   15, ite: 2665] train loss: 0.176593, tar: 0.017951 \n",
            "l0: 0.017788, l1: 0.018042, l2: 0.018053, l3: 0.020013, l4: 0.020138, l5: 0.025436, l6: 0.042281\n",
            "\n",
            "[epoch: 178/1000, batch:    11/   15, ite: 2666] train loss: 0.176368, tar: 0.017948 \n",
            "l0: 0.012085, l1: 0.011684, l2: 0.014232, l3: 0.015737, l4: 0.018706, l5: 0.024771, l6: 0.037252\n",
            "\n",
            "[epoch: 178/1000, batch:    12/   15, ite: 2667] train loss: 0.175743, tar: 0.017861 \n",
            "l0: 0.023893, l1: 0.025342, l2: 0.023872, l3: 0.028103, l4: 0.030782, l5: 0.034455, l6: 0.056064\n",
            "\n",
            "[epoch: 178/1000, batch:    13/   15, ite: 2668] train loss: 0.176431, tar: 0.017949 \n",
            "l0: 0.017164, l1: 0.016786, l2: 0.016507, l3: 0.019224, l4: 0.023840, l5: 0.034212, l6: 0.051594\n",
            "\n",
            "[epoch: 178/1000, batch:    14/   15, ite: 2669] train loss: 0.176473, tar: 0.017938 \n",
            "l0: 0.014176, l1: 0.015469, l2: 0.015602, l3: 0.015924, l4: 0.017621, l5: 0.023579, l6: 0.033948\n",
            "\n",
            "[epoch: 178/1000, batch:    15/   15, ite: 2670] train loss: 0.175899, tar: 0.017884 \n",
            "l0: 0.011972, l1: 0.011914, l2: 0.013109, l3: 0.013864, l4: 0.016023, l5: 0.020794, l6: 0.026495\n",
            "\n",
            "[epoch: 179/1000, batch:     1/   15, ite: 2671] train loss: 0.175030, tar: 0.017801 \n",
            "l0: 0.019805, l1: 0.018412, l2: 0.021725, l3: 0.024516, l4: 0.029255, l5: 0.032761, l6: 0.043487\n",
            "\n",
            "[epoch: 179/1000, batch:     2/   15, ite: 2672] train loss: 0.175237, tar: 0.017829 \n",
            "l0: 0.012161, l1: 0.011984, l2: 0.013381, l3: 0.016463, l4: 0.017858, l5: 0.026161, l6: 0.039672\n",
            "\n",
            "[epoch: 179/1000, batch:     3/   15, ite: 2673] train loss: 0.174723, tar: 0.017751 \n",
            "l0: 0.012888, l1: 0.013730, l2: 0.013803, l3: 0.016624, l4: 0.019181, l5: 0.023862, l6: 0.038571\n",
            "\n",
            "[epoch: 179/1000, batch:     4/   15, ite: 2674] train loss: 0.174235, tar: 0.017685 \n",
            "l0: 0.028099, l1: 0.031328, l2: 0.031989, l3: 0.030844, l4: 0.033140, l5: 0.034937, l6: 0.051758\n",
            "\n",
            "[epoch: 179/1000, batch:     5/   15, ite: 2675] train loss: 0.175140, tar: 0.017824 \n",
            "l0: 0.014139, l1: 0.012851, l2: 0.017490, l3: 0.019161, l4: 0.019691, l5: 0.021961, l6: 0.030543\n",
            "\n",
            "[epoch: 179/1000, batch:     6/   15, ite: 2676] train loss: 0.174623, tar: 0.017776 \n",
            "l0: 0.020290, l1: 0.020704, l2: 0.022752, l3: 0.024008, l4: 0.025492, l5: 0.029777, l6: 0.053079\n",
            "\n",
            "[epoch: 179/1000, batch:     7/   15, ite: 2677] train loss: 0.174902, tar: 0.017808 \n",
            "l0: 0.015159, l1: 0.016589, l2: 0.016015, l3: 0.019219, l4: 0.019984, l5: 0.021297, l6: 0.028664\n",
            "\n",
            "[epoch: 179/1000, batch:     8/   15, ite: 2678] train loss: 0.174415, tar: 0.017774 \n",
            "l0: 0.014893, l1: 0.015885, l2: 0.017322, l3: 0.020148, l4: 0.022073, l5: 0.026561, l6: 0.038849\n",
            "\n",
            "[epoch: 179/1000, batch:     9/   15, ite: 2679] train loss: 0.174178, tar: 0.017738 \n",
            "l0: 0.015091, l1: 0.015786, l2: 0.017543, l3: 0.019392, l4: 0.019163, l5: 0.022757, l6: 0.048497\n",
            "\n",
            "[epoch: 179/1000, batch:    10/   15, ite: 2680] train loss: 0.173979, tar: 0.017705 \n",
            "l0: 0.015489, l1: 0.016556, l2: 0.017387, l3: 0.019036, l4: 0.022136, l5: 0.029024, l6: 0.054964\n",
            "\n",
            "[epoch: 179/1000, batch:    11/   15, ite: 2681] train loss: 0.173987, tar: 0.017678 \n",
            "l0: 0.015533, l1: 0.016080, l2: 0.017160, l3: 0.018141, l4: 0.020927, l5: 0.024974, l6: 0.033147\n",
            "\n",
            "[epoch: 179/1000, batch:    12/   15, ite: 2682] train loss: 0.173645, tar: 0.017651 \n",
            "l0: 0.013182, l1: 0.012254, l2: 0.015047, l3: 0.016578, l4: 0.018344, l5: 0.025332, l6: 0.037368\n",
            "\n",
            "[epoch: 179/1000, batch:    13/   15, ite: 2683] train loss: 0.173217, tar: 0.017598 \n",
            "l0: 0.024286, l1: 0.025890, l2: 0.024159, l3: 0.027436, l4: 0.027852, l5: 0.030286, l6: 0.060411\n",
            "\n",
            "[epoch: 179/1000, batch:    14/   15, ite: 2684] train loss: 0.173777, tar: 0.017677 \n",
            "l0: 0.016731, l1: 0.015818, l2: 0.019510, l3: 0.020605, l4: 0.021642, l5: 0.029107, l6: 0.045633\n",
            "\n",
            "[epoch: 179/1000, batch:    15/   15, ite: 2685] train loss: 0.173722, tar: 0.017666 \n",
            "l0: 0.014663, l1: 0.014094, l2: 0.015945, l3: 0.017186, l4: 0.020401, l5: 0.025152, l6: 0.046046\n",
            "\n",
            "[epoch: 180/1000, batch:     1/   15, ite: 2686] train loss: 0.173487, tar: 0.017631 \n",
            "l0: 0.011880, l1: 0.011533, l2: 0.013945, l3: 0.015885, l4: 0.016602, l5: 0.021769, l6: 0.037138\n",
            "\n",
            "[epoch: 180/1000, batch:     2/   15, ite: 2687] train loss: 0.172972, tar: 0.017565 \n",
            "l0: 0.019657, l1: 0.017403, l2: 0.021878, l3: 0.024604, l4: 0.024762, l5: 0.030506, l6: 0.050015\n",
            "\n",
            "[epoch: 180/1000, batch:     3/   15, ite: 2688] train loss: 0.173152, tar: 0.017589 \n",
            "l0: 0.020555, l1: 0.019235, l2: 0.023097, l3: 0.025842, l4: 0.030468, l5: 0.032507, l6: 0.044650\n",
            "\n",
            "[epoch: 180/1000, batch:     4/   15, ite: 2689] train loss: 0.173413, tar: 0.017622 \n",
            "l0: 0.024468, l1: 0.027257, l2: 0.026928, l3: 0.026365, l4: 0.027766, l5: 0.033702, l6: 0.051182\n",
            "\n",
            "[epoch: 180/1000, batch:     5/   15, ite: 2690] train loss: 0.173905, tar: 0.017698 \n",
            "l0: 0.015400, l1: 0.015577, l2: 0.016731, l3: 0.017968, l4: 0.016676, l5: 0.021329, l6: 0.037206\n",
            "\n",
            "[epoch: 180/1000, batch:     6/   15, ite: 2691] train loss: 0.173542, tar: 0.017673 \n",
            "l0: 0.017032, l1: 0.016963, l2: 0.018196, l3: 0.019905, l4: 0.021959, l5: 0.027945, l6: 0.046817\n",
            "\n",
            "[epoch: 180/1000, batch:     7/   15, ite: 2692] train loss: 0.173491, tar: 0.017666 \n",
            "l0: 0.014242, l1: 0.015082, l2: 0.016305, l3: 0.017720, l4: 0.017630, l5: 0.020493, l6: 0.029670\n",
            "\n",
            "[epoch: 180/1000, batch:     8/   15, ite: 2693] train loss: 0.173035, tar: 0.017629 \n",
            "l0: 0.015438, l1: 0.016041, l2: 0.017698, l3: 0.016977, l4: 0.020848, l5: 0.025861, l6: 0.039312\n",
            "\n",
            "[epoch: 180/1000, batch:     9/   15, ite: 2694] train loss: 0.172813, tar: 0.017606 \n",
            "l0: 0.019079, l1: 0.019124, l2: 0.021791, l3: 0.024270, l4: 0.022050, l5: 0.028468, l6: 0.052264\n",
            "\n",
            "[epoch: 180/1000, batch:    10/   15, ite: 2695] train loss: 0.172963, tar: 0.017621 \n",
            "l0: 0.015383, l1: 0.014840, l2: 0.018572, l3: 0.021928, l4: 0.024245, l5: 0.023680, l6: 0.043039\n",
            "\n",
            "[epoch: 180/1000, batch:    11/   15, ite: 2696] train loss: 0.172846, tar: 0.017598 \n",
            "l0: 0.014333, l1: 0.014456, l2: 0.017380, l3: 0.020618, l4: 0.020040, l5: 0.024486, l6: 0.035427\n",
            "\n",
            "[epoch: 180/1000, batch:    12/   15, ite: 2697] train loss: 0.172577, tar: 0.017564 \n",
            "l0: 0.013460, l1: 0.014234, l2: 0.015880, l3: 0.017702, l4: 0.017340, l5: 0.021252, l6: 0.038571\n",
            "\n",
            "[epoch: 180/1000, batch:    13/   15, ite: 2698] train loss: 0.172228, tar: 0.017522 \n",
            "l0: 0.011650, l1: 0.011439, l2: 0.013081, l3: 0.014575, l4: 0.016281, l5: 0.025381, l6: 0.039024\n",
            "\n",
            "[epoch: 180/1000, batch:    14/   15, ite: 2699] train loss: 0.171816, tar: 0.017463 \n",
            "l0: 0.013075, l1: 0.012096, l2: 0.014758, l3: 0.016947, l4: 0.018765, l5: 0.028230, l6: 0.038567\n",
            "\n",
            "[epoch: 180/1000, batch:    15/   15, ite: 2700] train loss: 0.171522, tar: 0.017419 \n",
            "l0: 0.013774, l1: 0.014302, l2: 0.016135, l3: 0.017143, l4: 0.018338, l5: 0.023251, l6: 0.037986\n",
            "\n",
            "[epoch: 181/1000, batch:     1/   15, ite: 2701] train loss: 0.140928, tar: 0.013774 \n",
            "l0: 0.013539, l1: 0.014014, l2: 0.015880, l3: 0.017692, l4: 0.019536, l5: 0.024391, l6: 0.052323\n",
            "\n",
            "[epoch: 181/1000, batch:     2/   15, ite: 2702] train loss: 0.149152, tar: 0.013657 \n",
            "l0: 0.010875, l1: 0.010690, l2: 0.012576, l3: 0.014701, l4: 0.019228, l5: 0.020862, l6: 0.029911\n",
            "\n",
            "[epoch: 181/1000, batch:     3/   15, ite: 2703] train loss: 0.139049, tar: 0.012729 \n",
            "l0: 0.011722, l1: 0.011020, l2: 0.013496, l3: 0.015623, l4: 0.017872, l5: 0.020334, l6: 0.046162\n",
            "\n",
            "[epoch: 181/1000, batch:     4/   15, ite: 2704] train loss: 0.138344, tar: 0.012478 \n",
            "l0: 0.015328, l1: 0.013679, l2: 0.017235, l3: 0.019733, l4: 0.021259, l5: 0.024874, l6: 0.033708\n",
            "\n",
            "[epoch: 181/1000, batch:     5/   15, ite: 2705] train loss: 0.139838, tar: 0.013048 \n",
            "l0: 0.013525, l1: 0.014445, l2: 0.015398, l3: 0.016899, l4: 0.017437, l5: 0.020628, l6: 0.029762\n",
            "\n",
            "[epoch: 181/1000, batch:     6/   15, ite: 2706] train loss: 0.137881, tar: 0.013127 \n",
            "l0: 0.017027, l1: 0.016769, l2: 0.019340, l3: 0.021978, l4: 0.020946, l5: 0.025544, l6: 0.046275\n",
            "\n",
            "[epoch: 181/1000, batch:     7/   15, ite: 2707] train loss: 0.142166, tar: 0.013684 \n",
            "l0: 0.011810, l1: 0.012690, l2: 0.011917, l3: 0.013982, l4: 0.015493, l5: 0.017339, l6: 0.028032\n",
            "\n",
            "[epoch: 181/1000, batch:     8/   15, ite: 2708] train loss: 0.138303, tar: 0.013450 \n",
            "l0: 0.011891, l1: 0.010791, l2: 0.012684, l3: 0.016919, l4: 0.019245, l5: 0.023560, l6: 0.038765\n",
            "\n",
            "[epoch: 181/1000, batch:     9/   15, ite: 2709] train loss: 0.137809, tar: 0.013277 \n",
            "l0: 0.011791, l1: 0.010922, l2: 0.012956, l3: 0.015695, l4: 0.017413, l5: 0.020130, l6: 0.035613\n",
            "\n",
            "[epoch: 181/1000, batch:    10/   15, ite: 2710] train loss: 0.136480, tar: 0.013128 \n",
            "l0: 0.014387, l1: 0.013489, l2: 0.019379, l3: 0.020734, l4: 0.019188, l5: 0.021870, l6: 0.035004\n",
            "\n",
            "[epoch: 181/1000, batch:    11/   15, ite: 2711] train loss: 0.137169, tar: 0.013243 \n",
            "l0: 0.016721, l1: 0.016323, l2: 0.018895, l3: 0.020313, l4: 0.022234, l5: 0.026690, l6: 0.037788\n",
            "\n",
            "[epoch: 181/1000, batch:    12/   15, ite: 2712] train loss: 0.138985, tar: 0.013533 \n",
            "l0: 0.014167, l1: 0.014235, l2: 0.016553, l3: 0.016479, l4: 0.017823, l5: 0.022679, l6: 0.037162\n",
            "\n",
            "[epoch: 181/1000, batch:    13/   15, ite: 2713] train loss: 0.138994, tar: 0.013581 \n",
            "l0: 0.022469, l1: 0.022036, l2: 0.024449, l3: 0.027332, l4: 0.029843, l5: 0.034113, l6: 0.046579\n",
            "\n",
            "[epoch: 181/1000, batch:    14/   15, ite: 2714] train loss: 0.143839, tar: 0.014216 \n",
            "l0: 0.018843, l1: 0.018233, l2: 0.019826, l3: 0.022673, l4: 0.023459, l5: 0.027256, l6: 0.049127\n",
            "\n",
            "[epoch: 181/1000, batch:    15/   15, ite: 2715] train loss: 0.146210, tar: 0.014525 \n",
            "l0: 0.011083, l1: 0.010458, l2: 0.012060, l3: 0.012907, l4: 0.014526, l5: 0.020788, l6: 0.038179\n",
            "\n",
            "[epoch: 182/1000, batch:     1/   15, ite: 2716] train loss: 0.144572, tar: 0.014310 \n",
            "l0: 0.012276, l1: 0.011963, l2: 0.013560, l3: 0.014621, l4: 0.017388, l5: 0.022882, l6: 0.031804\n",
            "\n",
            "[epoch: 182/1000, batch:     2/   15, ite: 2717] train loss: 0.143391, tar: 0.014190 \n",
            "l0: 0.009771, l1: 0.009424, l2: 0.011165, l3: 0.013441, l4: 0.015192, l5: 0.017652, l6: 0.030566\n",
            "\n",
            "[epoch: 182/1000, batch:     3/   15, ite: 2718] train loss: 0.141381, tar: 0.013944 \n",
            "l0: 0.017456, l1: 0.016104, l2: 0.019407, l3: 0.021021, l4: 0.023048, l5: 0.029084, l6: 0.054262\n",
            "\n",
            "[epoch: 182/1000, batch:     4/   15, ite: 2719] train loss: 0.143434, tar: 0.014129 \n",
            "l0: 0.016424, l1: 0.016026, l2: 0.018831, l3: 0.019078, l4: 0.022844, l5: 0.026674, l6: 0.046343\n",
            "\n",
            "[epoch: 182/1000, batch:     5/   15, ite: 2720] train loss: 0.144573, tar: 0.014244 \n",
            "l0: 0.016403, l1: 0.017184, l2: 0.018526, l3: 0.021445, l4: 0.021832, l5: 0.026836, l6: 0.046662\n",
            "\n",
            "[epoch: 182/1000, batch:     6/   15, ite: 2721] train loss: 0.145731, tar: 0.014347 \n",
            "l0: 0.025769, l1: 0.024775, l2: 0.027041, l3: 0.026234, l4: 0.029919, l5: 0.035658, l6: 0.050060\n",
            "\n",
            "[epoch: 182/1000, batch:     7/   15, ite: 2722] train loss: 0.149082, tar: 0.014866 \n",
            "l0: 0.014449, l1: 0.013424, l2: 0.017466, l3: 0.020184, l4: 0.021932, l5: 0.024022, l6: 0.038856\n",
            "\n",
            "[epoch: 182/1000, batch:     8/   15, ite: 2723] train loss: 0.149137, tar: 0.014848 \n",
            "l0: 0.013137, l1: 0.013111, l2: 0.016406, l3: 0.019688, l4: 0.019840, l5: 0.025453, l6: 0.042249\n",
            "\n",
            "[epoch: 182/1000, batch:     9/   15, ite: 2724] train loss: 0.149168, tar: 0.014777 \n",
            "l0: 0.020026, l1: 0.019503, l2: 0.021776, l3: 0.023602, l4: 0.026517, l5: 0.033515, l6: 0.048514\n",
            "\n",
            "[epoch: 182/1000, batch:    10/   15, ite: 2725] train loss: 0.150939, tar: 0.014987 \n",
            "l0: 0.011539, l1: 0.011041, l2: 0.012956, l3: 0.015838, l4: 0.017981, l5: 0.021690, l6: 0.038912\n",
            "\n",
            "[epoch: 182/1000, batch:    11/   15, ite: 2726] train loss: 0.150132, tar: 0.014854 \n",
            "l0: 0.013342, l1: 0.013698, l2: 0.015602, l3: 0.018017, l4: 0.018526, l5: 0.023766, l6: 0.032465\n",
            "\n",
            "[epoch: 182/1000, batch:    12/   15, ite: 2727] train loss: 0.149587, tar: 0.014798 \n",
            "l0: 0.011838, l1: 0.011952, l2: 0.013694, l3: 0.016100, l4: 0.017197, l5: 0.018617, l6: 0.031225\n",
            "\n",
            "[epoch: 182/1000, batch:    13/   15, ite: 2728] train loss: 0.148553, tar: 0.014692 \n",
            "l0: 0.019241, l1: 0.018453, l2: 0.022508, l3: 0.023470, l4: 0.024630, l5: 0.028039, l6: 0.045270\n",
            "\n",
            "[epoch: 182/1000, batch:    14/   15, ite: 2729] train loss: 0.149693, tar: 0.014849 \n",
            "l0: 0.011906, l1: 0.011072, l2: 0.014045, l3: 0.015004, l4: 0.017955, l5: 0.020415, l6: 0.037114\n",
            "\n",
            "[epoch: 182/1000, batch:    15/   15, ite: 2730] train loss: 0.148953, tar: 0.014751 \n",
            "l0: 0.016245, l1: 0.016037, l2: 0.020894, l3: 0.022818, l4: 0.023328, l5: 0.028077, l6: 0.049882\n",
            "\n",
            "[epoch: 183/1000, batch:     1/   15, ite: 2731] train loss: 0.149867, tar: 0.014799 \n",
            "l0: 0.012095, l1: 0.012351, l2: 0.013887, l3: 0.017148, l4: 0.018501, l5: 0.022050, l6: 0.035615\n",
            "\n",
            "[epoch: 183/1000, batch:     2/   15, ite: 2732] train loss: 0.149298, tar: 0.014715 \n",
            "l0: 0.014372, l1: 0.014095, l2: 0.017125, l3: 0.020658, l4: 0.021213, l5: 0.022276, l6: 0.040216\n",
            "\n",
            "[epoch: 183/1000, batch:     3/   15, ite: 2733] train loss: 0.149318, tar: 0.014704 \n",
            "l0: 0.013425, l1: 0.015324, l2: 0.015142, l3: 0.017438, l4: 0.019045, l5: 0.023670, l6: 0.038639\n",
            "\n",
            "[epoch: 183/1000, batch:     4/   15, ite: 2734] train loss: 0.149122, tar: 0.014667 \n",
            "l0: 0.016325, l1: 0.015075, l2: 0.017549, l3: 0.020262, l4: 0.021779, l5: 0.027317, l6: 0.039636\n",
            "\n",
            "[epoch: 183/1000, batch:     5/   15, ite: 2735] train loss: 0.149374, tar: 0.014714 \n",
            "l0: 0.019454, l1: 0.018471, l2: 0.020459, l3: 0.023004, l4: 0.025145, l5: 0.031698, l6: 0.057830\n",
            "\n",
            "[epoch: 183/1000, batch:     6/   15, ite: 2736] train loss: 0.150671, tar: 0.014846 \n",
            "l0: 0.024870, l1: 0.025708, l2: 0.031264, l3: 0.028416, l4: 0.033031, l5: 0.035380, l6: 0.040520\n",
            "\n",
            "[epoch: 183/1000, batch:     7/   15, ite: 2737] train loss: 0.152523, tar: 0.015117 \n",
            "l0: 0.011683, l1: 0.012773, l2: 0.015505, l3: 0.015273, l4: 0.016689, l5: 0.021449, l6: 0.034076\n",
            "\n",
            "[epoch: 183/1000, batch:     8/   15, ite: 2738] train loss: 0.151863, tar: 0.015026 \n",
            "l0: 0.014903, l1: 0.014466, l2: 0.016022, l3: 0.019782, l4: 0.020688, l5: 0.025793, l6: 0.043294\n",
            "\n",
            "[epoch: 183/1000, batch:     9/   15, ite: 2739] train loss: 0.151942, tar: 0.015023 \n",
            "l0: 0.013737, l1: 0.014707, l2: 0.015058, l3: 0.017487, l4: 0.017268, l5: 0.019367, l6: 0.035033\n",
            "\n",
            "[epoch: 183/1000, batch:    10/   15, ite: 2740] train loss: 0.151460, tar: 0.014991 \n",
            "l0: 0.019129, l1: 0.021695, l2: 0.021775, l3: 0.021104, l4: 0.021338, l5: 0.019694, l6: 0.032587\n",
            "\n",
            "[epoch: 183/1000, batch:    11/   15, ite: 2741] train loss: 0.151603, tar: 0.015092 \n",
            "l0: 0.021198, l1: 0.022741, l2: 0.024236, l3: 0.024084, l4: 0.024515, l5: 0.027811, l6: 0.055442\n",
            "\n",
            "[epoch: 183/1000, batch:    12/   15, ite: 2742] train loss: 0.152756, tar: 0.015237 \n",
            "l0: 0.015682, l1: 0.014572, l2: 0.017858, l3: 0.020048, l4: 0.022631, l5: 0.027590, l6: 0.047916\n",
            "\n",
            "[epoch: 183/1000, batch:    13/   15, ite: 2743] train loss: 0.153071, tar: 0.015248 \n",
            "l0: 0.013825, l1: 0.014533, l2: 0.015096, l3: 0.016513, l4: 0.018117, l5: 0.021468, l6: 0.040831\n",
            "\n",
            "[epoch: 183/1000, batch:    14/   15, ite: 2744] train loss: 0.152783, tar: 0.015215 \n",
            "l0: 0.018367, l1: 0.018275, l2: 0.019341, l3: 0.019059, l4: 0.021813, l5: 0.031202, l6: 0.043479\n",
            "\n",
            "[epoch: 183/1000, batch:    15/   15, ite: 2745] train loss: 0.153199, tar: 0.015285 \n",
            "l0: 0.014558, l1: 0.014250, l2: 0.015580, l3: 0.018355, l4: 0.019400, l5: 0.027368, l6: 0.037945\n",
            "\n",
            "[epoch: 184/1000, batch:     1/   15, ite: 2746] train loss: 0.153075, tar: 0.015270 \n",
            "l0: 0.019293, l1: 0.019445, l2: 0.021139, l3: 0.023196, l4: 0.023037, l5: 0.025814, l6: 0.043472\n",
            "\n",
            "[epoch: 184/1000, batch:     2/   15, ite: 2747] train loss: 0.153549, tar: 0.015355 \n",
            "l0: 0.020293, l1: 0.021320, l2: 0.020172, l3: 0.022424, l4: 0.022861, l5: 0.031436, l6: 0.031174\n",
            "\n",
            "[epoch: 184/1000, batch:     3/   15, ite: 2748] train loss: 0.153886, tar: 0.015458 \n",
            "l0: 0.021424, l1: 0.024489, l2: 0.022240, l3: 0.022972, l4: 0.026419, l5: 0.032416, l6: 0.043792\n",
            "\n",
            "[epoch: 184/1000, batch:     4/   15, ite: 2749] train loss: 0.154699, tar: 0.015580 \n",
            "l0: 0.011629, l1: 0.011112, l2: 0.012152, l3: 0.013591, l4: 0.017702, l5: 0.024661, l6: 0.036986\n",
            "\n",
            "[epoch: 184/1000, batch:     5/   15, ite: 2750] train loss: 0.154162, tar: 0.015501 \n",
            "l0: 0.013113, l1: 0.013023, l2: 0.014244, l3: 0.015966, l4: 0.019516, l5: 0.022974, l6: 0.035092\n",
            "\n",
            "[epoch: 184/1000, batch:     6/   15, ite: 2751] train loss: 0.153765, tar: 0.015454 \n",
            "l0: 0.017952, l1: 0.019236, l2: 0.019128, l3: 0.021850, l4: 0.023535, l5: 0.027365, l6: 0.040726\n",
            "\n",
            "[epoch: 184/1000, batch:     7/   15, ite: 2752] train loss: 0.154073, tar: 0.015502 \n",
            "l0: 0.015754, l1: 0.017404, l2: 0.018315, l3: 0.017453, l4: 0.019517, l5: 0.025770, l6: 0.041527\n",
            "\n",
            "[epoch: 184/1000, batch:     8/   15, ite: 2753] train loss: 0.154105, tar: 0.015507 \n",
            "l0: 0.022902, l1: 0.021893, l2: 0.026243, l3: 0.026342, l4: 0.029429, l5: 0.033228, l6: 0.039550\n",
            "\n",
            "[epoch: 184/1000, batch:     9/   15, ite: 2754] train loss: 0.154947, tar: 0.015644 \n",
            "l0: 0.020540, l1: 0.021017, l2: 0.019450, l3: 0.020909, l4: 0.024114, l5: 0.032944, l6: 0.046314\n",
            "\n",
            "[epoch: 184/1000, batch:    10/   15, ite: 2755] train loss: 0.155499, tar: 0.015733 \n",
            "l0: 0.017649, l1: 0.017143, l2: 0.018686, l3: 0.020087, l4: 0.021967, l5: 0.029578, l6: 0.041639\n",
            "\n",
            "[epoch: 184/1000, batch:    11/   15, ite: 2756] train loss: 0.155700, tar: 0.015767 \n",
            "l0: 0.013492, l1: 0.012247, l2: 0.014280, l3: 0.019121, l4: 0.020786, l5: 0.024882, l6: 0.041493\n",
            "\n",
            "[epoch: 184/1000, batch:    12/   15, ite: 2757] train loss: 0.155535, tar: 0.015727 \n",
            "l0: 0.012181, l1: 0.011335, l2: 0.014071, l3: 0.017381, l4: 0.020172, l5: 0.027628, l6: 0.045997\n",
            "\n",
            "[epoch: 184/1000, batch:    13/   15, ite: 2758] train loss: 0.155418, tar: 0.015666 \n",
            "l0: 0.011774, l1: 0.011675, l2: 0.012965, l3: 0.015138, l4: 0.017890, l5: 0.022247, l6: 0.036735\n",
            "\n",
            "[epoch: 184/1000, batch:    14/   15, ite: 2759] train loss: 0.154960, tar: 0.015600 \n",
            "l0: 0.019476, l1: 0.017811, l2: 0.023562, l3: 0.025090, l4: 0.028194, l5: 0.030779, l6: 0.032911\n",
            "\n",
            "[epoch: 184/1000, batch:    15/   15, ite: 2760] train loss: 0.155341, tar: 0.015665 \n",
            "l0: 0.012501, l1: 0.012575, l2: 0.013717, l3: 0.015491, l4: 0.018892, l5: 0.023953, l6: 0.046345\n",
            "\n",
            "[epoch: 185/1000, batch:     1/   15, ite: 2761] train loss: 0.155147, tar: 0.015613 \n",
            "l0: 0.011290, l1: 0.011215, l2: 0.013110, l3: 0.016269, l4: 0.017019, l5: 0.021932, l6: 0.041941\n",
            "\n",
            "[epoch: 185/1000, batch:     2/   15, ite: 2762] train loss: 0.154786, tar: 0.015543 \n",
            "l0: 0.020174, l1: 0.020763, l2: 0.021251, l3: 0.025464, l4: 0.027650, l5: 0.030732, l6: 0.041596\n",
            "\n",
            "[epoch: 185/1000, batch:     3/   15, ite: 2763] train loss: 0.155307, tar: 0.015616 \n",
            "l0: 0.014346, l1: 0.014124, l2: 0.016273, l3: 0.017002, l4: 0.017777, l5: 0.022341, l6: 0.038969\n",
            "\n",
            "[epoch: 185/1000, batch:     4/   15, ite: 2764] train loss: 0.155081, tar: 0.015597 \n",
            "l0: 0.029105, l1: 0.030757, l2: 0.036855, l3: 0.032726, l4: 0.028762, l5: 0.030099, l6: 0.038884\n",
            "\n",
            "[epoch: 185/1000, batch:     5/   15, ite: 2765] train loss: 0.156191, tar: 0.015804 \n",
            "l0: 0.014593, l1: 0.014408, l2: 0.016270, l3: 0.017067, l4: 0.020208, l5: 0.025922, l6: 0.032192\n",
            "\n",
            "[epoch: 185/1000, batch:     6/   15, ite: 2766] train loss: 0.155955, tar: 0.015786 \n",
            "l0: 0.023498, l1: 0.019714, l2: 0.025353, l3: 0.026442, l4: 0.029948, l5: 0.040180, l6: 0.056920\n",
            "\n",
            "[epoch: 185/1000, batch:     7/   15, ite: 2767] train loss: 0.156942, tar: 0.015901 \n",
            "l0: 0.027184, l1: 0.029082, l2: 0.029122, l3: 0.027389, l4: 0.028697, l5: 0.029461, l6: 0.035238\n",
            "\n",
            "[epoch: 185/1000, batch:     8/   15, ite: 2768] train loss: 0.157666, tar: 0.016067 \n",
            "l0: 0.021765, l1: 0.021758, l2: 0.022518, l3: 0.022504, l4: 0.026999, l5: 0.030822, l6: 0.043330\n",
            "\n",
            "[epoch: 185/1000, batch:     9/   15, ite: 2769] train loss: 0.158130, tar: 0.016150 \n",
            "l0: 0.014331, l1: 0.014233, l2: 0.015283, l3: 0.016994, l4: 0.020390, l5: 0.027227, l6: 0.040492\n",
            "\n",
            "[epoch: 185/1000, batch:    10/   15, ite: 2770] train loss: 0.157999, tar: 0.016124 \n",
            "l0: 0.011906, l1: 0.011308, l2: 0.012779, l3: 0.014914, l4: 0.017050, l5: 0.022817, l6: 0.035274\n",
            "\n",
            "[epoch: 185/1000, batch:    11/   15, ite: 2771] train loss: 0.157549, tar: 0.016064 \n",
            "l0: 0.014731, l1: 0.015419, l2: 0.017435, l3: 0.020329, l4: 0.021348, l5: 0.026834, l6: 0.040253\n",
            "\n",
            "[epoch: 185/1000, batch:    12/   15, ite: 2772] train loss: 0.157532, tar: 0.016046 \n",
            "l0: 0.013466, l1: 0.012580, l2: 0.014611, l3: 0.017099, l4: 0.018282, l5: 0.020061, l6: 0.027177\n",
            "\n",
            "[epoch: 185/1000, batch:    13/   15, ite: 2773] train loss: 0.157063, tar: 0.016010 \n",
            "l0: 0.046062, l1: 0.046180, l2: 0.050649, l3: 0.055068, l4: 0.049385, l5: 0.054514, l6: 0.056875\n",
            "\n",
            "[epoch: 185/1000, batch:    14/   15, ite: 2774] train loss: 0.159788, tar: 0.016417 \n",
            "l0: 0.015170, l1: 0.016260, l2: 0.017266, l3: 0.018370, l4: 0.021271, l5: 0.028035, l6: 0.045687\n",
            "\n",
            "[epoch: 185/1000, batch:    15/   15, ite: 2775] train loss: 0.159818, tar: 0.016400 \n",
            "l0: 0.032614, l1: 0.028654, l2: 0.031589, l3: 0.037025, l4: 0.046536, l5: 0.059154, l6: 0.077505\n",
            "\n",
            "[epoch: 186/1000, batch:     1/   15, ite: 2776] train loss: 0.161835, tar: 0.016613 \n",
            "l0: 0.016949, l1: 0.017429, l2: 0.020822, l3: 0.022363, l4: 0.026426, l5: 0.026708, l6: 0.051658\n",
            "\n",
            "[epoch: 186/1000, batch:     2/   15, ite: 2777] train loss: 0.162102, tar: 0.016618 \n",
            "l0: 0.043367, l1: 0.040057, l2: 0.042221, l3: 0.046600, l4: 0.055134, l5: 0.072663, l6: 0.057644\n",
            "\n",
            "[epoch: 186/1000, batch:     3/   15, ite: 2778] train loss: 0.164609, tar: 0.016961 \n",
            "l0: 0.022251, l1: 0.020525, l2: 0.023028, l3: 0.027850, l4: 0.036221, l5: 0.039761, l6: 0.045720\n",
            "\n",
            "[epoch: 186/1000, batch:     4/   15, ite: 2779] train loss: 0.165251, tar: 0.017028 \n",
            "l0: 0.012264, l1: 0.013011, l2: 0.014348, l3: 0.015687, l4: 0.017257, l5: 0.023341, l6: 0.045281\n",
            "\n",
            "[epoch: 186/1000, batch:     5/   15, ite: 2780] train loss: 0.164951, tar: 0.016968 \n",
            "l0: 0.016075, l1: 0.016686, l2: 0.020656, l3: 0.021961, l4: 0.033427, l5: 0.037240, l6: 0.046496\n",
            "\n",
            "[epoch: 186/1000, batch:     6/   15, ite: 2781] train loss: 0.165291, tar: 0.016957 \n",
            "l0: 0.015752, l1: 0.017285, l2: 0.018090, l3: 0.018734, l4: 0.020469, l5: 0.036095, l6: 0.056866\n",
            "\n",
            "[epoch: 186/1000, batch:     7/   15, ite: 2782] train loss: 0.165511, tar: 0.016942 \n",
            "l0: 0.016469, l1: 0.017737, l2: 0.019405, l3: 0.022460, l4: 0.020395, l5: 0.027419, l6: 0.041052\n",
            "\n",
            "[epoch: 186/1000, batch:     8/   15, ite: 2783] train loss: 0.165504, tar: 0.016937 \n",
            "l0: 0.044837, l1: 0.039149, l2: 0.038991, l3: 0.042823, l4: 0.063663, l5: 0.098570, l6: 0.089470\n",
            "\n",
            "[epoch: 186/1000, batch:     9/   15, ite: 2784] train loss: 0.168504, tar: 0.017269 \n",
            "l0: 0.016432, l1: 0.014013, l2: 0.016436, l3: 0.019783, l4: 0.027772, l5: 0.043323, l6: 0.054813\n",
            "\n",
            "[epoch: 186/1000, batch:    10/   15, ite: 2785] train loss: 0.168787, tar: 0.017259 \n",
            "l0: 0.015671, l1: 0.015071, l2: 0.018308, l3: 0.021952, l4: 0.024587, l5: 0.029374, l6: 0.058216\n",
            "\n",
            "[epoch: 186/1000, batch:    11/   15, ite: 2786] train loss: 0.168954, tar: 0.017240 \n",
            "l0: 0.018854, l1: 0.017565, l2: 0.021034, l3: 0.025133, l4: 0.026409, l5: 0.033993, l6: 0.054233\n",
            "\n",
            "[epoch: 186/1000, batch:    12/   15, ite: 2787] train loss: 0.169279, tar: 0.017259 \n",
            "l0: 0.024800, l1: 0.027599, l2: 0.027299, l3: 0.028537, l4: 0.027224, l5: 0.028581, l6: 0.047810\n",
            "\n",
            "[epoch: 186/1000, batch:    13/   15, ite: 2788] train loss: 0.169763, tar: 0.017345 \n",
            "l0: 0.033143, l1: 0.042918, l2: 0.038073, l3: 0.032187, l4: 0.024780, l5: 0.026293, l6: 0.037181\n",
            "\n",
            "[epoch: 186/1000, batch:    14/   15, ite: 2789] train loss: 0.170491, tar: 0.017522 \n",
            "l0: 0.022216, l1: 0.022060, l2: 0.027038, l3: 0.026348, l4: 0.028965, l5: 0.032663, l6: 0.050486\n",
            "\n",
            "[epoch: 186/1000, batch:    15/   15, ite: 2790] train loss: 0.170928, tar: 0.017574 \n",
            "l0: 0.053350, l1: 0.055177, l2: 0.055056, l3: 0.059429, l4: 0.057214, l5: 0.060661, l6: 0.064902\n",
            "\n",
            "[epoch: 187/1000, batch:     1/   15, ite: 2791] train loss: 0.173509, tar: 0.017967 \n",
            "l0: 0.023511, l1: 0.021449, l2: 0.023071, l3: 0.039493, l4: 0.046599, l5: 0.044398, l6: 0.045594\n",
            "\n",
            "[epoch: 187/1000, batch:     2/   15, ite: 2792] train loss: 0.174276, tar: 0.018028 \n",
            "l0: 0.022836, l1: 0.028079, l2: 0.028765, l3: 0.030593, l4: 0.030665, l5: 0.033855, l6: 0.039035\n",
            "\n",
            "[epoch: 187/1000, batch:     3/   15, ite: 2793] train loss: 0.174701, tar: 0.018079 \n",
            "l0: 0.023706, l1: 0.021880, l2: 0.029599, l3: 0.028571, l4: 0.036598, l5: 0.055101, l6: 0.088414\n",
            "\n",
            "[epoch: 187/1000, batch:     4/   15, ite: 2794] train loss: 0.175863, tar: 0.018139 \n",
            "l0: 0.012749, l1: 0.014051, l2: 0.013491, l3: 0.015300, l4: 0.020735, l5: 0.033645, l6: 0.050119\n",
            "\n",
            "[epoch: 187/1000, batch:     5/   15, ite: 2795] train loss: 0.175697, tar: 0.018083 \n",
            "l0: 0.029179, l1: 0.036485, l2: 0.034945, l3: 0.029904, l4: 0.028500, l5: 0.052771, l6: 0.062798\n",
            "\n",
            "[epoch: 187/1000, batch:     6/   15, ite: 2796] train loss: 0.176727, tar: 0.018198 \n",
            "l0: 0.025924, l1: 0.024617, l2: 0.027747, l3: 0.029085, l4: 0.032016, l5: 0.051093, l6: 0.058059\n",
            "\n",
            "[epoch: 187/1000, batch:     7/   15, ite: 2797] train loss: 0.177467, tar: 0.018278 \n",
            "l0: 0.014790, l1: 0.013649, l2: 0.017308, l3: 0.020649, l4: 0.023939, l5: 0.032523, l6: 0.040728\n",
            "\n",
            "[epoch: 187/1000, batch:     8/   15, ite: 2798] train loss: 0.177325, tar: 0.018242 \n",
            "l0: 0.022631, l1: 0.023150, l2: 0.027441, l3: 0.030923, l4: 0.037358, l5: 0.041904, l6: 0.068192\n",
            "\n",
            "[epoch: 187/1000, batch:     9/   15, ite: 2799] train loss: 0.178076, tar: 0.018287 \n",
            "l0: 0.014291, l1: 0.015861, l2: 0.016716, l3: 0.017475, l4: 0.019762, l5: 0.024192, l6: 0.042000\n",
            "\n",
            "[epoch: 187/1000, batch:    10/   15, ite: 2800] train loss: 0.177798, tar: 0.018247 \n",
            "l0: 0.019356, l1: 0.021813, l2: 0.021605, l3: 0.027269, l4: 0.027665, l5: 0.028205, l6: 0.038542\n",
            "\n",
            "[epoch: 187/1000, batch:    11/   15, ite: 2801] train loss: 0.184454, tar: 0.019356 \n",
            "l0: 0.017254, l1: 0.014917, l2: 0.018890, l3: 0.024659, l4: 0.027357, l5: 0.033839, l6: 0.048027\n",
            "\n",
            "[epoch: 187/1000, batch:    12/   15, ite: 2802] train loss: 0.184699, tar: 0.018305 \n",
            "l0: 0.015210, l1: 0.016011, l2: 0.017236, l3: 0.019720, l4: 0.022304, l5: 0.029446, l6: 0.043231\n",
            "\n",
            "[epoch: 187/1000, batch:    13/   15, ite: 2803] train loss: 0.177519, tar: 0.017273 \n",
            "l0: 0.021827, l1: 0.023037, l2: 0.022841, l3: 0.025091, l4: 0.022134, l5: 0.022106, l6: 0.027848\n",
            "\n",
            "[epoch: 187/1000, batch:    14/   15, ite: 2804] train loss: 0.174360, tar: 0.018412 \n",
            "l0: 0.028656, l1: 0.033351, l2: 0.034373, l3: 0.035627, l4: 0.032251, l5: 0.042885, l6: 0.068344\n",
            "\n",
            "[epoch: 187/1000, batch:    15/   15, ite: 2805] train loss: 0.194586, tar: 0.020461 \n",
            "l0: 0.013736, l1: 0.012886, l2: 0.015802, l3: 0.016838, l4: 0.016959, l5: 0.025031, l6: 0.038741\n",
            "\n",
            "[epoch: 188/1000, batch:     1/   15, ite: 2806] train loss: 0.185487, tar: 0.019340 \n",
            "l0: 0.022634, l1: 0.022172, l2: 0.027826, l3: 0.029919, l4: 0.028889, l5: 0.029251, l6: 0.052213\n",
            "\n",
            "[epoch: 188/1000, batch:     2/   15, ite: 2807] train loss: 0.189404, tar: 0.019810 \n",
            "l0: 0.026026, l1: 0.026021, l2: 0.025072, l3: 0.027190, l4: 0.031279, l5: 0.057278, l6: 0.087046\n",
            "\n",
            "[epoch: 188/1000, batch:     3/   15, ite: 2808] train loss: 0.200717, tar: 0.020587 \n",
            "l0: 0.016903, l1: 0.018548, l2: 0.021748, l3: 0.021451, l4: 0.024299, l5: 0.032803, l6: 0.047934\n",
            "\n",
            "[epoch: 188/1000, batch:     4/   15, ite: 2809] train loss: 0.198825, tar: 0.020178 \n",
            "l0: 0.031793, l1: 0.031524, l2: 0.037970, l3: 0.044273, l4: 0.040626, l5: 0.040559, l6: 0.056511\n",
            "\n",
            "[epoch: 188/1000, batch:     5/   15, ite: 2810] train loss: 0.207268, tar: 0.021340 \n",
            "l0: 0.025056, l1: 0.026021, l2: 0.030019, l3: 0.029440, l4: 0.028143, l5: 0.035346, l6: 0.052548\n",
            "\n",
            "[epoch: 188/1000, batch:     6/   15, ite: 2811] train loss: 0.209023, tar: 0.021677 \n",
            "l0: 0.029122, l1: 0.035669, l2: 0.036917, l3: 0.031446, l4: 0.025682, l5: 0.026757, l6: 0.039710\n",
            "\n",
            "[epoch: 188/1000, batch:     7/   15, ite: 2812] train loss: 0.210380, tar: 0.022298 \n",
            "l0: 0.022746, l1: 0.025715, l2: 0.029660, l3: 0.025979, l4: 0.022395, l5: 0.026793, l6: 0.042062\n",
            "\n",
            "[epoch: 188/1000, batch:     8/   15, ite: 2813] train loss: 0.209223, tar: 0.022332 \n",
            "l0: 0.018612, l1: 0.020000, l2: 0.025201, l3: 0.021026, l4: 0.026785, l5: 0.026259, l6: 0.042321\n",
            "\n",
            "[epoch: 188/1000, batch:     9/   15, ite: 2814] train loss: 0.207151, tar: 0.022067 \n",
            "l0: 0.018291, l1: 0.017100, l2: 0.019830, l3: 0.023203, l4: 0.026498, l5: 0.032446, l6: 0.052097\n",
            "\n",
            "[epoch: 188/1000, batch:    10/   15, ite: 2815] train loss: 0.205972, tar: 0.021815 \n",
            "l0: 0.013068, l1: 0.013255, l2: 0.015636, l3: 0.016920, l4: 0.018988, l5: 0.027116, l6: 0.047291\n",
            "\n",
            "[epoch: 188/1000, batch:    11/   15, ite: 2816] train loss: 0.202615, tar: 0.021268 \n",
            "l0: 0.021633, l1: 0.026378, l2: 0.022831, l3: 0.024230, l4: 0.023866, l5: 0.027053, l6: 0.041612\n",
            "\n",
            "[epoch: 188/1000, batch:    12/   15, ite: 2817] train loss: 0.201732, tar: 0.021290 \n",
            "l0: 0.014934, l1: 0.014683, l2: 0.016567, l3: 0.020063, l4: 0.023897, l5: 0.034818, l6: 0.050252\n",
            "\n",
            "[epoch: 188/1000, batch:    13/   15, ite: 2818] train loss: 0.200259, tar: 0.020937 \n",
            "l0: 0.020400, l1: 0.021624, l2: 0.021046, l3: 0.025206, l4: 0.024566, l5: 0.034805, l6: 0.043997\n",
            "\n",
            "[epoch: 188/1000, batch:    14/   15, ite: 2819] train loss: 0.199806, tar: 0.020908 \n",
            "l0: 0.021871, l1: 0.025268, l2: 0.025361, l3: 0.026055, l4: 0.030296, l5: 0.031983, l6: 0.039846\n",
            "\n",
            "[epoch: 188/1000, batch:    15/   15, ite: 2820] train loss: 0.199849, tar: 0.020957 \n",
            "l0: 0.022785, l1: 0.026055, l2: 0.025237, l3: 0.025164, l4: 0.026400, l5: 0.035856, l6: 0.053975\n",
            "\n",
            "[epoch: 189/1000, batch:     1/   15, ite: 2821] train loss: 0.200593, tar: 0.021044 \n",
            "l0: 0.014794, l1: 0.014793, l2: 0.015922, l3: 0.017659, l4: 0.020555, l5: 0.027534, l6: 0.037950\n",
            "\n",
            "[epoch: 189/1000, batch:     2/   15, ite: 2822] train loss: 0.198258, tar: 0.020759 \n",
            "l0: 0.013376, l1: 0.012810, l2: 0.014843, l3: 0.018044, l4: 0.018658, l5: 0.021148, l6: 0.029658\n",
            "\n",
            "[epoch: 189/1000, batch:     3/   15, ite: 2823] train loss: 0.195226, tar: 0.020438 \n",
            "l0: 0.027748, l1: 0.028799, l2: 0.030564, l3: 0.034230, l4: 0.042584, l5: 0.048476, l6: 0.063999\n",
            "\n",
            "[epoch: 189/1000, batch:     4/   15, ite: 2824] train loss: 0.198608, tar: 0.020743 \n",
            "l0: 0.020569, l1: 0.018091, l2: 0.022198, l3: 0.026729, l4: 0.027910, l5: 0.040648, l6: 0.055896\n",
            "\n",
            "[epoch: 189/1000, batch:     5/   15, ite: 2825] train loss: 0.199146, tar: 0.020736 \n",
            "l0: 0.016454, l1: 0.016573, l2: 0.018371, l3: 0.021914, l4: 0.022190, l5: 0.026350, l6: 0.045818\n",
            "\n",
            "[epoch: 189/1000, batch:     6/   15, ite: 2826] train loss: 0.197935, tar: 0.020571 \n",
            "l0: 0.016681, l1: 0.016494, l2: 0.017323, l3: 0.020513, l4: 0.022660, l5: 0.027749, l6: 0.044330\n",
            "\n",
            "[epoch: 189/1000, batch:     7/   15, ite: 2827] train loss: 0.196743, tar: 0.020427 \n",
            "l0: 0.019983, l1: 0.021158, l2: 0.022569, l3: 0.025682, l4: 0.019516, l5: 0.027602, l6: 0.040796\n",
            "\n",
            "[epoch: 189/1000, batch:     8/   15, ite: 2828] train loss: 0.196049, tar: 0.020411 \n",
            "l0: 0.022352, l1: 0.021510, l2: 0.029354, l3: 0.040164, l4: 0.043563, l5: 0.029200, l6: 0.041477\n",
            "\n",
            "[epoch: 189/1000, batch:     9/   15, ite: 2829] train loss: 0.197138, tar: 0.020478 \n",
            "l0: 0.017714, l1: 0.018772, l2: 0.018455, l3: 0.022109, l4: 0.027321, l5: 0.033614, l6: 0.062824\n",
            "\n",
            "[epoch: 189/1000, batch:    10/   15, ite: 2830] train loss: 0.197260, tar: 0.020386 \n",
            "l0: 0.019055, l1: 0.023395, l2: 0.023176, l3: 0.027101, l4: 0.027262, l5: 0.028457, l6: 0.044639\n",
            "\n",
            "[epoch: 189/1000, batch:    11/   15, ite: 2831] train loss: 0.197125, tar: 0.020343 \n",
            "l0: 0.017811, l1: 0.019528, l2: 0.018624, l3: 0.024118, l4: 0.026417, l5: 0.030635, l6: 0.041389\n",
            "\n",
            "[epoch: 189/1000, batch:    12/   15, ite: 2832] train loss: 0.196544, tar: 0.020264 \n",
            "l0: 0.022467, l1: 0.026381, l2: 0.027866, l3: 0.031308, l4: 0.031671, l5: 0.033500, l6: 0.053193\n",
            "\n",
            "[epoch: 189/1000, batch:    13/   15, ite: 2833] train loss: 0.197448, tar: 0.020331 \n",
            "l0: 0.016308, l1: 0.016178, l2: 0.018318, l3: 0.022303, l4: 0.033816, l5: 0.038942, l6: 0.053203\n",
            "\n",
            "[epoch: 189/1000, batch:    14/   15, ite: 2834] train loss: 0.197496, tar: 0.020213 \n",
            "l0: 0.019763, l1: 0.021305, l2: 0.021199, l3: 0.022440, l4: 0.024851, l5: 0.025166, l6: 0.033166\n",
            "\n",
            "[epoch: 189/1000, batch:    15/   15, ite: 2835] train loss: 0.196650, tar: 0.020200 \n",
            "l0: 0.014622, l1: 0.014444, l2: 0.015631, l3: 0.017380, l4: 0.020598, l5: 0.027525, l6: 0.042791\n",
            "\n",
            "[epoch: 190/1000, batch:     1/   15, ite: 2836] train loss: 0.195437, tar: 0.020045 \n",
            "l0: 0.016479, l1: 0.015864, l2: 0.017995, l3: 0.022655, l4: 0.033311, l5: 0.029108, l6: 0.044187\n",
            "\n",
            "[epoch: 190/1000, batch:     2/   15, ite: 2837] train loss: 0.195009, tar: 0.019948 \n",
            "l0: 0.014339, l1: 0.012705, l2: 0.013819, l3: 0.017757, l4: 0.022525, l5: 0.024356, l6: 0.039665\n",
            "\n",
            "[epoch: 190/1000, batch:     3/   15, ite: 2838] train loss: 0.193697, tar: 0.019801 \n",
            "l0: 0.017999, l1: 0.020037, l2: 0.018840, l3: 0.019431, l4: 0.021227, l5: 0.022086, l6: 0.033452\n",
            "\n",
            "[epoch: 190/1000, batch:     4/   15, ite: 2839] train loss: 0.192656, tar: 0.019755 \n",
            "l0: 0.014026, l1: 0.014038, l2: 0.015445, l3: 0.016391, l4: 0.022697, l5: 0.020269, l6: 0.037163\n",
            "\n",
            "[epoch: 190/1000, batch:     5/   15, ite: 2840] train loss: 0.191340, tar: 0.019611 \n",
            "l0: 0.015352, l1: 0.015041, l2: 0.016734, l3: 0.018572, l4: 0.025388, l5: 0.033415, l6: 0.045197\n",
            "\n",
            "[epoch: 190/1000, batch:     6/   15, ite: 2841] train loss: 0.190812, tar: 0.019507 \n",
            "l0: 0.025206, l1: 0.026649, l2: 0.027932, l3: 0.029653, l4: 0.028626, l5: 0.029284, l6: 0.037037\n",
            "\n",
            "[epoch: 190/1000, batch:     7/   15, ite: 2842] train loss: 0.191136, tar: 0.019643 \n",
            "l0: 0.020961, l1: 0.020701, l2: 0.022294, l3: 0.029290, l4: 0.033535, l5: 0.033822, l6: 0.062222\n",
            "\n",
            "[epoch: 190/1000, batch:     8/   15, ite: 2843] train loss: 0.191872, tar: 0.019674 \n",
            "l0: 0.024034, l1: 0.025023, l2: 0.026591, l3: 0.029841, l4: 0.030932, l5: 0.033034, l6: 0.052307\n",
            "\n",
            "[epoch: 190/1000, batch:     9/   15, ite: 2844] train loss: 0.192552, tar: 0.019773 \n",
            "l0: 0.013492, l1: 0.014761, l2: 0.017457, l3: 0.018986, l4: 0.020280, l5: 0.027829, l6: 0.041572\n",
            "\n",
            "[epoch: 190/1000, batch:    10/   15, ite: 2845] train loss: 0.191703, tar: 0.019633 \n",
            "l0: 0.018933, l1: 0.020463, l2: 0.023077, l3: 0.024069, l4: 0.025189, l5: 0.026652, l6: 0.032716\n",
            "\n",
            "[epoch: 190/1000, batch:    11/   15, ite: 2846] train loss: 0.191256, tar: 0.019618 \n",
            "l0: 0.013125, l1: 0.013153, l2: 0.016269, l3: 0.019498, l4: 0.021755, l5: 0.025565, l6: 0.039389\n",
            "\n",
            "[epoch: 190/1000, batch:    12/   15, ite: 2847] train loss: 0.190351, tar: 0.019480 \n",
            "l0: 0.014473, l1: 0.014181, l2: 0.016853, l3: 0.017924, l4: 0.019841, l5: 0.023368, l6: 0.041653\n",
            "\n",
            "[epoch: 190/1000, batch:    13/   15, ite: 2848] train loss: 0.189475, tar: 0.019376 \n",
            "l0: 0.022202, l1: 0.026460, l2: 0.028846, l3: 0.027932, l4: 0.023249, l5: 0.024082, l6: 0.035345\n",
            "\n",
            "[epoch: 190/1000, batch:    14/   15, ite: 2849] train loss: 0.189447, tar: 0.019433 \n",
            "l0: 0.013131, l1: 0.013888, l2: 0.015011, l3: 0.019852, l4: 0.021054, l5: 0.020803, l6: 0.033501\n",
            "\n",
            "[epoch: 190/1000, batch:    15/   15, ite: 2850] train loss: 0.188403, tar: 0.019307 \n",
            "l0: 0.016682, l1: 0.016165, l2: 0.019768, l3: 0.026785, l4: 0.028360, l5: 0.024643, l6: 0.034989\n",
            "\n",
            "[epoch: 191/1000, batch:     1/   15, ite: 2851] train loss: 0.187991, tar: 0.019256 \n",
            "l0: 0.017171, l1: 0.016315, l2: 0.019382, l3: 0.021161, l4: 0.026364, l5: 0.032365, l6: 0.046713\n",
            "\n",
            "[epoch: 191/1000, batch:     2/   15, ite: 2852] train loss: 0.187827, tar: 0.019216 \n",
            "l0: 0.012371, l1: 0.012124, l2: 0.012930, l3: 0.015567, l4: 0.017762, l5: 0.023276, l6: 0.044066\n",
            "\n",
            "[epoch: 191/1000, batch:     3/   15, ite: 2853] train loss: 0.186889, tar: 0.019087 \n",
            "l0: 0.019731, l1: 0.020064, l2: 0.021138, l3: 0.021949, l4: 0.028140, l5: 0.038398, l6: 0.051124\n",
            "\n",
            "[epoch: 191/1000, batch:     4/   15, ite: 2854] train loss: 0.187142, tar: 0.019099 \n",
            "l0: 0.014353, l1: 0.014043, l2: 0.016477, l3: 0.019620, l4: 0.026182, l5: 0.028999, l6: 0.053716\n",
            "\n",
            "[epoch: 191/1000, batch:     5/   15, ite: 2855] train loss: 0.186892, tar: 0.019012 \n",
            "l0: 0.021068, l1: 0.023181, l2: 0.020503, l3: 0.024230, l4: 0.023121, l5: 0.028678, l6: 0.052394\n",
            "\n",
            "[epoch: 191/1000, batch:     6/   15, ite: 2856] train loss: 0.187004, tar: 0.019049 \n",
            "l0: 0.013620, l1: 0.012997, l2: 0.015903, l3: 0.018712, l4: 0.019069, l5: 0.024462, l6: 0.037420\n",
            "\n",
            "[epoch: 191/1000, batch:     7/   15, ite: 2857] train loss: 0.186218, tar: 0.018954 \n",
            "l0: 0.014082, l1: 0.014198, l2: 0.018098, l3: 0.018894, l4: 0.020563, l5: 0.024149, l6: 0.043116\n",
            "\n",
            "[epoch: 191/1000, batch:     8/   15, ite: 2858] train loss: 0.185647, tar: 0.018870 \n",
            "l0: 0.013964, l1: 0.014064, l2: 0.017536, l3: 0.018280, l4: 0.020669, l5: 0.026475, l6: 0.046023\n",
            "\n",
            "[epoch: 191/1000, batch:     9/   15, ite: 2859] train loss: 0.185161, tar: 0.018787 \n",
            "l0: 0.022013, l1: 0.022589, l2: 0.024345, l3: 0.027963, l4: 0.028012, l5: 0.033803, l6: 0.047916\n",
            "\n",
            "[epoch: 191/1000, batch:    10/   15, ite: 2860] train loss: 0.185519, tar: 0.018840 \n",
            "l0: 0.010866, l1: 0.010660, l2: 0.012314, l3: 0.014608, l4: 0.016835, l5: 0.022076, l6: 0.026715\n",
            "\n",
            "[epoch: 191/1000, batch:    11/   15, ite: 2861] train loss: 0.184348, tar: 0.018710 \n",
            "l0: 0.012958, l1: 0.012889, l2: 0.014087, l3: 0.016283, l4: 0.020699, l5: 0.027609, l6: 0.041143\n",
            "\n",
            "[epoch: 191/1000, batch:    12/   15, ite: 2862] train loss: 0.183724, tar: 0.018617 \n",
            "l0: 0.015290, l1: 0.014464, l2: 0.016943, l3: 0.020411, l4: 0.025245, l5: 0.028624, l6: 0.037663\n",
            "\n",
            "[epoch: 191/1000, batch:    13/   15, ite: 2863] train loss: 0.183326, tar: 0.018564 \n",
            "l0: 0.013547, l1: 0.012356, l2: 0.015274, l3: 0.020242, l4: 0.023236, l5: 0.032471, l6: 0.048421\n",
            "\n",
            "[epoch: 191/1000, batch:    14/   15, ite: 2864] train loss: 0.183048, tar: 0.018486 \n",
            "l0: 0.014154, l1: 0.014483, l2: 0.015496, l3: 0.017287, l4: 0.019199, l5: 0.030464, l6: 0.043005\n",
            "\n",
            "[epoch: 191/1000, batch:    15/   15, ite: 2865] train loss: 0.182603, tar: 0.018419 \n",
            "l0: 0.011881, l1: 0.014070, l2: 0.012996, l3: 0.013679, l4: 0.017151, l5: 0.020795, l6: 0.030840\n",
            "\n",
            "[epoch: 192/1000, batch:     1/   15, ite: 2866] train loss: 0.181676, tar: 0.018320 \n",
            "l0: 0.016737, l1: 0.017660, l2: 0.018260, l3: 0.020743, l4: 0.021852, l5: 0.032679, l6: 0.058244\n",
            "\n",
            "[epoch: 192/1000, batch:     2/   15, ite: 2867] train loss: 0.181743, tar: 0.018296 \n",
            "l0: 0.012844, l1: 0.013044, l2: 0.013962, l3: 0.014788, l4: 0.016467, l5: 0.021090, l6: 0.035953\n",
            "\n",
            "[epoch: 192/1000, batch:     3/   15, ite: 2868] train loss: 0.180955, tar: 0.018216 \n",
            "l0: 0.014751, l1: 0.016556, l2: 0.016149, l3: 0.018865, l4: 0.020325, l5: 0.025004, l6: 0.057634\n",
            "\n",
            "[epoch: 192/1000, batch:     4/   15, ite: 2869] train loss: 0.180785, tar: 0.018166 \n",
            "l0: 0.018495, l1: 0.018218, l2: 0.019366, l3: 0.020192, l4: 0.024767, l5: 0.032454, l6: 0.048895\n",
            "\n",
            "[epoch: 192/1000, batch:     5/   15, ite: 2870] train loss: 0.180808, tar: 0.018171 \n",
            "l0: 0.013658, l1: 0.014515, l2: 0.015632, l3: 0.016202, l4: 0.018389, l5: 0.022054, l6: 0.035438\n",
            "\n",
            "[epoch: 192/1000, batch:     6/   15, ite: 2871] train loss: 0.180176, tar: 0.018107 \n",
            "l0: 0.013020, l1: 0.013138, l2: 0.013897, l3: 0.016204, l4: 0.019813, l5: 0.024723, l6: 0.037197\n",
            "\n",
            "[epoch: 192/1000, batch:     7/   15, ite: 2872] train loss: 0.179590, tar: 0.018036 \n",
            "l0: 0.017233, l1: 0.015507, l2: 0.019752, l3: 0.023281, l4: 0.029346, l5: 0.033017, l6: 0.046691\n",
            "\n",
            "[epoch: 192/1000, batch:     8/   15, ite: 2873] train loss: 0.179661, tar: 0.018025 \n",
            "l0: 0.021047, l1: 0.021892, l2: 0.022559, l3: 0.022784, l4: 0.025995, l5: 0.029393, l6: 0.045449\n",
            "\n",
            "[epoch: 192/1000, batch:     9/   15, ite: 2874] train loss: 0.179789, tar: 0.018066 \n",
            "l0: 0.015733, l1: 0.013722, l2: 0.018343, l3: 0.022367, l4: 0.022278, l5: 0.028930, l6: 0.050148\n",
            "\n",
            "[epoch: 192/1000, batch:    10/   15, ite: 2875] train loss: 0.179679, tar: 0.018035 \n",
            "l0: 0.013589, l1: 0.012618, l2: 0.015252, l3: 0.017878, l4: 0.019627, l5: 0.025096, l6: 0.039702\n",
            "\n",
            "[epoch: 192/1000, batch:    11/   15, ite: 2876] train loss: 0.179206, tar: 0.017977 \n",
            "l0: 0.015680, l1: 0.015608, l2: 0.017009, l3: 0.018596, l4: 0.020236, l5: 0.025326, l6: 0.049646\n",
            "\n",
            "[epoch: 192/1000, batch:    12/   15, ite: 2877] train loss: 0.178984, tar: 0.017947 \n",
            "l0: 0.013836, l1: 0.013875, l2: 0.015323, l3: 0.018895, l4: 0.020540, l5: 0.022305, l6: 0.031931\n",
            "\n",
            "[epoch: 192/1000, batch:    13/   15, ite: 2878] train loss: 0.178442, tar: 0.017894 \n",
            "l0: 0.012414, l1: 0.014534, l2: 0.015162, l3: 0.017544, l4: 0.018493, l5: 0.025100, l6: 0.041174\n",
            "\n",
            "[epoch: 192/1000, batch:    14/   15, ite: 2879] train loss: 0.178012, tar: 0.017825 \n",
            "l0: 0.021483, l1: 0.023552, l2: 0.025573, l3: 0.023032, l4: 0.025893, l5: 0.031468, l6: 0.044337\n",
            "\n",
            "[epoch: 192/1000, batch:    15/   15, ite: 2880] train loss: 0.178228, tar: 0.017870 \n",
            "l0: 0.016819, l1: 0.016471, l2: 0.019047, l3: 0.022412, l4: 0.023055, l5: 0.027873, l6: 0.041196\n",
            "\n",
            "[epoch: 193/1000, batch:     1/   15, ite: 2881] train loss: 0.178088, tar: 0.017857 \n",
            "l0: 0.011836, l1: 0.011409, l2: 0.013365, l3: 0.015387, l4: 0.017376, l5: 0.021955, l6: 0.034213\n",
            "\n",
            "[epoch: 193/1000, batch:     2/   15, ite: 2882] train loss: 0.177447, tar: 0.017784 \n",
            "l0: 0.010486, l1: 0.009345, l2: 0.012138, l3: 0.014741, l4: 0.022425, l5: 0.024204, l6: 0.039512\n",
            "\n",
            "[epoch: 193/1000, batch:     3/   15, ite: 2883] train loss: 0.176910, tar: 0.017696 \n",
            "l0: 0.015282, l1: 0.014919, l2: 0.017671, l3: 0.018000, l4: 0.022930, l5: 0.027666, l6: 0.046786\n",
            "\n",
            "[epoch: 193/1000, batch:     4/   15, ite: 2884] train loss: 0.176747, tar: 0.017667 \n",
            "l0: 0.015978, l1: 0.019415, l2: 0.017773, l3: 0.018426, l4: 0.016863, l5: 0.021748, l6: 0.035304\n",
            "\n",
            "[epoch: 193/1000, batch:     5/   15, ite: 2885] train loss: 0.176380, tar: 0.017647 \n",
            "l0: 0.014685, l1: 0.013470, l2: 0.015789, l3: 0.016941, l4: 0.022146, l5: 0.029925, l6: 0.038246\n",
            "\n",
            "[epoch: 193/1000, batch:     6/   15, ite: 2886] train loss: 0.176087, tar: 0.017613 \n",
            "l0: 0.015640, l1: 0.013817, l2: 0.017614, l3: 0.020600, l4: 0.023729, l5: 0.026950, l6: 0.051780\n",
            "\n",
            "[epoch: 193/1000, batch:     7/   15, ite: 2887] train loss: 0.176018, tar: 0.017590 \n",
            "l0: 0.012829, l1: 0.011518, l2: 0.014797, l3: 0.016597, l4: 0.021272, l5: 0.023778, l6: 0.039699\n",
            "\n",
            "[epoch: 193/1000, batch:     8/   15, ite: 2888] train loss: 0.175615, tar: 0.017536 \n",
            "l0: 0.024875, l1: 0.024613, l2: 0.025648, l3: 0.031725, l4: 0.030902, l5: 0.034681, l6: 0.058150\n",
            "\n",
            "[epoch: 193/1000, batch:     9/   15, ite: 2889] train loss: 0.176233, tar: 0.017619 \n",
            "l0: 0.025145, l1: 0.024151, l2: 0.026670, l3: 0.028352, l4: 0.026510, l5: 0.035051, l6: 0.056705\n",
            "\n",
            "[epoch: 193/1000, batch:    10/   15, ite: 2890] train loss: 0.176748, tar: 0.017702 \n",
            "l0: 0.014644, l1: 0.014787, l2: 0.016509, l3: 0.018951, l4: 0.021830, l5: 0.026208, l6: 0.056400\n",
            "\n",
            "[epoch: 193/1000, batch:    11/   15, ite: 2891] train loss: 0.176666, tar: 0.017669 \n",
            "l0: 0.017745, l1: 0.016037, l2: 0.019131, l3: 0.023456, l4: 0.020111, l5: 0.025912, l6: 0.041952\n",
            "\n",
            "[epoch: 193/1000, batch:    12/   15, ite: 2892] train loss: 0.176532, tar: 0.017670 \n",
            "l0: 0.031474, l1: 0.029603, l2: 0.032627, l3: 0.034435, l4: 0.030979, l5: 0.037725, l6: 0.044212\n",
            "\n",
            "[epoch: 193/1000, batch:    13/   15, ite: 2893] train loss: 0.177226, tar: 0.017818 \n",
            "l0: 0.011818, l1: 0.011321, l2: 0.013100, l3: 0.014599, l4: 0.017113, l5: 0.024214, l6: 0.036202\n",
            "\n",
            "[epoch: 193/1000, batch:    14/   15, ite: 2894] train loss: 0.176706, tar: 0.017754 \n",
            "l0: 0.016861, l1: 0.018381, l2: 0.020477, l3: 0.018942, l4: 0.022584, l5: 0.024977, l6: 0.035443\n",
            "\n",
            "[epoch: 193/1000, batch:    15/   15, ite: 2895] train loss: 0.176506, tar: 0.017745 \n",
            "l0: 0.018859, l1: 0.019026, l2: 0.019673, l3: 0.022751, l4: 0.026113, l5: 0.034282, l6: 0.053586\n",
            "\n",
            "[epoch: 194/1000, batch:     1/   15, ite: 2896] train loss: 0.176691, tar: 0.017756 \n",
            "l0: 0.018838, l1: 0.018016, l2: 0.020371, l3: 0.024034, l4: 0.027628, l5: 0.031116, l6: 0.046253\n",
            "\n",
            "[epoch: 194/1000, batch:     2/   15, ite: 2897] train loss: 0.176790, tar: 0.017768 \n",
            "l0: 0.024587, l1: 0.021204, l2: 0.027202, l3: 0.029550, l4: 0.037845, l5: 0.052912, l6: 0.067127\n",
            "\n",
            "[epoch: 194/1000, batch:     3/   15, ite: 2898] train loss: 0.177643, tar: 0.017837 \n",
            "l0: 0.019391, l1: 0.020385, l2: 0.023252, l3: 0.020355, l4: 0.023504, l5: 0.031759, l6: 0.043411\n",
            "\n",
            "[epoch: 194/1000, batch:     4/   15, ite: 2899] train loss: 0.177688, tar: 0.017853 \n",
            "l0: 0.014505, l1: 0.016421, l2: 0.016522, l3: 0.019085, l4: 0.023475, l5: 0.026880, l6: 0.054841\n",
            "\n",
            "[epoch: 194/1000, batch:     5/   15, ite: 2900] train loss: 0.177628, tar: 0.017819 \n",
            "l0: 0.013307, l1: 0.014578, l2: 0.015067, l3: 0.017358, l4: 0.018349, l5: 0.026239, l6: 0.038445\n",
            "\n",
            "[epoch: 194/1000, batch:     6/   15, ite: 2901] train loss: 0.143343, tar: 0.013307 \n",
            "l0: 0.016672, l1: 0.017700, l2: 0.019385, l3: 0.020507, l4: 0.021552, l5: 0.031352, l6: 0.051317\n",
            "\n",
            "[epoch: 194/1000, batch:     7/   15, ite: 2902] train loss: 0.160914, tar: 0.014989 \n",
            "l0: 0.019988, l1: 0.019015, l2: 0.021878, l3: 0.025247, l4: 0.029603, l5: 0.039103, l6: 0.052184\n",
            "\n",
            "[epoch: 194/1000, batch:     8/   15, ite: 2903] train loss: 0.176282, tar: 0.016656 \n",
            "l0: 0.013082, l1: 0.012213, l2: 0.015773, l3: 0.018197, l4: 0.017510, l5: 0.023071, l6: 0.041458\n",
            "\n",
            "[epoch: 194/1000, batch:     9/   15, ite: 2904] train loss: 0.167537, tar: 0.015762 \n",
            "l0: 0.012201, l1: 0.011858, l2: 0.013473, l3: 0.013850, l4: 0.018026, l5: 0.024533, l6: 0.038718\n",
            "\n",
            "[epoch: 194/1000, batch:    10/   15, ite: 2905] train loss: 0.160561, tar: 0.015050 \n",
            "l0: 0.010711, l1: 0.010127, l2: 0.011923, l3: 0.014665, l4: 0.017257, l5: 0.022802, l6: 0.039511\n",
            "\n",
            "[epoch: 194/1000, batch:    11/   15, ite: 2906] train loss: 0.154967, tar: 0.014327 \n",
            "l0: 0.011476, l1: 0.010872, l2: 0.013334, l3: 0.014499, l4: 0.017039, l5: 0.021697, l6: 0.038389\n",
            "\n",
            "[epoch: 194/1000, batch:    12/   15, ite: 2907] train loss: 0.151015, tar: 0.013919 \n",
            "l0: 0.012883, l1: 0.013366, l2: 0.014443, l3: 0.016639, l4: 0.017579, l5: 0.023436, l6: 0.036684\n",
            "\n",
            "[epoch: 194/1000, batch:    13/   15, ite: 2908] train loss: 0.149017, tar: 0.013790 \n",
            "l0: 0.016781, l1: 0.018133, l2: 0.018358, l3: 0.020873, l4: 0.021241, l5: 0.024985, l6: 0.035021\n",
            "\n",
            "[epoch: 194/1000, batch:    14/   15, ite: 2909] train loss: 0.149726, tar: 0.014122 \n",
            "l0: 0.017120, l1: 0.016740, l2: 0.020879, l3: 0.020752, l4: 0.022922, l5: 0.027408, l6: 0.036455\n",
            "\n",
            "[epoch: 194/1000, batch:    15/   15, ite: 2910] train loss: 0.150981, tar: 0.014422 \n",
            "l0: 0.016626, l1: 0.016244, l2: 0.016974, l3: 0.021219, l4: 0.022325, l5: 0.024558, l6: 0.037744\n",
            "\n",
            "[epoch: 195/1000, batch:     1/   15, ite: 2911] train loss: 0.151409, tar: 0.014622 \n",
            "l0: 0.015023, l1: 0.013650, l2: 0.015790, l3: 0.020738, l4: 0.023821, l5: 0.027032, l6: 0.050895\n",
            "\n",
            "[epoch: 195/1000, batch:     2/   15, ite: 2912] train loss: 0.152704, tar: 0.014656 \n",
            "l0: 0.012441, l1: 0.011925, l2: 0.011796, l3: 0.013861, l4: 0.020791, l5: 0.028044, l6: 0.043574\n",
            "\n",
            "[epoch: 195/1000, batch:     3/   15, ite: 2913] train loss: 0.151914, tar: 0.014485 \n",
            "l0: 0.020123, l1: 0.019966, l2: 0.022596, l3: 0.025039, l4: 0.029506, l5: 0.033349, l6: 0.052662\n",
            "\n",
            "[epoch: 195/1000, batch:     4/   15, ite: 2914] train loss: 0.155580, tar: 0.014888 \n",
            "l0: 0.015773, l1: 0.015165, l2: 0.016426, l3: 0.019325, l4: 0.020853, l5: 0.030989, l6: 0.051249\n",
            "\n",
            "[epoch: 195/1000, batch:     5/   15, ite: 2915] train loss: 0.156527, tar: 0.014947 \n",
            "l0: 0.013556, l1: 0.013292, l2: 0.015847, l3: 0.017672, l4: 0.023080, l5: 0.028836, l6: 0.045779\n",
            "\n",
            "[epoch: 195/1000, batch:     6/   15, ite: 2916] train loss: 0.156622, tar: 0.014860 \n",
            "l0: 0.013655, l1: 0.012648, l2: 0.016188, l3: 0.018065, l4: 0.020368, l5: 0.021795, l6: 0.031144\n",
            "\n",
            "[epoch: 195/1000, batch:     7/   15, ite: 2917] train loss: 0.155284, tar: 0.014789 \n",
            "l0: 0.016301, l1: 0.015962, l2: 0.019844, l3: 0.019012, l4: 0.022090, l5: 0.028900, l6: 0.042481\n",
            "\n",
            "[epoch: 195/1000, batch:     8/   15, ite: 2918] train loss: 0.155801, tar: 0.014873 \n",
            "l0: 0.011947, l1: 0.011556, l2: 0.014338, l3: 0.019476, l4: 0.018823, l5: 0.025587, l6: 0.050980\n",
            "\n",
            "[epoch: 195/1000, batch:     9/   15, ite: 2919] train loss: 0.155638, tar: 0.014719 \n",
            "l0: 0.013488, l1: 0.013739, l2: 0.014860, l3: 0.015870, l4: 0.018961, l5: 0.025485, l6: 0.036532\n",
            "\n",
            "[epoch: 195/1000, batch:    10/   15, ite: 2920] train loss: 0.154803, tar: 0.014658 \n",
            "l0: 0.018570, l1: 0.018017, l2: 0.019841, l3: 0.025562, l4: 0.025588, l5: 0.029000, l6: 0.043991\n",
            "\n",
            "[epoch: 195/1000, batch:    11/   15, ite: 2921] train loss: 0.156030, tar: 0.014844 \n",
            "l0: 0.017262, l1: 0.018554, l2: 0.020137, l3: 0.018043, l4: 0.021332, l5: 0.023481, l6: 0.038683\n",
            "\n",
            "[epoch: 195/1000, batch:    12/   15, ite: 2922] train loss: 0.156096, tar: 0.014954 \n",
            "l0: 0.012206, l1: 0.012779, l2: 0.013689, l3: 0.015935, l4: 0.016223, l5: 0.019186, l6: 0.027084\n",
            "\n",
            "[epoch: 195/1000, batch:    13/   15, ite: 2923] train loss: 0.154401, tar: 0.014834 \n",
            "l0: 0.010453, l1: 0.010803, l2: 0.012116, l3: 0.012924, l4: 0.015047, l5: 0.017387, l6: 0.035327\n",
            "\n",
            "[epoch: 195/1000, batch:    14/   15, ite: 2924] train loss: 0.152720, tar: 0.014652 \n",
            "l0: 0.010419, l1: 0.010889, l2: 0.011319, l3: 0.012824, l4: 0.013647, l5: 0.017408, l6: 0.027289\n",
            "\n",
            "[epoch: 195/1000, batch:    15/   15, ite: 2925] train loss: 0.150763, tar: 0.014483 \n",
            "l0: 0.027273, l1: 0.025290, l2: 0.029777, l3: 0.031958, l4: 0.032004, l5: 0.041103, l6: 0.052114\n",
            "\n",
            "[epoch: 196/1000, batch:     1/   15, ite: 2926] train loss: 0.154176, tar: 0.014974 \n",
            "l0: 0.012869, l1: 0.012618, l2: 0.014122, l3: 0.015906, l4: 0.017490, l5: 0.023401, l6: 0.033306\n",
            "\n",
            "[epoch: 196/1000, batch:     2/   15, ite: 2927] train loss: 0.153270, tar: 0.014896 \n",
            "l0: 0.011713, l1: 0.011999, l2: 0.013652, l3: 0.015275, l4: 0.016292, l5: 0.019000, l6: 0.038825\n",
            "\n",
            "[epoch: 196/1000, batch:     3/   15, ite: 2928] train loss: 0.152323, tar: 0.014783 \n",
            "l0: 0.011721, l1: 0.011076, l2: 0.014108, l3: 0.016189, l4: 0.017858, l5: 0.022687, l6: 0.043570\n",
            "\n",
            "[epoch: 196/1000, batch:     4/   15, ite: 2929] train loss: 0.151802, tar: 0.014677 \n",
            "l0: 0.021580, l1: 0.018958, l2: 0.022743, l3: 0.028954, l4: 0.027653, l5: 0.031590, l6: 0.041165\n",
            "\n",
            "[epoch: 196/1000, batch:     5/   15, ite: 2930] train loss: 0.153163, tar: 0.014907 \n",
            "l0: 0.024256, l1: 0.028365, l2: 0.028751, l3: 0.026153, l4: 0.027098, l5: 0.027143, l6: 0.040185\n",
            "\n",
            "[epoch: 196/1000, batch:     6/   15, ite: 2931] train loss: 0.154737, tar: 0.015209 \n",
            "l0: 0.012520, l1: 0.012647, l2: 0.014956, l3: 0.015819, l4: 0.018774, l5: 0.025136, l6: 0.045098\n",
            "\n",
            "[epoch: 196/1000, batch:     7/   15, ite: 2932] train loss: 0.154431, tar: 0.015125 \n",
            "l0: 0.019956, l1: 0.019432, l2: 0.020695, l3: 0.022463, l4: 0.025289, l5: 0.029968, l6: 0.062794\n",
            "\n",
            "[epoch: 196/1000, batch:     8/   15, ite: 2933] train loss: 0.155830, tar: 0.015271 \n",
            "l0: 0.013710, l1: 0.016435, l2: 0.014971, l3: 0.015921, l4: 0.018726, l5: 0.021033, l6: 0.031253\n",
            "\n",
            "[epoch: 196/1000, batch:     9/   15, ite: 2934] train loss: 0.155131, tar: 0.015225 \n",
            "l0: 0.010221, l1: 0.010086, l2: 0.011455, l3: 0.012940, l4: 0.016977, l5: 0.022596, l6: 0.034032\n",
            "\n",
            "[epoch: 196/1000, batch:    10/   15, ite: 2935] train loss: 0.154079, tar: 0.015082 \n",
            "l0: 0.015144, l1: 0.013475, l2: 0.017383, l3: 0.020539, l4: 0.022178, l5: 0.026834, l6: 0.041771\n",
            "\n",
            "[epoch: 196/1000, batch:    11/   15, ite: 2936] train loss: 0.154169, tar: 0.015084 \n",
            "l0: 0.011157, l1: 0.011498, l2: 0.011498, l3: 0.013161, l4: 0.014638, l5: 0.018417, l6: 0.024161\n",
            "\n",
            "[epoch: 196/1000, batch:    12/   15, ite: 2937] train loss: 0.152827, tar: 0.014978 \n",
            "l0: 0.013037, l1: 0.013970, l2: 0.014941, l3: 0.015855, l4: 0.017969, l5: 0.021212, l6: 0.043975\n",
            "\n",
            "[epoch: 196/1000, batch:    13/   15, ite: 2938] train loss: 0.152515, tar: 0.014927 \n",
            "l0: 0.011283, l1: 0.011117, l2: 0.012264, l3: 0.013460, l4: 0.015631, l5: 0.019142, l6: 0.035436\n",
            "\n",
            "[epoch: 196/1000, batch:    14/   15, ite: 2939] train loss: 0.151639, tar: 0.014833 \n",
            "l0: 0.015085, l1: 0.014889, l2: 0.016426, l3: 0.017863, l4: 0.018579, l5: 0.022490, l6: 0.041054\n",
            "\n",
            "[epoch: 196/1000, batch:    15/   15, ite: 2940] train loss: 0.151507, tar: 0.014840 \n",
            "l0: 0.011695, l1: 0.011503, l2: 0.012935, l3: 0.014992, l4: 0.016697, l5: 0.020106, l6: 0.039087\n",
            "\n",
            "[epoch: 197/1000, batch:     1/   15, ite: 2941] train loss: 0.150910, tar: 0.014763 \n",
            "l0: 0.014650, l1: 0.014900, l2: 0.017188, l3: 0.019038, l4: 0.019395, l5: 0.023586, l6: 0.052076\n",
            "\n",
            "[epoch: 197/1000, batch:     2/   15, ite: 2942] train loss: 0.151146, tar: 0.014760 \n",
            "l0: 0.010727, l1: 0.009885, l2: 0.011859, l3: 0.015400, l4: 0.015979, l5: 0.020324, l6: 0.035811\n",
            "\n",
            "[epoch: 197/1000, batch:     3/   15, ite: 2943] train loss: 0.150421, tar: 0.014667 \n",
            "l0: 0.019890, l1: 0.018629, l2: 0.023367, l3: 0.022238, l4: 0.025575, l5: 0.029279, l6: 0.037160\n",
            "\n",
            "[epoch: 197/1000, batch:     4/   15, ite: 2944] train loss: 0.151006, tar: 0.014785 \n",
            "l0: 0.011512, l1: 0.011432, l2: 0.013753, l3: 0.016466, l4: 0.015686, l5: 0.020309, l6: 0.036002\n",
            "\n",
            "[epoch: 197/1000, batch:     5/   15, ite: 2945] train loss: 0.150432, tar: 0.014712 \n",
            "l0: 0.012111, l1: 0.012820, l2: 0.014362, l3: 0.014029, l4: 0.015662, l5: 0.020027, l6: 0.035737\n",
            "\n",
            "[epoch: 197/1000, batch:     6/   15, ite: 2946] train loss: 0.149873, tar: 0.014656 \n",
            "l0: 0.013111, l1: 0.013580, l2: 0.015061, l3: 0.016159, l4: 0.016031, l5: 0.018847, l6: 0.027351\n",
            "\n",
            "[epoch: 197/1000, batch:     7/   15, ite: 2947] train loss: 0.149241, tar: 0.014623 \n",
            "l0: 0.023887, l1: 0.023525, l2: 0.024066, l3: 0.030255, l4: 0.029771, l5: 0.032123, l6: 0.061116\n",
            "\n",
            "[epoch: 197/1000, batch:     8/   15, ite: 2948] train loss: 0.150814, tar: 0.014816 \n",
            "l0: 0.014692, l1: 0.013836, l2: 0.016717, l3: 0.019154, l4: 0.021201, l5: 0.023375, l6: 0.029444\n",
            "\n",
            "[epoch: 197/1000, batch:     9/   15, ite: 2949] train loss: 0.150561, tar: 0.014814 \n",
            "l0: 0.011299, l1: 0.011384, l2: 0.012709, l3: 0.015914, l4: 0.015775, l5: 0.018420, l6: 0.043530\n",
            "\n",
            "[epoch: 197/1000, batch:    10/   15, ite: 2950] train loss: 0.150130, tar: 0.014743 \n",
            "l0: 0.010227, l1: 0.010139, l2: 0.011419, l3: 0.012683, l4: 0.014159, l5: 0.022203, l6: 0.038168\n",
            "\n",
            "[epoch: 197/1000, batch:    11/   15, ite: 2951] train loss: 0.149520, tar: 0.014655 \n",
            "l0: 0.020827, l1: 0.020819, l2: 0.022567, l3: 0.022422, l4: 0.025860, l5: 0.027390, l6: 0.058000\n",
            "\n",
            "[epoch: 197/1000, batch:    12/   15, ite: 2952] train loss: 0.150450, tar: 0.014773 \n",
            "l0: 0.013347, l1: 0.013162, l2: 0.015416, l3: 0.015862, l4: 0.018010, l5: 0.023497, l6: 0.044491\n",
            "\n",
            "[epoch: 197/1000, batch:    13/   15, ite: 2953] train loss: 0.150324, tar: 0.014746 \n",
            "l0: 0.016551, l1: 0.015739, l2: 0.017332, l3: 0.021586, l4: 0.023748, l5: 0.025195, l6: 0.037438\n",
            "\n",
            "[epoch: 197/1000, batch:    14/   15, ite: 2954] train loss: 0.150459, tar: 0.014780 \n",
            "l0: 0.021433, l1: 0.019353, l2: 0.025031, l3: 0.028435, l4: 0.034835, l5: 0.034476, l6: 0.050032\n",
            "\n",
            "[epoch: 197/1000, batch:    15/   15, ite: 2955] train loss: 0.151607, tar: 0.014901 \n",
            "l0: 0.011812, l1: 0.011021, l2: 0.013770, l3: 0.016960, l4: 0.018140, l5: 0.020125, l6: 0.034088\n",
            "\n",
            "[epoch: 198/1000, batch:     1/   15, ite: 2956] train loss: 0.151148, tar: 0.014846 \n",
            "l0: 0.012783, l1: 0.012616, l2: 0.015600, l3: 0.016244, l4: 0.017009, l5: 0.022482, l6: 0.040504\n",
            "\n",
            "[epoch: 198/1000, batch:     2/   15, ite: 2957] train loss: 0.150904, tar: 0.014809 \n",
            "l0: 0.012741, l1: 0.012474, l2: 0.016261, l3: 0.016927, l4: 0.018852, l5: 0.022079, l6: 0.030764\n",
            "\n",
            "[epoch: 198/1000, batch:     3/   15, ite: 2958] train loss: 0.150545, tar: 0.014774 \n",
            "l0: 0.011845, l1: 0.011420, l2: 0.014291, l3: 0.017152, l4: 0.020191, l5: 0.022361, l6: 0.035375\n",
            "\n",
            "[epoch: 198/1000, batch:     4/   15, ite: 2959] train loss: 0.150241, tar: 0.014724 \n",
            "l0: 0.010256, l1: 0.009713, l2: 0.010857, l3: 0.012861, l4: 0.015857, l5: 0.020966, l6: 0.040284\n",
            "\n",
            "[epoch: 198/1000, batch:     5/   15, ite: 2960] train loss: 0.149751, tar: 0.014650 \n",
            "l0: 0.014683, l1: 0.014454, l2: 0.016646, l3: 0.020106, l4: 0.021118, l5: 0.023717, l6: 0.043994\n",
            "\n",
            "[epoch: 198/1000, batch:     6/   15, ite: 2961] train loss: 0.149832, tar: 0.014650 \n",
            "l0: 0.016727, l1: 0.019265, l2: 0.018928, l3: 0.018706, l4: 0.021965, l5: 0.023933, l6: 0.043059\n",
            "\n",
            "[epoch: 198/1000, batch:     7/   15, ite: 2962] train loss: 0.150038, tar: 0.014684 \n",
            "l0: 0.022468, l1: 0.020743, l2: 0.025045, l3: 0.026952, l4: 0.032894, l5: 0.036165, l6: 0.048194\n",
            "\n",
            "[epoch: 198/1000, batch:     8/   15, ite: 2963] train loss: 0.151029, tar: 0.014807 \n",
            "l0: 0.010960, l1: 0.010571, l2: 0.012174, l3: 0.013901, l4: 0.017339, l5: 0.026007, l6: 0.039119\n",
            "\n",
            "[epoch: 198/1000, batch:     9/   15, ite: 2964] train loss: 0.150701, tar: 0.014747 \n",
            "l0: 0.015772, l1: 0.017206, l2: 0.016110, l3: 0.016671, l4: 0.018542, l5: 0.022767, l6: 0.034684\n",
            "\n",
            "[epoch: 198/1000, batch:    10/   15, ite: 2965] train loss: 0.150563, tar: 0.014763 \n",
            "l0: 0.024856, l1: 0.025156, l2: 0.027242, l3: 0.030726, l4: 0.027201, l5: 0.030340, l6: 0.048935\n",
            "\n",
            "[epoch: 198/1000, batch:    11/   15, ite: 2966] train loss: 0.151532, tar: 0.014916 \n",
            "l0: 0.018666, l1: 0.018804, l2: 0.020266, l3: 0.022983, l4: 0.021689, l5: 0.025949, l6: 0.041086\n",
            "\n",
            "[epoch: 198/1000, batch:    12/   15, ite: 2967] train loss: 0.151799, tar: 0.014972 \n",
            "l0: 0.012598, l1: 0.012603, l2: 0.013673, l3: 0.015395, l4: 0.017142, l5: 0.020512, l6: 0.033347\n",
            "\n",
            "[epoch: 198/1000, batch:    13/   15, ite: 2968] train loss: 0.151409, tar: 0.014937 \n",
            "l0: 0.013478, l1: 0.013124, l2: 0.015201, l3: 0.014558, l4: 0.018671, l5: 0.022329, l6: 0.029019\n",
            "\n",
            "[epoch: 198/1000, batch:    14/   15, ite: 2969] train loss: 0.151046, tar: 0.014916 \n",
            "l0: 0.016362, l1: 0.016298, l2: 0.019019, l3: 0.022650, l4: 0.023247, l5: 0.029564, l6: 0.051812\n",
            "\n",
            "[epoch: 198/1000, batch:    15/   15, ite: 2970] train loss: 0.151445, tar: 0.014936 \n",
            "l0: 0.009870, l1: 0.009265, l2: 0.010629, l3: 0.013230, l4: 0.015526, l5: 0.020807, l6: 0.033639\n",
            "\n",
            "[epoch: 199/1000, batch:     1/   15, ite: 2971] train loss: 0.150903, tar: 0.014865 \n",
            "l0: 0.010680, l1: 0.011355, l2: 0.011903, l3: 0.013777, l4: 0.015114, l5: 0.021772, l6: 0.034326\n",
            "\n",
            "[epoch: 199/1000, batch:     2/   15, ite: 2972] train loss: 0.150459, tar: 0.014807 \n",
            "l0: 0.020372, l1: 0.022178, l2: 0.022686, l3: 0.024626, l4: 0.025486, l5: 0.031847, l6: 0.066152\n",
            "\n",
            "[epoch: 199/1000, batch:     3/   15, ite: 2973] train loss: 0.151320, tar: 0.014883 \n",
            "l0: 0.015182, l1: 0.016983, l2: 0.018688, l3: 0.017897, l4: 0.019608, l5: 0.027848, l6: 0.054011\n",
            "\n",
            "[epoch: 199/1000, batch:     4/   15, ite: 2974] train loss: 0.151575, tar: 0.014887 \n",
            "l0: 0.016483, l1: 0.018641, l2: 0.019322, l3: 0.020084, l4: 0.020506, l5: 0.021719, l6: 0.027400\n",
            "\n",
            "[epoch: 199/1000, batch:     5/   15, ite: 2975] train loss: 0.151477, tar: 0.014909 \n",
            "l0: 0.013900, l1: 0.012214, l2: 0.015688, l3: 0.018396, l4: 0.020517, l5: 0.026284, l6: 0.044923\n",
            "\n",
            "[epoch: 199/1000, batch:     6/   15, ite: 2976] train loss: 0.151482, tar: 0.014895 \n",
            "l0: 0.019027, l1: 0.017605, l2: 0.018837, l3: 0.022494, l4: 0.023902, l5: 0.034619, l6: 0.052756\n",
            "\n",
            "[epoch: 199/1000, batch:     7/   15, ite: 2977] train loss: 0.151973, tar: 0.014949 \n",
            "l0: 0.014122, l1: 0.014858, l2: 0.015206, l3: 0.016872, l4: 0.019070, l5: 0.021433, l6: 0.042527\n",
            "\n",
            "[epoch: 199/1000, batch:     8/   15, ite: 2978] train loss: 0.151872, tar: 0.014938 \n",
            "l0: 0.013528, l1: 0.012118, l2: 0.014818, l3: 0.018976, l4: 0.021386, l5: 0.024575, l6: 0.039743\n",
            "\n",
            "[epoch: 199/1000, batch:     9/   15, ite: 2979] train loss: 0.151787, tar: 0.014920 \n",
            "l0: 0.010791, l1: 0.011983, l2: 0.012826, l3: 0.014675, l4: 0.017443, l5: 0.023086, l6: 0.033083\n",
            "\n",
            "[epoch: 199/1000, batch:    10/   15, ite: 2980] train loss: 0.151438, tar: 0.014869 \n",
            "l0: 0.029798, l1: 0.034529, l2: 0.040379, l3: 0.036704, l4: 0.040358, l5: 0.036202, l6: 0.051023\n",
            "\n",
            "[epoch: 199/1000, batch:    11/   15, ite: 2981] train loss: 0.152889, tar: 0.015053 \n",
            "l0: 0.013740, l1: 0.015201, l2: 0.014576, l3: 0.016416, l4: 0.018088, l5: 0.025449, l6: 0.039964\n",
            "\n",
            "[epoch: 199/1000, batch:    12/   15, ite: 2982] train loss: 0.152774, tar: 0.015037 \n",
            "l0: 0.013419, l1: 0.013310, l2: 0.015237, l3: 0.016687, l4: 0.018537, l5: 0.020692, l6: 0.041386\n",
            "\n",
            "[epoch: 199/1000, batch:    13/   15, ite: 2983] train loss: 0.152611, tar: 0.015018 \n",
            "l0: 0.024720, l1: 0.023209, l2: 0.024823, l3: 0.025612, l4: 0.028972, l5: 0.036961, l6: 0.053413\n",
            "\n",
            "[epoch: 199/1000, batch:    14/   15, ite: 2984] train loss: 0.153386, tar: 0.015133 \n",
            "l0: 0.011200, l1: 0.010782, l2: 0.013183, l3: 0.015177, l4: 0.016328, l5: 0.023193, l6: 0.041645\n",
            "\n",
            "[epoch: 199/1000, batch:    15/   15, ite: 2985] train loss: 0.153129, tar: 0.015087 \n",
            "l0: 0.015361, l1: 0.014755, l2: 0.020587, l3: 0.019740, l4: 0.025969, l5: 0.026834, l6: 0.034620\n",
            "\n",
            "[epoch: 200/1000, batch:     1/   15, ite: 2986] train loss: 0.153184, tar: 0.015090 \n",
            "l0: 0.012426, l1: 0.012259, l2: 0.013882, l3: 0.015814, l4: 0.017909, l5: 0.022498, l6: 0.045582\n",
            "\n",
            "[epoch: 200/1000, batch:     2/   15, ite: 2987] train loss: 0.153036, tar: 0.015059 \n",
            "l0: 0.015032, l1: 0.017435, l2: 0.019322, l3: 0.020508, l4: 0.020371, l5: 0.021615, l6: 0.040737\n",
            "\n",
            "[epoch: 200/1000, batch:     3/   15, ite: 2988] train loss: 0.153059, tar: 0.015059 \n",
            "l0: 0.021450, l1: 0.027444, l2: 0.023271, l3: 0.020339, l4: 0.021048, l5: 0.022138, l6: 0.044067\n",
            "\n",
            "[epoch: 200/1000, batch:     4/   15, ite: 2989] train loss: 0.153359, tar: 0.015131 \n",
            "l0: 0.013983, l1: 0.014997, l2: 0.016794, l3: 0.018111, l4: 0.022914, l5: 0.031632, l6: 0.042709\n",
            "\n",
            "[epoch: 200/1000, batch:     5/   15, ite: 2990] train loss: 0.153445, tar: 0.015118 \n",
            "l0: 0.014206, l1: 0.014385, l2: 0.016946, l3: 0.020102, l4: 0.022202, l5: 0.024237, l6: 0.037473\n",
            "\n",
            "[epoch: 200/1000, batch:     6/   15, ite: 2991] train loss: 0.153403, tar: 0.015108 \n",
            "l0: 0.016345, l1: 0.016370, l2: 0.017991, l3: 0.022066, l4: 0.023228, l5: 0.022345, l6: 0.036543\n",
            "\n",
            "[epoch: 200/1000, batch:     7/   15, ite: 2992] train loss: 0.153419, tar: 0.015122 \n",
            "l0: 0.020203, l1: 0.028818, l2: 0.023649, l3: 0.021153, l4: 0.020413, l5: 0.022862, l6: 0.036247\n",
            "\n",
            "[epoch: 200/1000, batch:     8/   15, ite: 2993] train loss: 0.153633, tar: 0.015176 \n",
            "l0: 0.023671, l1: 0.025631, l2: 0.023997, l3: 0.024830, l4: 0.026041, l5: 0.028108, l6: 0.046057\n",
            "\n",
            "[epoch: 200/1000, batch:     9/   15, ite: 2994] train loss: 0.154109, tar: 0.015267 \n",
            "l0: 0.013599, l1: 0.013534, l2: 0.016014, l3: 0.018616, l4: 0.018989, l5: 0.024747, l6: 0.037913\n",
            "\n",
            "[epoch: 200/1000, batch:    10/   15, ite: 2995] train loss: 0.153996, tar: 0.015249 \n",
            "l0: 0.015178, l1: 0.015510, l2: 0.017634, l3: 0.017869, l4: 0.019368, l5: 0.021624, l6: 0.034434\n",
            "\n",
            "[epoch: 200/1000, batch:    11/   15, ite: 2996] train loss: 0.153867, tar: 0.015248 \n",
            "l0: 0.024743, l1: 0.022988, l2: 0.022654, l3: 0.027982, l4: 0.031832, l5: 0.035177, l6: 0.057974\n",
            "\n",
            "[epoch: 200/1000, batch:    12/   15, ite: 2997] train loss: 0.154583, tar: 0.015346 \n",
            "l0: 0.024896, l1: 0.027199, l2: 0.028430, l3: 0.030581, l4: 0.034618, l5: 0.032828, l6: 0.045088\n",
            "\n",
            "[epoch: 200/1000, batch:    13/   15, ite: 2998] train loss: 0.155288, tar: 0.015444 \n",
            "l0: 0.016791, l1: 0.016168, l2: 0.018162, l3: 0.020064, l4: 0.024788, l5: 0.027543, l6: 0.040628\n",
            "\n",
            "[epoch: 200/1000, batch:    14/   15, ite: 2999] train loss: 0.155377, tar: 0.015457 \n",
            "l0: 0.016477, l1: 0.016203, l2: 0.015997, l3: 0.017378, l4: 0.020945, l5: 0.028273, l6: 0.038716\n",
            "\n",
            "[epoch: 200/1000, batch:    15/   15, ite: 3000] train loss: 0.155363, tar: 0.015467 \n",
            "l0: 0.012177, l1: 0.013126, l2: 0.014592, l3: 0.016927, l4: 0.016006, l5: 0.022459, l6: 0.037006\n",
            "\n",
            "[epoch: 201/1000, batch:     1/   15, ite: 3001] train loss: 0.132292, tar: 0.012177 \n",
            "l0: 0.022125, l1: 0.022278, l2: 0.023356, l3: 0.026659, l4: 0.027537, l5: 0.034835, l6: 0.055968\n",
            "\n",
            "[epoch: 201/1000, batch:     2/   15, ite: 3002] train loss: 0.172525, tar: 0.017151 \n",
            "l0: 0.012002, l1: 0.011799, l2: 0.014184, l3: 0.015902, l4: 0.016567, l5: 0.024896, l6: 0.046732\n",
            "\n",
            "[epoch: 201/1000, batch:     3/   15, ite: 3003] train loss: 0.162377, tar: 0.015434 \n",
            "l0: 0.013704, l1: 0.012221, l2: 0.013989, l3: 0.018274, l4: 0.022375, l5: 0.035541, l6: 0.045534\n",
            "\n",
            "[epoch: 201/1000, batch:     4/   15, ite: 3004] train loss: 0.162193, tar: 0.015002 \n",
            "l0: 0.017645, l1: 0.017062, l2: 0.023260, l3: 0.022427, l4: 0.021367, l5: 0.032148, l6: 0.048287\n",
            "\n",
            "[epoch: 201/1000, batch:     5/   15, ite: 3005] train loss: 0.166193, tar: 0.015530 \n",
            "l0: 0.033106, l1: 0.040516, l2: 0.042915, l3: 0.028702, l4: 0.033507, l5: 0.032777, l6: 0.041436\n",
            "\n",
            "[epoch: 201/1000, batch:     6/   15, ite: 3006] train loss: 0.180654, tar: 0.018460 \n",
            "l0: 0.016021, l1: 0.017484, l2: 0.019361, l3: 0.020126, l4: 0.020456, l5: 0.025416, l6: 0.042336\n",
            "\n",
            "[epoch: 201/1000, batch:     7/   15, ite: 3007] train loss: 0.177875, tar: 0.018111 \n",
            "l0: 0.012458, l1: 0.012268, l2: 0.015476, l3: 0.018176, l4: 0.019973, l5: 0.024630, l6: 0.044171\n",
            "\n",
            "[epoch: 201/1000, batch:     8/   15, ite: 3008] train loss: 0.174034, tar: 0.017405 \n",
            "l0: 0.012653, l1: 0.012274, l2: 0.013624, l3: 0.015701, l4: 0.017428, l5: 0.020585, l6: 0.038284\n",
            "\n",
            "[epoch: 201/1000, batch:     9/   15, ite: 3009] train loss: 0.169203, tar: 0.016877 \n",
            "l0: 0.019098, l1: 0.019618, l2: 0.020390, l3: 0.020947, l4: 0.023475, l5: 0.027780, l6: 0.057161\n",
            "\n",
            "[epoch: 201/1000, batch:    10/   15, ite: 3010] train loss: 0.171129, tar: 0.017099 \n",
            "l0: 0.019541, l1: 0.021364, l2: 0.022774, l3: 0.020931, l4: 0.024511, l5: 0.025449, l6: 0.043998\n",
            "\n",
            "[epoch: 201/1000, batch:    11/   15, ite: 3011] train loss: 0.171806, tar: 0.017321 \n",
            "l0: 0.013896, l1: 0.013286, l2: 0.017933, l3: 0.020242, l4: 0.023450, l5: 0.026608, l6: 0.043562\n",
            "\n",
            "[epoch: 201/1000, batch:    12/   15, ite: 3012] train loss: 0.170737, tar: 0.017035 \n",
            "l0: 0.015310, l1: 0.015281, l2: 0.018117, l3: 0.015484, l4: 0.016068, l5: 0.021899, l6: 0.038299\n",
            "\n",
            "[epoch: 201/1000, batch:    13/   15, ite: 3013] train loss: 0.168408, tar: 0.016903 \n",
            "l0: 0.038592, l1: 0.031622, l2: 0.051415, l3: 0.049573, l4: 0.045637, l5: 0.054315, l6: 0.075257\n",
            "\n",
            "[epoch: 201/1000, batch:    14/   15, ite: 3014] train loss: 0.181122, tar: 0.018452 \n",
            "l0: 0.018142, l1: 0.017690, l2: 0.021192, l3: 0.021548, l4: 0.026121, l5: 0.035833, l6: 0.052012\n",
            "\n",
            "[epoch: 201/1000, batch:    15/   15, ite: 3015] train loss: 0.181883, tar: 0.018431 \n",
            "l0: 0.015774, l1: 0.014454, l2: 0.018546, l3: 0.022501, l4: 0.026144, l5: 0.031893, l6: 0.039876\n",
            "\n",
            "[epoch: 202/1000, batch:     1/   15, ite: 3016] train loss: 0.181090, tar: 0.018265 \n",
            "l0: 0.020537, l1: 0.020590, l2: 0.022877, l3: 0.024167, l4: 0.025647, l5: 0.031884, l6: 0.052476\n",
            "\n",
            "[epoch: 202/1000, batch:     2/   15, ite: 3017] train loss: 0.182095, tar: 0.018399 \n",
            "l0: 0.023694, l1: 0.024922, l2: 0.026631, l3: 0.027574, l4: 0.026756, l5: 0.032006, l6: 0.052898\n",
            "\n",
            "[epoch: 202/1000, batch:     3/   15, ite: 3018] train loss: 0.183894, tar: 0.018693 \n",
            "l0: 0.018607, l1: 0.019968, l2: 0.022178, l3: 0.021325, l4: 0.020539, l5: 0.029298, l6: 0.046177\n",
            "\n",
            "[epoch: 202/1000, batch:     4/   15, ite: 3019] train loss: 0.183589, tar: 0.018688 \n",
            "l0: 0.014628, l1: 0.015320, l2: 0.016481, l3: 0.017271, l4: 0.019437, l5: 0.029271, l6: 0.038277\n",
            "\n",
            "[epoch: 202/1000, batch:     5/   15, ite: 3020] train loss: 0.181943, tar: 0.018485 \n",
            "l0: 0.017357, l1: 0.017967, l2: 0.018612, l3: 0.019929, l4: 0.017490, l5: 0.024128, l6: 0.044759\n",
            "\n",
            "[epoch: 202/1000, batch:     6/   15, ite: 3021] train loss: 0.180910, tar: 0.018432 \n",
            "l0: 0.025939, l1: 0.025667, l2: 0.025985, l3: 0.026991, l4: 0.028285, l5: 0.045535, l6: 0.085768\n",
            "\n",
            "[epoch: 202/1000, batch:     7/   15, ite: 3022] train loss: 0.184695, tar: 0.018773 \n",
            "l0: 0.039767, l1: 0.041849, l2: 0.042875, l3: 0.037140, l4: 0.038829, l5: 0.050753, l6: 0.079804\n",
            "\n",
            "[epoch: 202/1000, batch:     8/   15, ite: 3023] train loss: 0.191056, tar: 0.019686 \n",
            "l0: 0.019754, l1: 0.021012, l2: 0.020934, l3: 0.025789, l4: 0.028607, l5: 0.036902, l6: 0.066338\n",
            "\n",
            "[epoch: 202/1000, batch:     9/   15, ite: 3024] train loss: 0.192235, tar: 0.019689 \n",
            "l0: 0.022826, l1: 0.022167, l2: 0.025219, l3: 0.028728, l4: 0.034852, l5: 0.043797, l6: 0.053056\n",
            "\n",
            "[epoch: 202/1000, batch:    10/   15, ite: 3025] train loss: 0.193771, tar: 0.019814 \n",
            "l0: 0.020734, l1: 0.022047, l2: 0.024789, l3: 0.022946, l4: 0.022434, l5: 0.029009, l6: 0.047276\n",
            "\n",
            "[epoch: 202/1000, batch:    11/   15, ite: 3026] train loss: 0.193597, tar: 0.019850 \n",
            "l0: 0.028179, l1: 0.029487, l2: 0.030416, l3: 0.032613, l4: 0.045056, l5: 0.043743, l6: 0.039008\n",
            "\n",
            "[epoch: 202/1000, batch:    12/   15, ite: 3027] train loss: 0.195630, tar: 0.020158 \n",
            "l0: 0.012092, l1: 0.011891, l2: 0.013545, l3: 0.016672, l4: 0.018926, l5: 0.028135, l6: 0.038400\n",
            "\n",
            "[epoch: 202/1000, batch:    13/   15, ite: 3028] train loss: 0.193631, tar: 0.019870 \n",
            "l0: 0.016301, l1: 0.015199, l2: 0.013956, l3: 0.018930, l4: 0.025452, l5: 0.041706, l6: 0.067328\n",
            "\n",
            "[epoch: 202/1000, batch:    14/   15, ite: 3029] train loss: 0.193812, tar: 0.019747 \n",
            "l0: 0.011824, l1: 0.011517, l2: 0.012391, l3: 0.013528, l4: 0.018308, l5: 0.033456, l6: 0.053059\n",
            "\n",
            "[epoch: 202/1000, batch:    15/   15, ite: 3030] train loss: 0.192488, tar: 0.019483 \n",
            "l0: 0.018064, l1: 0.017483, l2: 0.019545, l3: 0.022702, l4: 0.021461, l5: 0.027898, l6: 0.052447\n",
            "\n",
            "[epoch: 203/1000, batch:     1/   15, ite: 3031] train loss: 0.192072, tar: 0.019437 \n",
            "l0: 0.011391, l1: 0.011844, l2: 0.012625, l3: 0.013605, l4: 0.015672, l5: 0.020056, l6: 0.037529\n",
            "\n",
            "[epoch: 203/1000, batch:     2/   15, ite: 3032] train loss: 0.189905, tar: 0.019186 \n",
            "l0: 0.024116, l1: 0.024582, l2: 0.022322, l3: 0.028530, l4: 0.030140, l5: 0.034005, l6: 0.058879\n",
            "\n",
            "[epoch: 203/1000, batch:     3/   15, ite: 3033] train loss: 0.190895, tar: 0.019335 \n",
            "l0: 0.022295, l1: 0.023114, l2: 0.025798, l3: 0.024984, l4: 0.029108, l5: 0.037452, l6: 0.053393\n",
            "\n",
            "[epoch: 203/1000, batch:     4/   15, ite: 3034] train loss: 0.191637, tar: 0.019422 \n",
            "l0: 0.023920, l1: 0.026641, l2: 0.026055, l3: 0.023756, l4: 0.025948, l5: 0.037035, l6: 0.054845\n",
            "\n",
            "[epoch: 203/1000, batch:     5/   15, ite: 3035] train loss: 0.192396, tar: 0.019551 \n",
            "l0: 0.016407, l1: 0.015917, l2: 0.018895, l3: 0.022186, l4: 0.020040, l5: 0.024217, l6: 0.041368\n",
            "\n",
            "[epoch: 203/1000, batch:     6/   15, ite: 3036] train loss: 0.191469, tar: 0.019463 \n",
            "l0: 0.031508, l1: 0.031502, l2: 0.035460, l3: 0.040036, l4: 0.035010, l5: 0.033812, l6: 0.070505\n",
            "\n",
            "[epoch: 203/1000, batch:     7/   15, ite: 3037] train loss: 0.193804, tar: 0.019789 \n",
            "l0: 0.018345, l1: 0.022260, l2: 0.020293, l3: 0.023273, l4: 0.025222, l5: 0.027850, l6: 0.032596\n",
            "\n",
            "[epoch: 203/1000, batch:     8/   15, ite: 3038] train loss: 0.193173, tar: 0.019751 \n",
            "l0: 0.033683, l1: 0.032741, l2: 0.031976, l3: 0.043335, l4: 0.053258, l5: 0.053727, l6: 0.051497\n",
            "\n",
            "[epoch: 203/1000, batch:     9/   15, ite: 3039] train loss: 0.195918, tar: 0.020108 \n",
            "l0: 0.014907, l1: 0.015049, l2: 0.016069, l3: 0.019304, l4: 0.021745, l5: 0.027814, l6: 0.041266\n",
            "\n",
            "[epoch: 203/1000, batch:    10/   15, ite: 3040] train loss: 0.194924, tar: 0.019978 \n",
            "l0: 0.016640, l1: 0.016101, l2: 0.016366, l3: 0.018090, l4: 0.026917, l5: 0.061272, l6: 0.097284\n",
            "\n",
            "[epoch: 203/1000, batch:    11/   15, ite: 3041] train loss: 0.196332, tar: 0.019897 \n",
            "l0: 0.020701, l1: 0.021092, l2: 0.026075, l3: 0.025397, l4: 0.027988, l5: 0.050059, l6: 0.076054\n",
            "\n",
            "[epoch: 203/1000, batch:    12/   15, ite: 3042] train loss: 0.197547, tar: 0.019916 \n",
            "l0: 0.038882, l1: 0.042736, l2: 0.044189, l3: 0.041239, l4: 0.040055, l5: 0.040249, l6: 0.047912\n",
            "\n",
            "[epoch: 203/1000, batch:    13/   15, ite: 3043] train loss: 0.199820, tar: 0.020357 \n",
            "l0: 0.021997, l1: 0.024058, l2: 0.023747, l3: 0.026652, l4: 0.029773, l5: 0.032772, l6: 0.042530\n",
            "\n",
            "[epoch: 203/1000, batch:    14/   15, ite: 3044] train loss: 0.199858, tar: 0.020394 \n",
            "l0: 0.026188, l1: 0.030981, l2: 0.029355, l3: 0.030711, l4: 0.032181, l5: 0.035119, l6: 0.046398\n",
            "\n",
            "[epoch: 203/1000, batch:    15/   15, ite: 3045] train loss: 0.200549, tar: 0.020523 \n",
            "l0: 0.024540, l1: 0.025407, l2: 0.026295, l3: 0.028315, l4: 0.030107, l5: 0.039437, l6: 0.058490\n",
            "\n",
            "[epoch: 204/1000, batch:     1/   15, ite: 3046] train loss: 0.201245, tar: 0.020610 \n",
            "l0: 0.015547, l1: 0.014208, l2: 0.022289, l3: 0.016398, l4: 0.020373, l5: 0.028175, l6: 0.043626\n",
            "\n",
            "[epoch: 204/1000, batch:     2/   15, ite: 3047] train loss: 0.200381, tar: 0.020502 \n",
            "l0: 0.029153, l1: 0.029606, l2: 0.035222, l3: 0.033328, l4: 0.038126, l5: 0.051429, l6: 0.058250\n",
            "\n",
            "[epoch: 204/1000, batch:     3/   15, ite: 3048] train loss: 0.201938, tar: 0.020683 \n",
            "l0: 0.021199, l1: 0.021781, l2: 0.023784, l3: 0.026180, l4: 0.034451, l5: 0.035092, l6: 0.051622\n",
            "\n",
            "[epoch: 204/1000, batch:     4/   15, ite: 3049] train loss: 0.202186, tar: 0.020693 \n",
            "l0: 0.016822, l1: 0.016696, l2: 0.018082, l3: 0.019518, l4: 0.020343, l5: 0.020907, l6: 0.026288\n",
            "\n",
            "[epoch: 204/1000, batch:     5/   15, ite: 3050] train loss: 0.200916, tar: 0.020616 \n",
            "l0: 0.023676, l1: 0.025969, l2: 0.030465, l3: 0.030480, l4: 0.029303, l5: 0.026732, l6: 0.041341\n",
            "\n",
            "[epoch: 204/1000, batch:     6/   15, ite: 3051] train loss: 0.201054, tar: 0.020676 \n",
            "l0: 0.019039, l1: 0.019915, l2: 0.020208, l3: 0.020591, l4: 0.025033, l5: 0.031576, l6: 0.046596\n",
            "\n",
            "[epoch: 204/1000, batch:     7/   15, ite: 3052] train loss: 0.200706, tar: 0.020644 \n",
            "l0: 0.025505, l1: 0.025722, l2: 0.028477, l3: 0.034860, l4: 0.041268, l5: 0.041127, l6: 0.077330\n",
            "\n",
            "[epoch: 204/1000, batch:     8/   15, ite: 3053] train loss: 0.202094, tar: 0.020736 \n",
            "l0: 0.015324, l1: 0.019048, l2: 0.018664, l3: 0.016220, l4: 0.017713, l5: 0.028144, l6: 0.049212\n",
            "\n",
            "[epoch: 204/1000, batch:     9/   15, ite: 3054] train loss: 0.201395, tar: 0.020636 \n",
            "l0: 0.022459, l1: 0.024614, l2: 0.023862, l3: 0.030363, l4: 0.028523, l5: 0.030511, l6: 0.049753\n",
            "\n",
            "[epoch: 204/1000, batch:    10/   15, ite: 3055] train loss: 0.201553, tar: 0.020669 \n",
            "l0: 0.030694, l1: 0.035219, l2: 0.036408, l3: 0.033654, l4: 0.035355, l5: 0.031237, l6: 0.054690\n",
            "\n",
            "[epoch: 204/1000, batch:    11/   15, ite: 3056] train loss: 0.202548, tar: 0.020848 \n",
            "l0: 0.015088, l1: 0.017930, l2: 0.016425, l3: 0.017070, l4: 0.018630, l5: 0.024688, l6: 0.034489\n",
            "\n",
            "[epoch: 204/1000, batch:    12/   15, ite: 3057] train loss: 0.201526, tar: 0.020747 \n",
            "l0: 0.015112, l1: 0.014702, l2: 0.017135, l3: 0.021831, l4: 0.021866, l5: 0.031859, l6: 0.047499\n",
            "\n",
            "[epoch: 204/1000, batch:    13/   15, ite: 3058] train loss: 0.200983, tar: 0.020650 \n",
            "l0: 0.020141, l1: 0.027024, l2: 0.022797, l3: 0.020434, l4: 0.017847, l5: 0.017248, l6: 0.027503\n",
            "\n",
            "[epoch: 204/1000, batch:    14/   15, ite: 3059] train loss: 0.200169, tar: 0.020641 \n",
            "l0: 0.016918, l1: 0.014803, l2: 0.016964, l3: 0.022268, l4: 0.022425, l5: 0.029762, l6: 0.050342\n",
            "\n",
            "[epoch: 204/1000, batch:    15/   15, ite: 3060] train loss: 0.199724, tar: 0.020579 \n",
            "l0: 0.016401, l1: 0.015195, l2: 0.020365, l3: 0.023653, l4: 0.025517, l5: 0.031509, l6: 0.045239\n",
            "\n",
            "[epoch: 205/1000, batch:     1/   15, ite: 3061] train loss: 0.199366, tar: 0.020511 \n",
            "l0: 0.019964, l1: 0.017391, l2: 0.019826, l3: 0.027345, l4: 0.027910, l5: 0.039726, l6: 0.066616\n",
            "\n",
            "[epoch: 205/1000, batch:     2/   15, ite: 3062] train loss: 0.199679, tar: 0.020502 \n",
            "l0: 0.027015, l1: 0.023622, l2: 0.028365, l3: 0.035351, l4: 0.041544, l5: 0.047849, l6: 0.083547\n",
            "\n",
            "[epoch: 205/1000, batch:     3/   15, ite: 3063] train loss: 0.201070, tar: 0.020605 \n",
            "l0: 0.019614, l1: 0.020152, l2: 0.023261, l3: 0.024692, l4: 0.026745, l5: 0.028740, l6: 0.039984\n",
            "\n",
            "[epoch: 205/1000, batch:     4/   15, ite: 3064] train loss: 0.200791, tar: 0.020590 \n",
            "l0: 0.018553, l1: 0.017585, l2: 0.020178, l3: 0.021960, l4: 0.026103, l5: 0.029494, l6: 0.036359\n",
            "\n",
            "[epoch: 205/1000, batch:     5/   15, ite: 3065] train loss: 0.200321, tar: 0.020558 \n",
            "l0: 0.012183, l1: 0.011810, l2: 0.013568, l3: 0.015782, l4: 0.018142, l5: 0.020371, l6: 0.035628\n",
            "\n",
            "[epoch: 205/1000, batch:     6/   15, ite: 3066] train loss: 0.199217, tar: 0.020431 \n",
            "l0: 0.015599, l1: 0.014902, l2: 0.018629, l3: 0.020840, l4: 0.024571, l5: 0.035438, l6: 0.063550\n",
            "\n",
            "[epoch: 205/1000, batch:     7/   15, ite: 3067] train loss: 0.199132, tar: 0.020359 \n",
            "l0: 0.011045, l1: 0.010859, l2: 0.012013, l3: 0.013832, l4: 0.016343, l5: 0.023866, l6: 0.033559\n",
            "\n",
            "[epoch: 205/1000, batch:     8/   15, ite: 3068] train loss: 0.197991, tar: 0.020222 \n",
            "l0: 0.023083, l1: 0.025820, l2: 0.026130, l3: 0.029371, l4: 0.034754, l5: 0.033549, l6: 0.057260\n",
            "\n",
            "[epoch: 205/1000, batch:     9/   15, ite: 3069] train loss: 0.198454, tar: 0.020264 \n",
            "l0: 0.015510, l1: 0.016190, l2: 0.018268, l3: 0.021168, l4: 0.025837, l5: 0.027312, l6: 0.040790\n",
            "\n",
            "[epoch: 205/1000, batch:    10/   15, ite: 3070] train loss: 0.197977, tar: 0.020196 \n",
            "l0: 0.016525, l1: 0.014973, l2: 0.018110, l3: 0.020547, l4: 0.020763, l5: 0.028541, l6: 0.040919\n",
            "\n",
            "[epoch: 205/1000, batch:    11/   15, ite: 3071] train loss: 0.197448, tar: 0.020144 \n",
            "l0: 0.013437, l1: 0.012245, l2: 0.016929, l3: 0.020688, l4: 0.023396, l5: 0.028524, l6: 0.044902\n",
            "\n",
            "[epoch: 205/1000, batch:    12/   15, ite: 3072] train loss: 0.196929, tar: 0.020051 \n",
            "l0: 0.034307, l1: 0.037787, l2: 0.034172, l3: 0.032645, l4: 0.040627, l5: 0.046144, l6: 0.069597\n",
            "\n",
            "[epoch: 205/1000, batch:    13/   15, ite: 3073] train loss: 0.198277, tar: 0.020246 \n",
            "l0: 0.013870, l1: 0.013217, l2: 0.016882, l3: 0.018937, l4: 0.020075, l5: 0.023521, l6: 0.046960\n",
            "\n",
            "[epoch: 205/1000, batch:    14/   15, ite: 3074] train loss: 0.197671, tar: 0.020160 \n",
            "l0: 0.021302, l1: 0.019764, l2: 0.023481, l3: 0.025270, l4: 0.027473, l5: 0.035493, l6: 0.056523\n",
            "\n",
            "[epoch: 205/1000, batch:    15/   15, ite: 3075] train loss: 0.197826, tar: 0.020175 \n",
            "l0: 0.012645, l1: 0.011711, l2: 0.016900, l3: 0.016153, l4: 0.019446, l5: 0.026494, l6: 0.046812\n",
            "\n",
            "[epoch: 206/1000, batch:     1/   15, ite: 3076] train loss: 0.197199, tar: 0.020076 \n",
            "l0: 0.014614, l1: 0.014059, l2: 0.016533, l3: 0.017535, l4: 0.019707, l5: 0.019885, l6: 0.031485\n",
            "\n",
            "[epoch: 206/1000, batch:     2/   15, ite: 3077] train loss: 0.196376, tar: 0.020005 \n",
            "l0: 0.017486, l1: 0.016502, l2: 0.019529, l3: 0.022371, l4: 0.025421, l5: 0.028064, l6: 0.039012\n",
            "\n",
            "[epoch: 206/1000, batch:     3/   15, ite: 3078] train loss: 0.196017, tar: 0.019973 \n",
            "l0: 0.011992, l1: 0.012347, l2: 0.013631, l3: 0.016073, l4: 0.016348, l5: 0.020020, l6: 0.040586\n",
            "\n",
            "[epoch: 206/1000, batch:     4/   15, ite: 3079] train loss: 0.195194, tar: 0.019872 \n",
            "l0: 0.018270, l1: 0.020318, l2: 0.019461, l3: 0.022610, l4: 0.022244, l5: 0.026995, l6: 0.042570\n",
            "\n",
            "[epoch: 206/1000, batch:     5/   15, ite: 3080] train loss: 0.194910, tar: 0.019852 \n",
            "l0: 0.012026, l1: 0.011637, l2: 0.012419, l3: 0.014360, l4: 0.016781, l5: 0.022201, l6: 0.037507\n",
            "\n",
            "[epoch: 206/1000, batch:     6/   15, ite: 3081] train loss: 0.194071, tar: 0.019755 \n",
            "l0: 0.014870, l1: 0.014582, l2: 0.017420, l3: 0.019735, l4: 0.021789, l5: 0.027862, l6: 0.043668\n",
            "\n",
            "[epoch: 206/1000, batch:     7/   15, ite: 3082] train loss: 0.193654, tar: 0.019696 \n",
            "l0: 0.018467, l1: 0.020233, l2: 0.019404, l3: 0.021583, l4: 0.023736, l5: 0.028052, l6: 0.044359\n",
            "\n",
            "[epoch: 206/1000, batch:     8/   15, ite: 3083] train loss: 0.193439, tar: 0.019681 \n",
            "l0: 0.028804, l1: 0.028344, l2: 0.031407, l3: 0.035474, l4: 0.033446, l5: 0.042496, l6: 0.061971\n",
            "\n",
            "[epoch: 206/1000, batch:     9/   15, ite: 3084] train loss: 0.194255, tar: 0.019790 \n",
            "l0: 0.032375, l1: 0.029926, l2: 0.029757, l3: 0.034551, l4: 0.038235, l5: 0.048848, l6: 0.068006\n",
            "\n",
            "[epoch: 206/1000, batch:    10/   15, ite: 3085] train loss: 0.195284, tar: 0.019938 \n",
            "l0: 0.024455, l1: 0.022488, l2: 0.032124, l3: 0.034829, l4: 0.041366, l5: 0.043081, l6: 0.047827\n",
            "\n",
            "[epoch: 206/1000, batch:    11/   15, ite: 3086] train loss: 0.195875, tar: 0.019990 \n",
            "l0: 0.018727, l1: 0.017961, l2: 0.025062, l3: 0.025485, l4: 0.025667, l5: 0.028381, l6: 0.036909\n",
            "\n",
            "[epoch: 206/1000, batch:    12/   15, ite: 3087] train loss: 0.195672, tar: 0.019976 \n",
            "l0: 0.014615, l1: 0.015013, l2: 0.014846, l3: 0.016554, l4: 0.020550, l5: 0.027534, l6: 0.041922\n",
            "\n",
            "[epoch: 206/1000, batch:    13/   15, ite: 3088] train loss: 0.195165, tar: 0.019915 \n",
            "l0: 0.013465, l1: 0.014061, l2: 0.015393, l3: 0.017929, l4: 0.022139, l5: 0.032377, l6: 0.056639\n",
            "\n",
            "[epoch: 206/1000, batch:    14/   15, ite: 3089] train loss: 0.194905, tar: 0.019842 \n",
            "l0: 0.035508, l1: 0.037139, l2: 0.032741, l3: 0.038471, l4: 0.061879, l5: 0.073189, l6: 0.100770\n",
            "\n",
            "[epoch: 206/1000, batch:    15/   15, ite: 3090] train loss: 0.196958, tar: 0.020016 \n",
            "l0: 0.013984, l1: 0.013159, l2: 0.015619, l3: 0.017558, l4: 0.020612, l5: 0.025907, l6: 0.045950\n",
            "\n",
            "[epoch: 207/1000, batch:     1/   15, ite: 3091] train loss: 0.196473, tar: 0.019950 \n",
            "l0: 0.020927, l1: 0.018648, l2: 0.024705, l3: 0.025574, l4: 0.033027, l5: 0.037905, l6: 0.058363\n",
            "\n",
            "[epoch: 207/1000, batch:     2/   15, ite: 3092] train loss: 0.196719, tar: 0.019961 \n",
            "l0: 0.016489, l1: 0.016630, l2: 0.018357, l3: 0.018479, l4: 0.023161, l5: 0.033301, l6: 0.049721\n",
            "\n",
            "[epoch: 207/1000, batch:     3/   15, ite: 3093] train loss: 0.196498, tar: 0.019923 \n",
            "l0: 0.017941, l1: 0.018387, l2: 0.020276, l3: 0.023055, l4: 0.027869, l5: 0.032791, l6: 0.054072\n",
            "\n",
            "[epoch: 207/1000, batch:     4/   15, ite: 3094] train loss: 0.196475, tar: 0.019902 \n",
            "l0: 0.025987, l1: 0.024113, l2: 0.027633, l3: 0.031945, l4: 0.042588, l5: 0.041443, l6: 0.052633\n",
            "\n",
            "[epoch: 207/1000, batch:     5/   15, ite: 3095] train loss: 0.197000, tar: 0.019966 \n",
            "l0: 0.015064, l1: 0.014691, l2: 0.017516, l3: 0.019192, l4: 0.021408, l5: 0.024672, l6: 0.042911\n",
            "\n",
            "[epoch: 207/1000, batch:     6/   15, ite: 3096] train loss: 0.196567, tar: 0.019915 \n",
            "l0: 0.012149, l1: 0.012052, l2: 0.012625, l3: 0.014593, l4: 0.015922, l5: 0.024853, l6: 0.043679\n",
            "\n",
            "[epoch: 207/1000, batch:     7/   15, ite: 3097] train loss: 0.195942, tar: 0.019835 \n",
            "l0: 0.016857, l1: 0.016002, l2: 0.019726, l3: 0.023345, l4: 0.025718, l5: 0.028413, l6: 0.052489\n",
            "\n",
            "[epoch: 207/1000, batch:     8/   15, ite: 3098] train loss: 0.195805, tar: 0.019805 \n",
            "l0: 0.022807, l1: 0.021353, l2: 0.027037, l3: 0.037531, l4: 0.045218, l5: 0.047917, l6: 0.054052\n",
            "\n",
            "[epoch: 207/1000, batch:     9/   15, ite: 3099] train loss: 0.196412, tar: 0.019835 \n",
            "l0: 0.042609, l1: 0.042224, l2: 0.046285, l3: 0.048470, l4: 0.057878, l5: 0.070307, l6: 0.064832\n",
            "\n",
            "[epoch: 207/1000, batch:    10/   15, ite: 3100] train loss: 0.198174, tar: 0.020063 \n",
            "l0: 0.018261, l1: 0.017751, l2: 0.019254, l3: 0.026213, l4: 0.033990, l5: 0.037287, l6: 0.050587\n",
            "\n",
            "[epoch: 207/1000, batch:    11/   15, ite: 3101] train loss: 0.203343, tar: 0.018261 \n",
            "l0: 0.019557, l1: 0.020898, l2: 0.020593, l3: 0.020863, l4: 0.030216, l5: 0.033149, l6: 0.044084\n",
            "\n",
            "[epoch: 207/1000, batch:    12/   15, ite: 3102] train loss: 0.196352, tar: 0.018909 \n",
            "l0: 0.030900, l1: 0.036643, l2: 0.033987, l3: 0.040139, l4: 0.044740, l5: 0.035263, l6: 0.050972\n",
            "\n",
            "[epoch: 207/1000, batch:    13/   15, ite: 3103] train loss: 0.221783, tar: 0.022906 \n",
            "l0: 0.030863, l1: 0.032347, l2: 0.039395, l3: 0.040098, l4: 0.037069, l5: 0.035171, l6: 0.044382\n",
            "\n",
            "[epoch: 207/1000, batch:    14/   15, ite: 3104] train loss: 0.231168, tar: 0.024895 \n",
            "l0: 0.018713, l1: 0.019359, l2: 0.022225, l3: 0.023369, l4: 0.024749, l5: 0.043702, l6: 0.055485\n",
            "\n",
            "[epoch: 207/1000, batch:    15/   15, ite: 3105] train loss: 0.226455, tar: 0.023659 \n",
            "l0: 0.022561, l1: 0.026030, l2: 0.024111, l3: 0.024679, l4: 0.027097, l5: 0.029568, l6: 0.045174\n",
            "\n",
            "[epoch: 208/1000, batch:     1/   15, ite: 3106] train loss: 0.221916, tar: 0.023476 \n",
            "l0: 0.037296, l1: 0.032463, l2: 0.035036, l3: 0.045308, l4: 0.058123, l5: 0.071429, l6: 0.055240\n",
            "\n",
            "[epoch: 208/1000, batch:     2/   15, ite: 3107] train loss: 0.238055, tar: 0.025450 \n",
            "l0: 0.017924, l1: 0.015903, l2: 0.017550, l3: 0.019827, l4: 0.024241, l5: 0.035559, l6: 0.049441\n",
            "\n",
            "[epoch: 208/1000, batch:     3/   15, ite: 3108] train loss: 0.230854, tar: 0.024509 \n",
            "l0: 0.020981, l1: 0.018370, l2: 0.021298, l3: 0.026628, l4: 0.028266, l5: 0.035617, l6: 0.045731\n",
            "\n",
            "[epoch: 208/1000, batch:     4/   15, ite: 3109] train loss: 0.227081, tar: 0.024117 \n",
            "l0: 0.023288, l1: 0.020004, l2: 0.023939, l3: 0.034098, l4: 0.038507, l5: 0.052268, l6: 0.059012\n",
            "\n",
            "[epoch: 208/1000, batch:     5/   15, ite: 3110] train loss: 0.229484, tar: 0.024034 \n",
            "l0: 0.024167, l1: 0.025088, l2: 0.023354, l3: 0.024067, l4: 0.024559, l5: 0.037219, l6: 0.058065\n",
            "\n",
            "[epoch: 208/1000, batch:     6/   15, ite: 3111] train loss: 0.228306, tar: 0.024046 \n",
            "l0: 0.022104, l1: 0.023952, l2: 0.023870, l3: 0.023388, l4: 0.023036, l5: 0.026177, l6: 0.042801\n",
            "\n",
            "[epoch: 208/1000, batch:     7/   15, ite: 3112] train loss: 0.224724, tar: 0.023885 \n",
            "l0: 0.034470, l1: 0.039015, l2: 0.047413, l3: 0.030056, l4: 0.028099, l5: 0.030774, l6: 0.041916\n",
            "\n",
            "[epoch: 208/1000, batch:     8/   15, ite: 3113] train loss: 0.226803, tar: 0.024699 \n",
            "l0: 0.018893, l1: 0.021367, l2: 0.021551, l3: 0.022664, l4: 0.024459, l5: 0.032252, l6: 0.057991\n",
            "\n",
            "[epoch: 208/1000, batch:     9/   15, ite: 3114] train loss: 0.224829, tar: 0.024284 \n",
            "l0: 0.026939, l1: 0.028312, l2: 0.031543, l3: 0.028056, l4: 0.035922, l5: 0.052228, l6: 0.072521\n",
            "\n",
            "[epoch: 208/1000, batch:    10/   15, ite: 3115] train loss: 0.228209, tar: 0.024461 \n",
            "l0: 0.016723, l1: 0.017609, l2: 0.017414, l3: 0.020490, l4: 0.022430, l5: 0.035665, l6: 0.060317\n",
            "\n",
            "[epoch: 208/1000, batch:    11/   15, ite: 3116] train loss: 0.225861, tar: 0.023978 \n",
            "l0: 0.015643, l1: 0.014537, l2: 0.019606, l3: 0.021511, l4: 0.021821, l5: 0.029147, l6: 0.040403\n",
            "\n",
            "[epoch: 208/1000, batch:    12/   15, ite: 3117] train loss: 0.222144, tar: 0.023487 \n",
            "l0: 0.013733, l1: 0.013289, l2: 0.017381, l3: 0.018705, l4: 0.021078, l5: 0.026440, l6: 0.040841\n",
            "\n",
            "[epoch: 208/1000, batch:    13/   15, ite: 3118] train loss: 0.218218, tar: 0.022945 \n",
            "l0: 0.024128, l1: 0.023456, l2: 0.026412, l3: 0.029687, l4: 0.031462, l5: 0.042818, l6: 0.065291\n",
            "\n",
            "[epoch: 208/1000, batch:    14/   15, ite: 3119] train loss: 0.219535, tar: 0.023008 \n",
            "l0: 0.020195, l1: 0.021962, l2: 0.024764, l3: 0.029401, l4: 0.030383, l5: 0.031131, l6: 0.046641\n",
            "\n",
            "[epoch: 208/1000, batch:    15/   15, ite: 3120] train loss: 0.218782, tar: 0.022867 \n",
            "l0: 0.017971, l1: 0.021542, l2: 0.017790, l3: 0.018190, l4: 0.021847, l5: 0.026600, l6: 0.041859\n",
            "\n",
            "[epoch: 209/1000, batch:     1/   15, ite: 3121] train loss: 0.216259, tar: 0.022634 \n",
            "l0: 0.036368, l1: 0.039660, l2: 0.038045, l3: 0.038975, l4: 0.037481, l5: 0.045322, l6: 0.069047\n",
            "\n",
            "[epoch: 209/1000, batch:     2/   15, ite: 3122] train loss: 0.220288, tar: 0.023258 \n",
            "l0: 0.024338, l1: 0.030800, l2: 0.021804, l3: 0.019805, l4: 0.022543, l5: 0.030890, l6: 0.038685\n",
            "\n",
            "[epoch: 209/1000, batch:     3/   15, ite: 3123] train loss: 0.218922, tar: 0.023305 \n",
            "l0: 0.021946, l1: 0.025110, l2: 0.023968, l3: 0.024594, l4: 0.025353, l5: 0.026713, l6: 0.034888\n",
            "\n",
            "[epoch: 209/1000, batch:     4/   15, ite: 3124] train loss: 0.217408, tar: 0.023248 \n",
            "l0: 0.017878, l1: 0.016645, l2: 0.020376, l3: 0.022150, l4: 0.025362, l5: 0.033533, l6: 0.046117\n",
            "\n",
            "[epoch: 209/1000, batch:     5/   15, ite: 3125] train loss: 0.215994, tar: 0.023034 \n",
            "l0: 0.012103, l1: 0.011402, l2: 0.012526, l3: 0.015515, l4: 0.017151, l5: 0.022127, l6: 0.029504\n",
            "\n",
            "[epoch: 209/1000, batch:     6/   15, ite: 3126] train loss: 0.212314, tar: 0.022613 \n",
            "l0: 0.020077, l1: 0.018176, l2: 0.019881, l3: 0.024216, l4: 0.030805, l5: 0.038297, l6: 0.045500\n",
            "\n",
            "[epoch: 209/1000, batch:     7/   15, ite: 3127] train loss: 0.211745, tar: 0.022519 \n",
            "l0: 0.023433, l1: 0.022007, l2: 0.024000, l3: 0.028414, l4: 0.031242, l5: 0.040053, l6: 0.062525\n",
            "\n",
            "[epoch: 209/1000, batch:     8/   15, ite: 3128] train loss: 0.212457, tar: 0.022552 \n",
            "l0: 0.023520, l1: 0.022093, l2: 0.024686, l3: 0.029235, l4: 0.026048, l5: 0.028822, l6: 0.046288\n",
            "\n",
            "[epoch: 209/1000, batch:     9/   15, ite: 3129] train loss: 0.212051, tar: 0.022585 \n",
            "l0: 0.016206, l1: 0.016860, l2: 0.015960, l3: 0.019407, l4: 0.020065, l5: 0.027027, l6: 0.041839\n",
            "\n",
            "[epoch: 209/1000, batch:    10/   15, ite: 3130] train loss: 0.210229, tar: 0.022373 \n",
            "l0: 0.015575, l1: 0.014687, l2: 0.019119, l3: 0.020248, l4: 0.019558, l5: 0.025324, l6: 0.042834\n",
            "\n",
            "[epoch: 209/1000, batch:    11/   15, ite: 3131] train loss: 0.208523, tar: 0.022153 \n",
            "l0: 0.021505, l1: 0.026176, l2: 0.024730, l3: 0.021003, l4: 0.023884, l5: 0.033440, l6: 0.045916\n",
            "\n",
            "[epoch: 209/1000, batch:    12/   15, ite: 3132] train loss: 0.208152, tar: 0.022133 \n",
            "l0: 0.028615, l1: 0.029899, l2: 0.031594, l3: 0.030292, l4: 0.035363, l5: 0.046743, l6: 0.079153\n",
            "\n",
            "[epoch: 209/1000, batch:    13/   15, ite: 3133] train loss: 0.210379, tar: 0.022330 \n",
            "l0: 0.015869, l1: 0.015426, l2: 0.014895, l3: 0.018335, l4: 0.023732, l5: 0.034319, l6: 0.051026\n",
            "\n",
            "[epoch: 209/1000, batch:    14/   15, ite: 3134] train loss: 0.209298, tar: 0.022139 \n",
            "l0: 0.018696, l1: 0.019226, l2: 0.020010, l3: 0.022992, l4: 0.025763, l5: 0.033527, l6: 0.051632\n",
            "\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4cae2598c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1322, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/popen_fork.py\", line 45, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.7/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"u2net_train.py\", line 148, in <module>\n",
            "    running_loss += loss.data.item()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "!python u2net_test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDmLX1oK1A5B",
        "outputId": "cc456c41-31c0-43be-ed7f-cdf8dd714f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/U-2-Net/test_data/test_images/images/49.jpg', '/content/drive/MyDrive/U-2-Net/test_data/test_images/images/41.jpg', '/content/drive/MyDrive/U-2-Net/test_data/test_images/images/39.jpg', '/content/drive/MyDrive/U-2-Net/test_data/test_images/images/45.jpg', '/content/drive/MyDrive/U-2-Net/test_data/test_images/images/46.jpg']\n",
            "...load U2NET---173.6 MB\n",
            "inferencing: 49.jpg\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
            "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3704: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "inferencing: 41.jpg\n",
            "inferencing: 39.jpg\n",
            "inferencing: 45.jpg\n",
            "inferencing: 46.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating Dice Score**"
      ],
      "metadata": {
        "id": "6wuNfQs46nIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "VzI7taFy1J9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the Dice score function.\n",
        "def dice_coefficient(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(y_true * y_pred, axis=[1, 2, 3])\n",
        "    union = K.sum(y_true, axis=[1, 2, 3]) + K.sum(y_pred, axis=[1, 2, 3])\n",
        "    return K.mean((2. * intersection + smooth) / (union + smooth), axis=0)"
      ],
      "metadata": {
        "id": "Z5Vyjebl6QR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the predicted mask\n",
        "pred = cv2.imread('/content/drive/MyDrive/U-2-Net/test_data/u2net_results/39.png')/255\n",
        "\n",
        "# Read the actual mask\n",
        "target = cv2.imread('/content/drive/MyDrive/U-2-Net/test_data/test_images/labels/39.png')/255"
      ],
      "metadata": {
        "id": "4cd4cOUC595W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred.shape)\n",
        "print(target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-ozSQf144Ht",
        "outputId": "196b9678-2e3a-4713-e363-ec87a7928363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(860, 645, 3)\n",
            "(860, 645, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the images to four dimentional.\n",
        "new_pred=pred.reshape((-1, 860, 645, 3 ))\n",
        "new_target=target.reshape((-1, 860, 645, 3 ))"
      ],
      "metadata": {
        "id": "_63kyoRV44Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_pred.shape)\n",
        "print(new_target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFVPMxHZ44lF",
        "outputId": "73acb14e-1a24-48d8-ef5a-ea8e686cb922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 860, 645, 3)\n",
            "(1, 860, 645, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now Calculate the Dice Score.\n",
        "dice_score = dice_coefficient(new_target, new_pred)\n",
        "\n",
        "print (\"Dice Coeff: {}\".format(dice_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMS1OC396c9F",
        "outputId": "529f519e-f282-4a77-ad5a-9c00ee4d70d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dice Coeff: 0.6318872990193791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now calculate the IOU score**"
      ],
      "metadata": {
        "id": "un18zeOS8aft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the IOU function.\n",
        "def IOU(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(y_true * y_pred, axis=[1, 2, 3])\n",
        "    union = K.sum(y_true, axis=[1, 2, 3]) + K.sum(y_pred, axis=[1, 2, 3])\n",
        "    return K.mean((intersection + smooth) / (union + smooth), axis=0)"
      ],
      "metadata": {
        "id": "tyfg_8L77k_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now Calculate the IOU Score.\n",
        "IoU_score = IOU(new_target, new_pred)\n",
        "\n",
        "print (\"IOU Score: {}\".format(IoU_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCbyga3D7vFq",
        "outputId": "5d185b37-011c-48c1-c8ad-97390ae017d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IOU Score: 0.31594400212692086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2\n",
        "\n",
        "Traing u2-net model using dice loss as loss function "
      ],
      "metadata": {
        "id": "NuseKByE9OXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run the code to start the training.\n",
        "%cd /content/drive/MyDrive/U-2-Net/\n",
        "!python custom_u2net_train.py"
      ],
      "metadata": {
        "id": "hm5CmNmC9S1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "!python u2net_test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfhcpYq32DsJ",
        "outputId": "20b0159a-ea85-442e-8f17-84a3bea4c197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/U-2-Net/test_data/test_images/images/49.jpg', '/content/drive/MyDrive/U-2-Net/test_data/test_images/images/41.jpg', '/content/drive/MyDrive/U-2-Net/test_data/test_images/images/39.jpg', '/content/drive/MyDrive/U-2-Net/test_data/test_images/images/45.jpg', '/content/drive/MyDrive/U-2-Net/test_data/test_images/images/46.jpg']\n",
            "...load U2NET---168.3 MB\n",
            "inferencing: 49.jpg\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
            "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3704: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "inferencing: 41.jpg\n",
            "inferencing: 39.jpg\n",
            "inferencing: 45.jpg\n",
            "inferencing: 46.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Dice Score for dice loss function."
      ],
      "metadata": {
        "id": "hhCnAk153XWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "HFRTvPqG3RO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the Dice score function.\n",
        "def dice_coefficient(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(y_true * y_pred, axis=[1, 2, 3])\n",
        "    union = K.sum(y_true, axis=[1, 2, 3]) + K.sum(y_pred, axis=[1, 2, 3])\n",
        "    return K.mean((2. * intersection + smooth) / (union + smooth), axis=0)"
      ],
      "metadata": {
        "id": "0OKnfwyq3gQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the predicted mask\n",
        "pred = cv2.imread('/content/drive/MyDrive/U-2-Net/test_data/u2net_results/39.png')/255\n",
        "\n",
        "# Read the actual mask\n",
        "target = cv2.imread('/content/drive/MyDrive/U-2-Net/test_data/test_images/labels/39.png')/255"
      ],
      "metadata": {
        "id": "VYD1OT4z3mKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred.shape)\n",
        "print(target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFhnnHCH3qQD",
        "outputId": "0334020f-2db0-43ec-b127-5c97426d3d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(860, 645, 3)\n",
            "(860, 645, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the images to four dimentional.\n",
        "new_pred=pred.reshape((-1, 860, 645, 3 ))\n",
        "new_target=target.reshape((-1, 860, 645, 3 ))"
      ],
      "metadata": {
        "id": "W2DqmpWf3tb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_pred.shape)\n",
        "print(new_target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ECsHO6r3yNT",
        "outputId": "1998fe16-b552-4d91-ed88-a4bd017dd571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 860, 645, 3)\n",
            "(1, 860, 645, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now Calculate the Dice Score.\n",
        "dice_score = dice_coefficient(new_target, new_pred)\n",
        "\n",
        "print (\"Dice Coeff: {}\".format(dice_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LT6n9hY30x9",
        "outputId": "364ff023-cbe5-4949-9e84-03cde09582b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dice Coeff: 0.6215403036762658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the IOU function.\n",
        "def IOU(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(y_true * y_pred, axis=[1, 2, 3])\n",
        "    union = K.sum(y_true, axis=[1, 2, 3]) + K.sum(y_pred, axis=[1, 2, 3])\n",
        "    return K.mean((intersection + smooth) / (union + smooth), axis=0)"
      ],
      "metadata": {
        "id": "SJi7d-ED35vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now Calculate the IOU Score.\n",
        "IoU_score = IOU(new_target, new_pred)\n",
        "\n",
        "print (\"IOU Score: {}\".format(IoU_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0NRNZkM3_Zc",
        "outputId": "28264945-a5e7-4cc0-b110-3e2090e98600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IOU Score: 0.31077049749078145\n"
          ]
        }
      ]
    }
  ]
}